---
title: "Table vs graph"
author: ''
date: '2021-08-20'
slug: ML
categories: [ML, data vis]
tags: [ML, data vis]
output: 
  bookdown::html_document2: 
    fig_caption: yes
header:
  caption: ''
  image: ''
link-citations: yes
draft: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(tidyr)
library(stringr)
```

Shall I display my data using a table or a graph? The usual answer is: it depends. Mostly, it depends on who the audience is and how the data will be used. I agree, but [Alaa et al, 2021](https://doi.org/10.1038/s42256-021-00353-8) may have gone a bit too far using tables.  

I'll start with a brief summary of the paper. 

It is about the development of Adjutorium - a machine learning algorithm for breast cancer prognostication. The authors motivate the development of Adjutorium by stating that a widely used model (PREDICT v2.1) under-performs in specific subgroups of patients. They then compare the accuracy of Adjutorium in predicting all-cause and breast cancer-specific mortality at 3, 5 and 10 years from baseline with PREDICT v2.1. In addition, they compare Adjutorium to an in-house Cox proportional hazards (Cox PH) regression model. They use a series of measures to assess the three models, AUC-ROC, Harrel's C-index and Uno’s C-index. They conclude that "Adjutorium uniformly outperformed PREDICT v2.1 and the conventional Cox PH model in predicting all-cause and breast cancer-specific mortality". 

This statement is mostly based on [Table 1](https://www.nature.com/articles/s42256-021-00353-8/tables/1). But, the table is cramped with so many values that is difficult to draw any conclusions - unless you spend hours on it. 

I argue that the main message they are trying to convey is not contained in the actual values, which would justify this tabular form, but  in the "shape" of the values. They want to reveal the relationships among the three models. That is why I believe a graph would communicate the message more efficiently. So, below I plot the bottom panel (external validation cohort) of their Table 1. 

The horizontal lines show the performance of Adjutorium. In general, Adjutorium performs better. The improvement in performance is more evident for the cancer-specific mortality (right panel).

Interestingly though, the conclusions depend the choice of performance measure. For example, using the AUC-ROC and Uno’s C-index the simpler Cox PH model predicts all-cause mortality equally well to Adjutorium. 

In general, I find graphs more informative - it is easier to see trends in the data when it is displayed visually compared to when it is displayed numerically in a table.  

```{r, echo=F}
# paper
# https://www.nature.com/articles/s42256-021-00353-8#Sec2
ML_data <- read.delim("~//Desktop/Website//content/post/table_vs_graph/ML_data.txt", comment.char="#")

data_long <- gather(ML_data, model, measurement, Adjutorium:PREDICT, factor_key=TRUE)

# post-processing
one_step <- str_split_fixed(data_long$measurement, ' \\(', 2)
two_step <- str_split_fixed(one_step[,2], '\xd0', 2)
three_step <- str_split_fixed(two_step[,2], '\\)', 2)

means <- one_step[, 1]
low_ci <- two_step[, 1]
upper_ci <- three_step[,1]


ML_data_2 <- cbind(data_long[1:4], means, low_ci, upper_ci)


ggplot(ML_data_2) + 
    geom_point(aes(x = factor(model), y = as.numeric(means), col = factor(time)), size = 1.8) + 
    geom_errorbar(aes(x = factor(model), ymin = as.numeric(low_ci), ymax = as.numeric(upper_ci), col = factor(time)), 
                  width = .01, size = 1.8) +
    facet_grid( metric~mortality) +
    geom_hline(data = ML_data_2 %>% filter(model == "Adjutorium"), 
               aes(yintercept = as.numeric(means)), linetype = "dashed") +
    theme_bw() +
    labs(x = "", y = "Mean 95% CI", col = "Time horizon")

```
