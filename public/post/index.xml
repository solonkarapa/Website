<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | </title>
    <link>https://solon-karapanagiotis.com/post/</link>
      <atom:link href="https://solon-karapanagiotis.com/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Solon Karapanagiotis 2018-2021</copyright><lastBuildDate>Thu, 20 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://solon-karapanagiotis.com/media/icon.png</url>
      <title>Posts</title>
      <link>https://solon-karapanagiotis.com/post/</link>
    </image>
    
    <item>
      <title>Naive classification beats deep-learning</title>
      <link>https://solon-karapanagiotis.com/post/auc_post/model-evaluation-auc/</link>
      <pubDate>Thu, 20 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://solon-karapanagiotis.com/post/auc_post/model-evaluation-auc/</guid>
      <description>
&lt;script src=&#34;https://solon-karapanagiotis.com/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://solon-karapanagiotis.com/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;div id=&#34;overview&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nature.com/articles/s41551-019-0487-z#Abs1&#34;&gt;Mitani and co-authors’&lt;/a&gt; present a deep-learning algorithm trained with retinal images and participants’ clinical data from the UK Biobank to estimate blood-haemoglobin levels and predict the presence or absence of anaemia &lt;span class=&#34;citation&#34;&gt;(Mitani et al. &lt;a href=&#34;#ref-mitani2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. A major limitation of the study is the inadequate evaluation of the algorithm. I will show how a naïve classification (i.e. classify everybody as healthy) performs much better than their deep-learning approach, despite their model having AUC of around 80%. I will then explain why this is the case and finish with some thoughts on how (clinical) predictions models should be evaluated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The goal of the paper was to investigate whether anaemia can be detected via machine-learning algorithms trained using retinal images, study participants’ metadata or the combination of both.&lt;/p&gt;
&lt;p&gt;First, the authors develop a deep-learning algorithm to predict haemoglobin concentration (Hb) (which is the most reliable indicator of anaemia) and then three others to predict anaemia itself. They develop a deep convolutional neural network classification model to directly predict whether a patient is anaemic (rather than predicting Hb). They used the World Health Organization Hb cut-off values to label each participant as not having anaemia, mild, moderate or severe anaemia. One of the models was trained to classify: normal versus mild, moderate or severe. For concreteness, I’ll focus on this model but the reasoning below is valid for the others as well. Also, I focus on the combined model (images and metadata) because it showed the best performance (AUC of 0.88).&lt;/p&gt;
&lt;p&gt;The authors present a detailed analysis of the data and their model. Nevertheless, a crucial point is missing. Is the model useful? If it is implemented tomorrow will it result in better care? This is important, especially since the authors argue their model potentially enables automated anaemia screening (see Discussion). This is slightly far-fetched in my opinion given they only evaluated their algorithm on a test set; with unsatisfactory results as I argue below. Algorithms need to be compared with human experts (i.e. ophthalmologists in this case), followed by extensive field testing to prove their trustworthiness and usefulness &lt;span class=&#34;citation&#34;&gt;(Spiegelhalter &lt;a href=&#34;#ref-Spiegelhalter2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-issue&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;The Issue&lt;/h1&gt;
&lt;p&gt;As with many other models out there the authors have fallen in the trap of evaluating the absolute performance of their model when the important metric is the relative performance with respect to the actual reality of retinal screening. It is exactly the same idea when in clinical trials the experimental treatment is compared with the standard of care (or placebo). We want to see if the new treatment (deep-learning model classification in this case) is better than simply classifying every participant as having anaemia or not. This should be the benchmark. I call these naive classifications and I will demonstrate the model does not perform better than the naive rule of classifying everybody as healthy.&lt;/p&gt;
&lt;p&gt;First, some preliminary stuff. The performance of a model can be represented in a &lt;a href=&#34;https://en.wikipedia.org/wiki/Confusion_matrix&#34;&gt;confusion matrix&lt;/a&gt; with four categories (see table below). True positives (TP) are positive examples that are correctly labelled as positives, and False positives (FP) are negative examples that are labelled incorrectly as positive. Likewise, True negatives (TN) are negatives labelled correctly as negative, and false negatives (FN) refer to positive examples labelled incorrectly as negative.&lt;/p&gt;
&lt;table class=&#34;table table-striped table-responsive&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Confusion matrix showing correct classifications (in red) and incorrect (in blue)
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;empty-cells: hide;border-bottom:hidden;&#34; colspan=&#34;2&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; color: black !important;&#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Truth
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;color: gray !important;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;color: gray !important;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;color: gray !important;&#34;&gt;
positive
&lt;/th&gt;
&lt;th style=&#34;text-align:left;color: gray !important;&#34;&gt;
negative
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;&#34;&gt;
Predict
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: gray !important;&#34;&gt;
positive
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     color: red !important;&#34;&gt;TP&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     color: blue !important;&#34;&gt;FP&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: gray !important;&#34;&gt;
negative
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     color: blue !important;&#34;&gt;FN&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     color: red !important;&#34;&gt;TN&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s use the information from the study to construct our confusion matrix using the data from Table 1 (last column; validation dataset) and Table 2 (column 1 and anaemia combined model). There are in total 10949 negatives (non anaemic) out of 11388 participants. The reported specificity is 0.7 and sensitivity is 0.875. Using these we can calculate the number of true negatives and true positives as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Specificity = true negative rate (TNR) = TN/#negative, so TN = 0.7*10949 = 7664.&lt;/li&gt;
&lt;li&gt;Sensitivity = true positive rate (TPR) = TP/ #positive, so TP = 0.875*439 = 384.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So the confusion matrix is&lt;/p&gt;
&lt;table class=&#34;table table-striped table-responsive&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;empty-cells: hide;border-bottom:hidden;&#34; colspan=&#34;2&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; color: black !important;&#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Truth
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;color: gray !important;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;color: gray !important;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;color: gray !important;&#34;&gt;
positive
&lt;/th&gt;
&lt;th style=&#34;text-align:left;color: gray !important;&#34;&gt;
negative
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;&#34;&gt;
Predict
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: gray !important;&#34;&gt;
positive
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     color: red !important;&#34;&gt;384&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     color: blue !important;&#34;&gt;3285&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: gray !important;&#34;&gt;
negative
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     color: blue !important;&#34;&gt;55&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     color: red !important;&#34;&gt;7664&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Looking at the table we see that in total 3285+55 = 3340 subjects have been misclassified. That is 29% misclassification rate.&lt;/p&gt;
&lt;p&gt;Now, let’s construct the confusion matrix for my naïve classification: “classify everybody as negative”,&lt;/p&gt;
&lt;table class=&#34;table table-striped table-responsive&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;empty-cells: hide;border-bottom:hidden;&#34; colspan=&#34;2&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; color: black !important;&#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Truth
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;color: gray !important;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;color: gray !important;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;color: gray !important;&#34;&gt;
positive
&lt;/th&gt;
&lt;th style=&#34;text-align:left;color: gray !important;&#34;&gt;
negative
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;&#34;&gt;
Predict
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: gray !important;&#34;&gt;
positive
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     color: red !important;&#34;&gt;0&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     color: blue !important;&#34;&gt;0&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;color: gray !important;&#34;&gt;
negative
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     color: blue !important;&#34;&gt;439&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span style=&#34;     color: red !important;&#34;&gt;10949&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In total, 439 subjects have been misclassified. This is 4% misclassification rate.&lt;/p&gt;
&lt;p&gt;This means the naive classification achieves 86% better performance!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-and-auc-is-to-blame&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;ROC (and AUC) is to blame&lt;/h1&gt;
&lt;p&gt;Why was this not spotted by the authors? Probably because the model evaluation was based on the receiver operating curve (ROC) and area under the ROC (AUC). ROC curves can present an overly optimistic view of a model’s performance if there is a large skew in the class distribution. In this study the ratio positive (i.e. anaemic) to negative (i.e. not anaemic) participants is 439/10949 = 0.04 (see Table 2, validation column)!&lt;/p&gt;
&lt;p&gt;ROC curves (and AUC) have the (un-)attractive property of being insensitive to changes in class distribution &lt;span class=&#34;citation&#34;&gt;(Fawcett &lt;a href=&#34;#ref-fawcett2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;. That is, if the proportion of positive to negative instances changes in a dataset, the ROC curves (and AUC) will not change. This is because ROC plots are based upon TPR and FPR which do not depend on class distributions. Increasing the number of positive samples by 10x would increase both TP and FN by 10x, which would not change the TPR at any threshold. Similarly, increasing the number of negative samples by 10x would increase both TN and FP by 10x, which would not change the FPR at any threshold. Thus, both the shape of the ROC curve and the AUC are insensitive to the class distribution. On the contrary, any performance metric that uses values from both columns will be inherently sensitive to class skews, for instance the misclassification rate.&lt;/p&gt;
&lt;p&gt;Let’s make this more concrete with a simple simulation example. I simulate one covariate, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, which follows a standard Gaussian distribution in negative cases: &lt;span class=&#34;math inline&#34;&gt;\(X \sim N(0, 1)\)&lt;/span&gt;. Among positives, it follows &lt;span class=&#34;math inline&#34;&gt;\(X \sim N(1.5, 1)\)&lt;/span&gt;. The event rate (i.e. prevalence) is varied to be &lt;span class=&#34;math inline&#34;&gt;\(20\%\)&lt;/span&gt; (I call this scenario 1) and &lt;span class=&#34;math inline&#34;&gt;\(2\%\)&lt;/span&gt; (scenario 2). (The &lt;span class=&#34;math inline&#34;&gt;\(2\%\)&lt;/span&gt; is close to the one observed in the study &lt;span class=&#34;math inline&#34;&gt;\(\approx 4\%\)&lt;/span&gt;). Then, I derive true risks (&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;) based on the event rate (&lt;span class=&#34;math inline&#34;&gt;\(ER\)&lt;/span&gt;) and the density of the covariate distributions for positives (&lt;span class=&#34;math inline&#34;&gt;\(D_p\)&lt;/span&gt;) and negatives (&lt;span class=&#34;math inline&#34;&gt;\(D_{n}\)&lt;/span&gt;) at the covariate values:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ R = \frac{ER × D_p}{[ER × D_p] + [(1 − ER) × D_{n}]}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I simulate two large samples of 5000 and 50000 and plot the ROC. The two plots below are almost identical despite the fact that scenario 2 has 10x more negative examples than scenario 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pROC)
library(tibble)
sim_data &amp;lt;- function(n_positives, n_negatives){# simulates dataset as described above and calculates the ROC
    # input arguments: the number of positives and negatives 
    
    y &amp;lt;- c(rep(0, n_negatives), rep(1, n_positives)) # binary response 
    x &amp;lt;- c(rnorm(n_negatives), rnorm(n_positives, mean = 1.5)) # simulate covariate
    df &amp;lt;- data.frame(y = y, x = x)
    
    ER &amp;lt;- mean(df$y) # event rate 
    Dp &amp;lt;- dnorm(df$x, mean = 1.5, sd = 1) # covariate density for positives
    Dn &amp;lt;- dnorm(df$x, mean = 0, sd = 1) # covariate density for negatives 
   
    true_risk &amp;lt;- (ER * Dp)/((ER * Dp) + ((1 - ER) * Dn))  # true risks
    
    roc_sim &amp;lt;- roc(df$y, true_risk) # calculates ROC curve
    
    df &amp;lt;- tibble(FPR = 1 - roc_sim$specificities, # false positive rate
                 TPR = roc_sim$sensitivities) # true positive rate
    
    return(df)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n.sims &amp;lt;- 20 # times simulation is repeated
n.positives &amp;lt;- 1000 # number of positives 
n.negatives &amp;lt;- 4000 # number of negatives

library(purrr)
library(dplyr)
# scenario 1
multiplier &amp;lt;- 1 # the multiplier adjusts the number of the negatives - so I can have the event rate I want
sims1 &amp;lt;- n.sims %&amp;gt;%
    rerun(sim_data(n.positives, n.negatives * multiplier)) %&amp;gt;%
    map(~ data.frame(.x)) %&amp;gt;%
    plyr::ldply(., data.frame, .id = &amp;quot;Name&amp;quot;) %&amp;gt;% 
    mutate(sims = rep(1:n.sims, each = sum(n.positives + n.negatives * multiplier) + 1),
           Scenario = &amp;quot;Scenario 1&amp;quot;)

# scenario 2
multiplier &amp;lt;- 10 
sims2 &amp;lt;- n.sims %&amp;gt;%
    rerun(sim_data(n.positives, n.negatives * multiplier)) %&amp;gt;%
    map(~ data.frame(.x)) %&amp;gt;%
    plyr::ldply(., data.frame, .id = &amp;quot;Name&amp;quot;) %&amp;gt;% 
    mutate(sims = rep(1:n.sims, each = sum(n.positives + n.negatives * multiplier) + 1),
           Scenario = &amp;quot;Scenario 2&amp;quot;)

df_final &amp;lt;- rbind(sims1, sims2)

library(ggplot2)
ggplot(df_final) +
  geom_line(aes(x = FPR, y = TPR, group = sims, col = Scenario), alpha = 0.8) + 
  facet_grid(~ Scenario)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://solon-karapanagiotis.com/post/auc_post/classification_AUC_files/figure-html/figs-1.png&#34; alt=&#34;ROC plots for each scenario; 20 repetitions each.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: ROC plots for each scenario; 20 repetitions each.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;unequal-misclassification-costs&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Unequal misclassification costs&lt;/h1&gt;
&lt;p&gt;Of course, my naive classification can be easily debated by noting that the misclassification rate makes an inherent assumption, which is unlikely to be true in anaemia screening: it assumes that misclassifying someone with anaemia is of the same severity as misclassifying a healthy subject. This implies that one type of error is more costly (i.e. worse) than the other. In other words, the costs are asymmetric. I agree that most of time this is the case. This information should be taken into account when evaluating or fitting models.&lt;/p&gt;
&lt;p&gt;Some methods to account for differing consequences of correct and incorrect classification when evaluating models are the Weighted Net Reclassification Improvement &lt;span class=&#34;citation&#34;&gt;(Pencina et al. &lt;a href=&#34;#ref-pencina2011&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;, Relative Utility &lt;span class=&#34;citation&#34;&gt;(Baker et al. &lt;a href=&#34;#ref-baker2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;, Net Benefit &lt;span class=&#34;citation&#34;&gt;(Vickers and Elkin &lt;a href=&#34;#ref-vickers2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; and the &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; measure &lt;span class=&#34;citation&#34;&gt;(Hand &lt;a href=&#34;#ref-hand2009&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt;. Another option is to design models/algorithms that take misclassification costs into consideration. This area of research is called cost-sensitive learning &lt;span class=&#34;citation&#34;&gt;(Elkan &lt;a href=&#34;#ref-elkan2001&#34; role=&#34;doc-biblioref&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This is another reason the ROC (AUC) is inadequate metric (in addition to the insensitivity in class imbalance). It ignores clinical differentials in misclassification costs and, therefore, risks finding a model worthwhile (or worthless) when patients and clinicians would consider otherwise. Strictly speaking, ROC weighs changes in sensitivity and specificity equally only where the curve slope equals one &lt;span class=&#34;citation&#34;&gt;(Fawcett &lt;a href=&#34;#ref-fawcett2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;. Other points assign different weights, determined by curve shape and without considering any clinically meaningful information. Thus, AUC can consider a model that increases sensitivity at low specificity superior to one that increases sensitivity at high specificity. However, in some situations, in disease screening for instance, better tests must increase sensitivity at high specificity to avoid numerous false positives.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-way-forward-estimate-and-validate-probabilities&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;A way forward: estimate and validate probabilities&lt;/h1&gt;
&lt;p&gt;Ultimately, the quality of algorithms is exposed to the nature of the performance metrics chosen. We must carefully choose the goals we ask these systems to optimize. Evaluation of models for use in healthcare should take the intended purpose of the model into account. Metrics such as AUC are rarely of any use in clinical practise. AUC represents how likely it is that the model will rank a pair of subjects; one with anaemia and one without, in the correct order, across all possible thresholds. More intuitively, AUC is the chance that a randomly selected participant with anaemia will be ranked above a randomly selected healthy participant. However, patients do not walk into the clinician’s room in pairs, and patients want their results, rather than the order of their results compared with another patient. They care about their individual risk of having a disease/condition (being anaemic in this case). Hence, the focus of modelling should be on estimating and validating risks/probabilities rather than the chance of correctly ranking a pair of patients.&lt;/p&gt;
&lt;p&gt;Consequently, model evaluation/comparison should focus (primarily) on calibration. Calibration refers to the agreement between observed and predicted probabilities. This means that for future cases predicted to be in class &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, a proportion of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; cases will truly belong in class &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, and this should be true for all &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; in (0,1). In other words, for every 100 patients given a risk of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;%, close to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; have the event. Calibration approaches appropriate for representing prediction accuracy are crucial, especially when treatment decisions are made based on probability thresholds. Calibration of a model can be evaluated graphically by plotting expected against observed probabilities &lt;span class=&#34;citation&#34;&gt;(Steyerberg et al. &lt;a href=&#34;#ref-steyerberg2004&#34; role=&#34;doc-biblioref&#34;&gt;2004&lt;/a&gt;)&lt;/span&gt; or using an aggregate score. The most widely used is the Brier score, which is given by the average over all squared differences between an observation and its predicted probability &lt;span class=&#34;citation&#34;&gt;(Brier &lt;a href=&#34;#ref-brier1950&#34; role=&#34;doc-biblioref&#34;&gt;1950&lt;/a&gt;)&lt;/span&gt;. (It has nice properties &lt;span class=&#34;citation&#34;&gt;(see e.g. Spiegelhalter &lt;a href=&#34;#ref-spiegelhalter1986&#34; role=&#34;doc-biblioref&#34;&gt;1986&lt;/a&gt;; Gneiting and Raftery &lt;a href=&#34;#ref-gneiting2007&#34; role=&#34;doc-biblioref&#34;&gt;2007&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;To conclude, machine learning models in healthcare need rigorous evaluation. Beyond ethical, legal and moral issues, the technical/statistical fitness of the models needs thorough assessment. Statistical analysis should consider clinically relevant evaluation metrics. Motivated by the paper of &lt;span class=&#34;citation&#34;&gt;Mitani et al. (&lt;a href=&#34;#ref-mitani2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; I have re-demonstrated why AUC is an irrelevant metric for clinical practise. This is because it is insensitive to class imbalances and integrates over all error regimes (under the best case scenario). This becomes increasingly important in predicting rare outcomes, where operating in a regime that corresponds to a high false positive rate may be impractical, because costly interventions might be applied in situations in which patients are unlikely to benefit. A way forward is to focus on evaluating predictions rather than (in addition to) classifications. That is, focus on estimating and evaluating probabilities. These convey more useful information to clinicians and patients in order to aid decision making.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reading&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Further reading&lt;/h1&gt;
&lt;p&gt;The limitations of the ROC (and AUC) have been discussed in&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cook NR . Use and misuse of the receiver operating characteristic curve in risk prediction. Circulation (2007).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pencina, Michael J., et al. “Evaluating the added predictive ability of a new marker: from area under the ROC curve to reclassification and beyond.” Statistics in medicine (2008).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hand, David J. “Evaluating diagnostic tests: the area under the ROC curve and the balance of errors.” Statistics in medicine (2010).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hand, David J., and Christoforos Anagnostopoulos. “When is the area under the receiver operating characteristic curve an appropriate measure of classifier performance?.” Pattern Recognition Letters (2013).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Halligan, Steve, Douglas G. Altman, and Susan Mallett. “Disadvantages of using the area under the receiver operating characteristic curve to assess imaging tests: a discussion and proposal for an alternative approach.” European radiology (2015).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On probability estimation and evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kruppa, Jochen, Andreas Ziegler, and Inke R. König. “Risk estimation and risk prediction using machine-learning methods.” Human genetics (2012).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kruppa, Jochen, et al. “Probability estimation with machine learning methods for dichotomous and multicategory outcome: Theory.” Biometrical Journal (2014).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kruppa, Jochen, et al. “Probability estimation with machine learning methods for dichotomous and multicategory outcome: Applications.” Biometrical Journal (2014).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-baker2009&#34;&gt;
&lt;p&gt;Baker, S. G., Cook, N. R., Vickers, A., and Kramer, B. S. (2009), “Using relative utility curves to evaluate risk prediction,” &lt;em&gt;Journal of the Royal Statistical Society: Series A (Statistics in Society)&lt;/em&gt;, Wiley Online Library, 172, 729–748.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brier1950&#34;&gt;
&lt;p&gt;Brier, G. W. (1950), “Verification of forecasts expressed in terms of probability,” &lt;em&gt;Monthly weather review&lt;/em&gt;, 78, 1–3.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-elkan2001&#34;&gt;
&lt;p&gt;Elkan, C. (2001), “The foundations of cost-sensitive learning,” in &lt;em&gt;International joint conference on artificial intelligence&lt;/em&gt;, Lawrence Erlbaum Associates Ltd, pp. 973–978.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-fawcett2006&#34;&gt;
&lt;p&gt;Fawcett, T. (2006), “An introduction to roc analysis,” &lt;em&gt;Pattern recognition letters&lt;/em&gt;, Elsevier, 27, 861–874.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gneiting2007&#34;&gt;
&lt;p&gt;Gneiting, T., and Raftery, A. E. (2007), “Strictly proper scoring rules, prediction, and estimation,” &lt;em&gt;Journal of the American statistical Association&lt;/em&gt;, Taylor &amp;amp; Francis, 102, 359–378.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hand2009&#34;&gt;
&lt;p&gt;Hand, D. J. (2009), “Measuring classifier performance: A coherent alternative to the area under the roc curve,” &lt;em&gt;Machine learning&lt;/em&gt;, Springer, 77, 103–123.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mitani2020&#34;&gt;
&lt;p&gt;Mitani, A., Huang, A., Venugopalan, S., Corrado, G. S., Peng, L., Webster, D. R., Hammel, N., Liu, Y., and Varadarajan, A. V. (2020), “Detection of anaemia from retinal fundus images via deep learning,” &lt;em&gt;Nature Biomedical Engineering&lt;/em&gt;, Nature Publishing Group, 4, 18–27.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pencina2011&#34;&gt;
&lt;p&gt;Pencina, M. J., D’Agostino Sr, R. B., and Steyerberg, E. W. (2011), “Extensions of net reclassification improvement calculations to measure usefulness of new biomarkers,” &lt;em&gt;Statistics in medicine&lt;/em&gt;, Wiley Online Library, 30, 11–21.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Spiegelhalter2020&#34;&gt;
&lt;p&gt;Spiegelhalter, D. (2020), “Should we trust algorithms?” &lt;em&gt;Harvard Data Science Review&lt;/em&gt;, 2. &lt;a href=&#34;https://doi.org/10.1162/99608f92.cb91a35a&#34;&gt;https://doi.org/10.1162/99608f92.cb91a35a&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-spiegelhalter1986&#34;&gt;
&lt;p&gt;Spiegelhalter, D. J. (1986), “Probabilistic prediction in patient management and clinical trials,” &lt;em&gt;Statistics in medicine&lt;/em&gt;, Wiley Online Library, 5, 421–433.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-steyerberg2004&#34;&gt;
&lt;p&gt;Steyerberg, E. W., Borsboom, G. J., Houwelingen, H. C. van, Eijkemans, M. J., and Habbema, J. D. F. (2004), “Validation and updating of predictive logistic regression models: A study on sample size and shrinkage,” &lt;em&gt;Statistics in medicine&lt;/em&gt;, Wiley Online Library, 23, 2567–2586.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vickers2006&#34;&gt;
&lt;p&gt;Vickers, A. J., and Elkin, E. B. (2006), “Decision curve analysis: A novel method for evaluating prediction models,” &lt;em&gt;Medical Decision Making&lt;/em&gt;, Sage Publications Sage CA: Thousand Oaks, CA, 26, 565–574.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Approximating Binomial with Poisson</title>
      <link>https://solon-karapanagiotis.com/post/approx_binomial/approximating-binomial-with-poisson/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://solon-karapanagiotis.com/post/approx_binomial/approximating-binomial-with-poisson/</guid>
      <description>


&lt;p&gt;It is usually taught in statistics classes that Binomial probabilities can be approximated by Poisson probabilities, which are generally easier to calculate. This approximation is valid “when &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is large and &lt;span class=&#34;math inline&#34;&gt;\(np\)&lt;/span&gt; is small,” and rules of thumb are sometimes given.&lt;/p&gt;
&lt;p&gt;In this post I’ll walk through a simple proof showing that the Poisson distribution is really just the Binomial with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; (the number of trials) approaching infinity and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; (the probability of success in each trail) approaching zero. I’ll then provide some numerical examples to investigate how good is the approximation.&lt;/p&gt;
&lt;div id=&#34;proof&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Proof&lt;/h2&gt;
&lt;p&gt;The Binomial distribution describes the probability that there will be &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; successes in a sample
of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, chosen with replacement from a population where the probability of success is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X \sim Binomial(n, p)\)&lt;/span&gt;, that is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:binom&#34;&gt;\[\begin{equation}
\tag{1}
   P(X = x) = {n\choose x} p^x (1-p)^{n-x},
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(x= 0, 1, \dots, n\)&lt;/span&gt;. Define the number&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\lambda = np\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is the rate of success. That’s the number of trials &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;—however many there are—times the chance of success &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; for each of those trials. If we repeat the experiment every day, we will be getting &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; successes per day on average.&lt;/p&gt;
&lt;p&gt;Solving for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p = \frac{\lambda}{n}\]&lt;/span&gt;
We then substitute this into &lt;a href=&#34;#eq:binom&#34;&gt;(1)&lt;/a&gt;, and take the limit as &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; goes to infinity&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \lim_{n \to \infty}P(X = x) =  \lim_{n \to \infty} \frac{n!}{x!(n-x)!} \bigg( \frac{\lambda}{n} \bigg)^x \bigg( 1-\frac{\lambda}{n} \bigg)^{n-x}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I then collect the constants (terms that don’t depend on &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;) in front and split the last term into two&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:limit&#34;&gt;\[\begin{equation}
   \tag{2}
   \frac{\lambda^x}{x!}  \lim_{n \to \infty} \color{blue}{\frac{n!}{(n-x)!} \bigg( \frac{1}{n} \bigg)^x} \color{red}{ \bigg( 1-\frac{\lambda}{n} \bigg)^n } \color{green}{\bigg( 1-\frac{\lambda}{n} \bigg)^{-x}}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now let’s take the limit of this right-hand side one term at a time.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We start with the blue term&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\color{blue}{ \lim_{n \to \infty} \frac{n!}{(n-x)!} \bigg( \frac{1}{n} \bigg)^x }\]&lt;/span&gt;
The numerator and denominator can be expanded as follows&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\color{blue}{ \lim_{n \to \infty} \frac{(n)(n-1)(n-2)\dots(n-x)(n-x-1)\dots (1)}{(n-x)(n-x-1)(n-x-2)\dots (1)}\bigg( \frac{1}{n} \bigg)^x }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\((n-x)(n-x-1)\dots(1)\)&lt;/span&gt; terms cancel from both the numerator and denominator, leaving the following&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\color{blue}{ \lim_{n \to \infty} \frac{(n)(n-1)(n-2)(n-x+1)}{n^x} }\]&lt;/span&gt;
This can be rewrited as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\color{blue}{  \lim_{n \to \infty} \frac{n}{n} \frac{(n-1)}{n} \frac{(n-2)}{n} \frac{(n-x+1)}{n} }\]&lt;/span&gt;
This is because there were &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; terms in both the numerator and denominator. Clearly, every one of these &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; terms approaches 1 as &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; approaches infinity. So we know this just simplifies to one. So we’re done with the first step.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Now we focus on the red term of &lt;a href=&#34;#eq:limit&#34;&gt;(2)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\color{red}{ \lim_{n \to \infty} \bigg( 1-\frac{\lambda}{n} \bigg)^n }\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Recall the &lt;a href=&#34;https://en.wikipedia.org/wiki/E_(mathematical_constant)&#34;&gt;definition&lt;/a&gt; of &lt;span class=&#34;math inline&#34;&gt;\(e= 2.7182\dots\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \lim_{a \to \infty} \bigg(1 + \frac{1}{a}\bigg)^a\]&lt;/span&gt;
Our goal here is to find a way to manipulate our expression to look more like the definition of &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;, which we know the limit of. Let’s define a number &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ a = -\frac{n}{\lambda}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substituting it into our expression we get&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \color{red}{ \lim_{n \to \infty} \bigg( 1-\frac{\lambda}{n} \bigg)^n = \lim_{n \to \infty} \bigg( 1+\frac{1}{a} \bigg)^{-a\lambda} = e^{-\lambda} }\]&lt;/span&gt;
So we’ve finished with the middle term.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The third term of &lt;a href=&#34;#eq:limit&#34;&gt;(2)&lt;/a&gt; is&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\color{green}{ \lim_{n \to \infty}  \bigg( 1-\frac{\lambda}{n} \bigg)^{-x} }\]&lt;/span&gt;
As &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; approaches infinity, this term becomes &lt;span class=&#34;math inline&#34;&gt;\(1^{-x}\)&lt;/span&gt; which is equal to one. And that takes care of our last term.&lt;/p&gt;
&lt;p&gt;Putting these together we can re-write &lt;a href=&#34;#eq:limit&#34;&gt;(2)&lt;/a&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{\lambda^x}{x!}  \lim_{n \to \infty} \color{blue}{ \frac{n!}{(n-x)!} \bigg( \frac{1}{n} \bigg)^x} \color{red}{ \bigg( 1-\frac{\lambda}{n} \bigg)^n} \color{green}{ \bigg( 1-\frac{\lambda}{n} \bigg)^{-x} } = \frac{\lambda^x}{x!} \color{red}{ e^{-\lambda} }\]&lt;/span&gt;
which is the probability mass function of a Poisson random variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, i.e&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(Y = y)  = \frac{\lambda^y}{y!} e^{-\lambda}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y = 0, 1, \dots\)&lt;/span&gt;. So we have shown that the Poisson distribution is a special case of the Binomial, in which the number of trials grows to infinity and the chance of success in any trial approaches zero. And that completes the proof.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Casella and Berger (&lt;a href=&#34;#ref-casella2002statistical&#34; role=&#34;doc-biblioref&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt; provide a much shorter proof based on moment generating functions.&lt;/p&gt;
&lt;p&gt;A natural question is how good is this approximation? It turns out it is quite good even for moderate &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; as we’ll see with a few numerical examples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;A &lt;a href=&#34;https://www.itl.nist.gov/div898/handbook/pmc/section3/pmc331.htm&#34;&gt;rule of thumb&lt;/a&gt; says for the approximation to be good:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; should be equal to or larger than 20 and the probability of a single success, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, should be smaller than or equal to 0.05. If &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; &amp;gt; 100, the approximation is excellent if &lt;span class=&#34;math inline&#34;&gt;\(np\)&lt;/span&gt; is also &amp;lt; 10.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let’s try a few scenarios. I have slightly modified the code from &lt;a href=&#34;https://www.math.utah.edu/~treiberg/M3074PoisApproxEg.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plots the pmfs of Binomial and Poisson
pl &amp;lt;- function(n, p, a, b) {
   
   clr &amp;lt;- rainbow(15)[ceiling(c(10.68978, 14.24863))]
   lambda &amp;lt;- n * p
   mx &amp;lt;- max(dbinom(a:b, n, p))
      
   plot(
      c(a:b, a:b),
      c(dbinom(a:b, n, p), dpois(a:b, lambda)),
      type = &amp;quot;n&amp;quot;,
      main = paste(&amp;quot;Poisson Approx. to Binomial, n=&amp;quot;, n, &amp;quot;, p=&amp;quot;, p, &amp;quot;, lambda=&amp;quot;, lambda),
      ylab = &amp;quot;Probability&amp;quot;,
      xlab = &amp;quot;x&amp;quot;)
   points((a:b) - .15,
          dbinom(a:b, n, p),
          type = &amp;quot;h&amp;quot;,
          col = clr[1],
          lwd = 10)
   points((a:b) + .15,
          dpois(a:b, lambda),
          type = &amp;quot;h&amp;quot;,
          col = clr[2],
          lwd = 10)
   legend(b - 3.5, mx, legend = c(&amp;quot;Binomial(x,n,p)&amp;quot;, &amp;quot;Poisson(x,lambda)&amp;quot;), fill = clr, bg = &amp;quot;white&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I start with the recommendation: &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; = 20, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; = 0.05. This gives &lt;span class=&#34;math inline&#34;&gt;\(\lambda= 1\)&lt;/span&gt;. Already the approximation seems reasonable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pl(20, 0.05, 0, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://solon-karapanagiotis.com/post/approx_binomial/approximating-binomial-with-poisson_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; = 10, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; = 0.3 it doesn’t seem to work very well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pl(10, 0.3, 0, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://solon-karapanagiotis.com/post/approx_binomial/approximating-binomial-with-poisson_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But if we increase &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and decrease &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; in order to come home with the same &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; value things improve.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pl(100, 0.03, 0, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://solon-karapanagiotis.com/post/approx_binomial/approximating-binomial-with-poisson_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;
Lastly, for 1000 trials the distributions are indistinguishable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pl(1000, 0.003, 0, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://solon-karapanagiotis.com/post/approx_binomial/approximating-binomial-with-poisson_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-casella2002statistical&#34;&gt;
&lt;p&gt;Casella, George, and Roger L Berger. 2002. &lt;em&gt;Statistical Inference&lt;/em&gt;. Vol. 2. Duxbury Pacific Grove, CA.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Another solution to the &#39;The Hardest Logic Puzzle Ever&#39; using probability</title>
      <link>https://solon-karapanagiotis.com/post/hardest_puzzle/the-hardest-logic-puzzle/</link>
      <pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://solon-karapanagiotis.com/post/hardest_puzzle/the-hardest-logic-puzzle/</guid>
      <description>


&lt;p&gt;I present a solution to a modification of the “hardest logic puzzle ever” using probability theory.&lt;/p&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;“The hardest logic puzzle” was originally presented by &lt;span class=&#34;citation&#34;&gt;Boolos (&lt;a href=&#34;#ref-boolos1996hardest&#34; role=&#34;doc-biblioref&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt; and since then it has been amended several times in order to make it harder &lt;span class=&#34;citation&#34;&gt;(see Rabern and Rabern &lt;a href=&#34;#ref-rabern2008simple&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;, @novozhilov2012hardest)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The puzzle: &lt;em&gt;Three gods A, B, and C are called, in some order, True, False, and Random. True always speaks truly, False always speaks falsely, but whether Random speaks truly or falsely is a completely random matter. Your task is to determine the identities of A, B, and C by asking three yes-no questions; &lt;strong&gt;each question must be put to exactly one god&lt;/strong&gt;. The gods understand English, but will answer all questions in their own language, in which the words for “yes” and “no” are “da” and “ja,” in some order. You do not know which word means which. &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Boolos (&lt;a href=&#34;#ref-boolos1996hardest&#34; role=&#34;doc-biblioref&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt; then provides the following guidelines:&lt;br /&gt;
1. It could be that some god gets asked more than one question (and hence that some god is not asked any question at all).&lt;br /&gt;
2. What the second question is, and to which god it is put, may depend on the answer to the first question. (And of course similarly for the third question.)&lt;br /&gt;
3. Whether Random speaks truly or not should be thought of as depending on the flip of a coin hidden in his brain: if the coin comes down heads, he speaks truly; if tails, falsely.&lt;br /&gt;
4. Random will answer da or ja when asked any yes-no question.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Rabern and Rabern (&lt;a href=&#34;#ref-rabern2008simple&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt; proposed to modify the third point above with the following:
“Whether Random answers ‘da’ or ‘ja’ should be thought of as depending on the flip of a coin hidden in his
brain: if the coin comes down heads, he answers ‘yes’; if tails, ‘no’.”&lt;/p&gt;
&lt;p&gt;Boolos’ article includes multiple ways of solving the problem. &lt;span class=&#34;citation&#34;&gt;Rabern and Rabern (&lt;a href=&#34;#ref-rabern2008simple&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt; give a simpler solution. The main ideas for the solutions can be found &lt;a href=&#34;https://www.technologyreview.com/s/428189/the-hardest-logic-puzzle-ever-made-even-harder/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://nautil.us/issue/30/identity/how-to-solve-the-hardest-logic-puzzle-ever&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My solution&lt;/h2&gt;
&lt;p&gt;My solution is based on the long-run frequency interpretation of probability. It involves two steps. At the first step we will identify the Random god and in step 2 we distinguish between the True and False gods.&lt;/p&gt;
&lt;p&gt;Step 1&lt;/p&gt;
&lt;p&gt;Imagine the following scenario: you keep asking the same question to each god. The question is different for each god. Under the interpretation of probability as long-run frequency both True and False will always give the same answer. For example, if A is the True god he will always answer “da” or “ja” and similarly the False god will always answer the opposite. The crucial point is that Random will change between “da” and “ja” because his answers are random, “they depend on the flip of a coin hidden in his brain”. Suppose you ask your question to Random ten times, and assuming the coin in his head is fair (i.e., P(heads) = P(tails) = 0.5) then the probability that all his answers are same ( “da” or “ja” ) is &lt;span class=&#34;math inline&#34;&gt;\(0.5^{10}\)&lt;/span&gt;, that is highly unlikely. In fact, you do not need to pre-specify how many times you ask the question, since the moment a given god switches from “da” to “ja” or vice-versa you know he is Random. Having identified Random we proceed to distinguish between True and False. An example question to each one is “are you True”?&lt;/p&gt;
&lt;p&gt;Step 2&lt;/p&gt;
&lt;p&gt;For simplicity let’s assume C is Random. Now, we only need to identify one more god. Let’s use god A for illustration. All possibilities regarding god A and the word “da” are given below:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A is True and “da” means “yes”,&lt;/li&gt;
&lt;li&gt;A is True and “da” means “no”,&lt;/li&gt;
&lt;li&gt;A is False and “da” means “yes”,&lt;/li&gt;
&lt;li&gt;A is False and “da” means “no”,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then ask A the following question:&lt;/p&gt;
&lt;p&gt;Q1: Is C Random?&lt;/p&gt;
&lt;p&gt;And B:&lt;/p&gt;
&lt;p&gt;Q2: Is A True?&lt;/p&gt;
&lt;p&gt;For each scenario above we end up with the following pattern:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If scenario (1) the answers are “da” and “ja” for Q1 and Q2 respectively.&lt;/li&gt;
&lt;li&gt;If scenario (2) the answers are “ja” and “da”.&lt;/li&gt;
&lt;li&gt;If scenario (3) the answers are “ja” and “da”.&lt;/li&gt;
&lt;li&gt;If scenario (4) the answers are “da” and “da”.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Looking more carefully at the answers we distinguish 3 distinct patterns for the answers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P1: “da”and “ja”,&lt;/li&gt;
&lt;li&gt;P2: “ja” and “da” and&lt;/li&gt;
&lt;li&gt;P3: “da” and “da”.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Furthermore, P1 and P3 are unique, they appear only once. That means if the gods answer Q1 and Q2 using P1 or P3 we have identified them and the game is over! For example, if they answer with P3 then scenario (4) was correct: A is False and “da” means “no” and consequently B is True and “ja” means “yes”. If they answer using P2 then we need a further question because both scenarios (2) and (3) may be right. We can ask A: (repeat the 1st question)&lt;/p&gt;
&lt;p&gt;Q3: Are you True?&lt;/p&gt;
&lt;p&gt;Now, if scenario (2) the answer is “ja” and if scenario (3) the answer is “da”.&lt;/p&gt;
&lt;p&gt;Using this approach we have also identified the meanings of “da” and “ja”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comments&lt;/h2&gt;
&lt;p&gt;The modification I used was allowing each question to be put to more than one god. In step 1 the question “Are you True?” was put to all gods and was repeated in step 2 as Q3. So technically I have solved the puzzle using only three questions in total, but allowing myself to repeat the same questions to more than one god.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Boolos (&lt;a href=&#34;#ref-boolos1996hardest&#34; role=&#34;doc-biblioref&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt; provided his solution in the same article in which he introduced the puzzle.
He states that the “first move is to find a god that you can be certain is not Random, and hence is either True or False”. My approach does the reverse; first identifies the Random god and then the True and False gods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-boolos1996hardest&#34;&gt;
&lt;p&gt;Boolos, George. 1996. “The Hardest Logic Puzzle Ever.” &lt;em&gt;The Harvard Review of Philosophy&lt;/em&gt; 6 (1): 62–65.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-novozhilov2012hardest&#34;&gt;
&lt;p&gt;Novozhilov, Nikolay. 2012. “The Hardest Logic Puzzle Ever Becomes Even Tougher.” &lt;em&gt;arXiv Preprint arXiv:1206.1926&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rabern2008simple&#34;&gt;
&lt;p&gt;Rabern, Brian, and Landon Rabern. 2008. “A Simple Solution to the Hardest Logic Puzzle Ever.” &lt;em&gt;Analysis&lt;/em&gt; 68 (2): 105–12.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Plastic waste and disease on coral reefs - Another misinterpretation of a statistical model</title>
      <link>https://solon-karapanagiotis.com/post/reefs/plastic-waste-and-disease-on-coral-reefs/</link>
      <pubDate>Sun, 20 May 2018 00:00:00 +0000</pubDate>
      <guid>https://solon-karapanagiotis.com/post/reefs/plastic-waste-and-disease-on-coral-reefs/</guid>
      <description>


&lt;p&gt;Recently, I came across this very interesting article published in &lt;a href=&#34;http://science.sciencemag.org/content/359/6374/460.long&#34;&gt;Science&lt;/a&gt; about how plastic waste is associated with disease on coral reefs &lt;span class=&#34;citation&#34;&gt;(J. B. Lamb et al. &lt;a href=&#34;#ref-lamb2018plastic&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;. The main conclusions are&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;contact with plastic increases the probability of disease,&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;the morphological structure of the reefs is associated with the probability of being in contact with plastic with more complex ones being more likely to be affected by plastic,&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;the plastic levels correspond to estimates of mismanaged plastic waste into the ocean.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Overall, this study provides evidence how plastic waste negatively affects coral reefs, making them more susceptible to diseases. The authors made available both the datasets they used and the code &lt;span class=&#34;citation&#34;&gt;(both can be downloaded from J. Lamb et al. &lt;a href=&#34;#ref-dryad_mp480&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt; - an excellent example of reproducible research)&lt;/span&gt;. The methods section is straightforward to follow (see &lt;a href=&#34;http://science.sciencemag.org/content/suppl/2018/01/24/359.6374.460.DC1?_ga=2.198123375.1394041835.1523546630-1357771364.1523546630&#34;&gt;Supplementary Materials&lt;/a&gt;). My comment is about the 2nd point above, and more specifically the methodology that led to this conclusion (see Fig. 4 of the article). The issue is the authors interpret the models they are using wrongly. Let me explain …&lt;/p&gt;
&lt;p&gt;Their model is a simple generalised linear mixed model (GLMM) - binomial error distribution and logistic link. The outcome is the disease prevalence (binary) among coral reefs with different morphology. The morphology assignments were massive, tabular, and branching (3-level categorical covariate). The morphological assignments were treated as fixed factors and the site as random (in order to take into account the correlation between reefs due to their geographical position). The model is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ logit(Disease Presense_{ik}) =  \sum_j \beta_j x_{ik} + b_i \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(j_{1:3} = \{massive, tabular, branching\}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_i\)&lt;/span&gt; are the reef-specific intercepts. Such a &lt;span class=&#34;math inline&#34;&gt;\(b_i\)&lt;/span&gt; represents the deviation of the intercept of a specific reef from the
average intercept in the group to which that reef belongs, i.e deviation from &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt;. The model is fitted only for reefs unaffected by plastic waste. The output is given in Fig. 4(B) in the paper. The conclusion is the disease risk increases from massive to branching and tabular reefs when not in contact with plastic debris (Fig. 4(B) and table S13).&lt;/p&gt;
&lt;p&gt;The issue with this figure is the authors give a population-average interpretation of the coefficients. In GLLMs the fixed effects have a site-specific interpretation but not a
population-average one. Let us now consider the logistic random-intercepts model above. The conditional means &lt;span class=&#34;math inline&#34;&gt;\(E[Disease Presense_{ik}|b_i]\)&lt;/span&gt; are given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ E[Disease Presense_{ik}|b_i] = \frac{\exp(\sum_j \beta_j x_{ik} + b_i)}{1 + \exp(\sum_j \beta_j x_{ik} + b_i)} \]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(E[.]\)&lt;/span&gt; is the expectation operator. The above model assumes logistic change in prevalence of disease for each morphology, all having different intercepts &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + b_i\)&lt;/span&gt;. The average reef, i.e, the reef with intercept &lt;span class=&#34;math inline&#34;&gt;\(b_i = 0\)&lt;/span&gt;, has disease probability given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ E[Disease Presense_{ik}|b_i = 0] = \frac{\exp(\sum_j \beta_j x_{ik} + 0)}{1 + \exp(\sum_j \beta_j x_{ik} + 0)} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is what the authors have calculated and produced Fig. 4. In other words, the authors have calculated the probability of disease for an “average” reef. They proceed interpreting this as marginal effect, which is wrong.&lt;/p&gt;
&lt;p&gt;The issue arises due to the conditional interpretation, conditionally upon level of random effects, of the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;s in a GLMM model. And this is due to the fact that &lt;span class=&#34;math inline&#34;&gt;\(E[g(Y )] \neq g[E(Y)]\)&lt;/span&gt; unless &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is linear, which is not the case for this model. In what follows I fit the same model and demonstrate how the conclusions change when conditioning of different levels of the random coefficients. The code the authors use is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# GLMM, Baseline Disease levels for different growth forms, Asia Pacific --------
library(lme4)
Normal.Disease.Growth = glmer(Disease ~ -1 + Growth2+(1|Reef_Name), 
                              data = Plastic[which(Plastic$Plastic==0),], 
                              family = &amp;#39;binomial&amp;#39;, 
                              control = glmerControl(optimizer =&amp;quot;bobyqa&amp;quot;))

# As a sidenote: This code uses a Laplace approximation (nAGQ = 1 - the default) on the integral over the random effects space. &amp;quot;Values greater than 1 produce greater accuracy in the evaluation of the log-likelihood at the expense of speed&amp;quot;. The authors of the package suggest values up to 25 (see the documentation). &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following reproduces Fig. 4(B) of the publication.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# PDF, Baseline disease levels by growth form -----------------------------
NormalDisease.by.Growth = data.frame(Tabular = rnorm(100000, mean = -3.1332, sd = .1549),
                                     Massive = rnorm(100000, mean = -3.8153, sd = .1095),
                                     Branching = rnorm(100000, mean = -3.5534, sd = .1103))
NormalDisease.by.Growth$Tabularbt = plogis(NormalDisease.by.Growth$Tabular)
NormalDisease.by.Growth$Massivebt = plogis(NormalDisease.by.Growth$Massive)
NormalDisease.by.Growth$Branchingbt = plogis(NormalDisease.by.Growth$Branching)
NormalDisease.by.Growth = gather(NormalDisease.by.Growth, Growth, Estimate, Tabularbt:Branchingbt)

library(ggplot2)
ggplot(aes(x = Estimate*100), data = NormalDisease.by.Growth) +
   geom_density(aes(y = ..scaled.., fill = Growth)) +
   scale_x_continuous(limits = c(0, 10)) + 
   ylab(&amp;quot;&amp;quot;) + 
   labs(fill = &amp;quot;Morphology&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://solon-karapanagiotis.com/post/reefs/2018-03-26-plastic-waste-and-disease-on-coral-reefs_files/figure-html/plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is evident from the code that they plot the fixed effects estimates with their standard errors. This plot ignores the random effects and it only takes into consideration the variation of the fixed coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt;. To get an idea for the variability of the random effects I simulate them from the model and plot them. Points that are distinguishable from zero (i.e. the confidence band based on level does not cross the red line) are highlighted. We see substantial variation on the random effects estimates with many “outliers” with both high and low averages that need to be accounted for.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(merTools)
sim_rfs_Normal.Disease &amp;lt;- REsim(Normal.Disease.Growth, n.sims = 200) 
plotREsim(sim_rfs_Normal.Disease)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://solon-karapanagiotis.com/post/reefs/2018-03-26-plastic-waste-and-disease-on-coral-reefs_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What the authors are effectively doing in Fig. 4(B) (see density plot above) is presenting the results for reefs with &lt;span class=&#34;math inline&#34;&gt;\(b_i = 0\)&lt;/span&gt; which corresponds to the red horizontal line. Let’s see how the density plot changes when we condition on more “extreme” reefs. I use the 0.1 and 0.9 quantiles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile0.9 &amp;lt;- REquantile(Normal.Disease.Growth, quantile = 0.9, groupFctr = &amp;quot;Reef_Name&amp;quot;)
#which(sim_rfs_Normal.Disease$groupID == quantile0.9)

quantile0.1 &amp;lt;- REquantile(Normal.Disease.Growth, quantile = 0.1, groupFctr = &amp;quot;Reef_Name&amp;quot;)
#which(sim_rfs_Normal.Disease$groupID == quantile0.1)

NormalDisease.by.Growth_quantile0.9 = data.frame(
   Tabular = rnorm(100000, mean = -3.1332 + 1.495773, sd = .1549),
   Massive = rnorm(100000, mean = -3.8153 + 1.495773, sd = .1095),
   Branching = rnorm(100000, mean = -3.5534 + 1.495773, sd = .1103))
NormalDisease.by.Growth_quantile0.9$Tabularbt = plogis(NormalDisease.by.Growth_quantile0.9$Tabular)
NormalDisease.by.Growth_quantile0.9$Massivebt = plogis(NormalDisease.by.Growth_quantile0.9$Massive)
NormalDisease.by.Growth_quantile0.9$Branchingbt = plogis(NormalDisease.by.Growth_quantile0.9$Branching)
NormalDisease.by.Growth_quantile0.9 = gather(NormalDisease.by.Growth_quantile0.9, Growth, Estimate, Tabularbt:Branchingbt)
NormalDisease.by.Growth_quantile0.1 = data.frame(
   Tabular = rnorm(100000, mean = -3.1332 - 1.689569, sd = .1549),
   Massive = rnorm(100000, mean = -3.8153 - 1.689569, sd = .1095),
   Branching = rnorm(100000, mean = -3.5534 - 1.689569, sd = .1103))

NormalDisease.by.Growth_quantile0.1$Tabularbt = plogis(NormalDisease.by.Growth_quantile0.1$Tabular)
NormalDisease.by.Growth_quantile0.1$Massivebt = plogis(NormalDisease.by.Growth_quantile0.1$Massive)
NormalDisease.by.Growth_quantile0.1$Branchingbt = plogis(NormalDisease.by.Growth_quantile0.1$Branching)
NormalDisease.by.Growth_quantile0.1 = gather(NormalDisease.by.Growth_quantile0.1, Growth, Estimate, Tabularbt:Branchingbt)

NormalDisease.by.Growth$ID = &amp;quot;average&amp;quot;
NormalDisease.by.Growth_quantile0.1$ID = &amp;quot;0.1quantile&amp;quot;
NormalDisease.by.Growth_quantile0.9$ID = &amp;quot;0.9quantile&amp;quot;

overall &amp;lt;- rbind(NormalDisease.by.Growth, NormalDisease.by.Growth_quantile0.1, NormalDisease.by.Growth_quantile0.9)
ggplot(aes(x = Estimate*100, col = ID), data = overall) +
   geom_density(aes(y = ..scaled.., fill = Growth), alpha = 0.9, size = 1.3) +
   scale_fill_brewer(palette = &amp;quot;Spectral&amp;quot;) + 
   #scale_fill_manual(values = c(&amp;quot;#D55E00&amp;quot;, &amp;quot;#009E73&amp;quot;, &amp;quot;#0072B2&amp;quot;)) + 
   scale_color_manual(values = c(&amp;quot;#000000&amp;quot;, &amp;quot;dodgerblue&amp;quot;, &amp;quot;darkmagenta&amp;quot;)) +
   scale_x_continuous(limits = c(0, 10)) + 
   ylab(&amp;quot;&amp;quot;) + 
   labs(col = &amp;quot;R effect&amp;quot;, fill = &amp;quot;Morphology&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://solon-karapanagiotis.com/post/reefs/2018-03-26-plastic-waste-and-disease-on-coral-reefs_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is evident both the center and the variability of the distributions change depending whether we look an “average” coral reef (purple line), a reef towards the upper extreme (blue line) or the lower extreme (black line). So the conclusions should be something along the lines: the increase in disease likelihood with plastic debris depends also on inherit/unobserved characteristics of the reefs, captured by the random effects, in addition to their morphology.&lt;/p&gt;
&lt;p&gt;Of course, what I have presented above is still conditional interpretation of the parameters. Ideally, we want the marginal population-average interpretation which is obtained from averaging over the random effects. This allows to take into account both the residual (observation-level) variance, the uncertainty in the variance parameters for the grouping factors added to the uncertainty in the fixed coefficients. See for example the &lt;code&gt;predictInterval()&lt;/code&gt; function of the &lt;a href=&#34;https://cran.r-project.org/web/packages/merTools/merTools.pdf&#34;&gt;&lt;code&gt;merTools&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-dryad_mp480&#34;&gt;
&lt;p&gt;Lamb, JB, BL Willis, EA Fiorenza, CS Couch, R Howard, DN Rader, JD True, et al. 2018. “Data from: Plastic Waste Associated with Disease on Coral Reefs.” &lt;em&gt;Science&lt;/em&gt;. Dryad Digital Repository. &lt;a href=&#34;https://doi.org/10.5061/dryad.mp480&#34;&gt;https://doi.org/10.5061/dryad.mp480&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lamb2018plastic&#34;&gt;
&lt;p&gt;Lamb, Joleah B, Bette L Willis, Evan A Fiorenza, Courtney S Couch, Robert Howard, Douglas N Rader, James D True, et al. 2018. “Plastic Waste Associated with Disease on Coral Reefs.” &lt;em&gt;Science&lt;/em&gt; 359 (6374): 460–62.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>On statistical reporting in biomedical journals</title>
      <link>https://solon-karapanagiotis.com/post/stat_reporting/on-statistical-reporting-in-biomedical-journals/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      <guid>https://solon-karapanagiotis.com/post/stat_reporting/on-statistical-reporting-in-biomedical-journals/</guid>
      <description>


&lt;p&gt;Poor quality statistical reporting in the biomedical literature is not uncommon. Here is another example by &lt;span class=&#34;citation&#34;&gt;Cirio et al. (&lt;a href=&#34;#ref-cirio2016effects&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;. The study itself is well planed, executed and reported.
The aim was to assess whether heated and humidified high flow gases delivered through nasal cannula (HFNC) improve exercise performance in severe chronic obstructive pulmonary disease (COPD) patients. It all started when I saw their Fig.1. Here is my attempt to reproduce it&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:figs&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://solon-karapanagiotis.com/post/stat_reporting/2018-04-25-on-statistical-reporting-in-biomedical-journals_files/figure-html/figs-1.png&#34; alt=&#34;Effect of the HFNC on exercise capacity compared to a control condition. Tlim = exercise duration.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Effect of the HFNC on exercise capacity compared to a control condition. Tlim = exercise duration.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In total there are 12 patients tested twice; once under the control test and once under the HFNC test. The outcome of interest is the endurance time (Tlim; y axis). This is practically how long each test lasted. The authors hypothesized that HFNC would improve exercise performance, that is the test would last longer. This was the case since Tlim increased for all subjects under the HFNC test (see figure &lt;a href=&#34;#fig:figs&#34;&gt;1&lt;/a&gt;). Moreover, this increase reached statistical significance (p-value = 0.015) - ready to publish! Looking at the plot I was pondered about the “outlying” patient (red dot). His/her Tlim increased by a whooping 400 seconds! This is huge compared to the other patients. Then I wondered how would the results change if we excluded him/her from the analysis? And here is where the problems start.&lt;/p&gt;
&lt;p&gt;There is no way from the text to figure out which test was used to produce the p-value of 0.015. Is is a paired t-test or a Wilcoxon test? (they mention both in the statistical analysis section). So it is impossible to evaluate and/or try to reproduce the results.&lt;/p&gt;
&lt;p&gt;Having abandoned the idea of being able to reproduce the analysis I started thinking about reporting guidelines, hence the title of this post. I thought the journal must have guidelines for reporting statistical analysis. No, it does not and unfortunately, most of the biomedical journals don’t have such guidelines even though 40 years ago O’Fallon and colleges recommended that “Standards governing the content and format of statistical aspects should be developed to guide authors in the preparation of manuscripts” &lt;span class=&#34;citation&#34;&gt;(O’Fallon et al. &lt;a href=&#34;#ref-o1978should&#34; role=&#34;doc-biblioref&#34;&gt;1978&lt;/a&gt;)&lt;/span&gt;. Since then many have repeated the message. A few sporadic attempts are usually editorials such as &lt;span class=&#34;citation&#34;&gt;Cummings and Rivara (&lt;a href=&#34;#ref-cummings2003reporting&#34; role=&#34;doc-biblioref&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Curran-Everett and Benos (&lt;a href=&#34;#ref-curran2004guidelines&#34; role=&#34;doc-biblioref&#34;&gt;2004&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Arifin et al. (&lt;a href=&#34;#ref-arifin2016reporting&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Recently, &lt;span class=&#34;citation&#34;&gt;Lang and Altman (&lt;a href=&#34;#ref-lang2013basic&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; published a comprehensive set of statistical reporting guidelines suitable for medical journals - the SAMPL guidelines. “The SAMPL guidelines are designed to be included in a journal’s Instructions for Authors”. So the journals just need to refer to them! As there are many general reporting guidelines based on the study design as such CONSORT, STROBE, PRISMA etc (see &lt;a href=&#34;http://www.equator-network.org/&#34; class=&#34;uri&#34;&gt;http://www.equator-network.org/&lt;/a&gt;) that authors in many journals must adhere to, I believe the SAMPL guidelines is a big step forward on reporting statistics. The only journal (that I know of) that suggests the use of the SAMPL guidelines is the British Journal of Dermatology &lt;span class=&#34;citation&#34;&gt;(Hollestein and Nijsten &lt;a href=&#34;#ref-hollestein2015guidelines&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;. (I’ll keep adding to this list).&lt;/p&gt;
&lt;p&gt;Now that the guidelines exist, let’s make use of them.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-arifin2016reporting&#34;&gt;
&lt;p&gt;Arifin, Wan Nor, Abdullah Sarimah, Bachok Norsa’adah, Yaacob Najib Majdi, Ab Hamid Siti-Azrin, Musa Kamarul Imran, Abd Aziz Aniza, and Lin Naing. 2016. “Reporting Statistical Results in Medical Journals.” &lt;em&gt;The Malaysian Journal of Medical Sciences: MJMS&lt;/em&gt; 23 (5): 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cirio2016effects&#34;&gt;
&lt;p&gt;Cirio, Serena, Manuela Piran, Michele Vitacca, Giancarlo Piaggi, Piero Ceriana, Matteo Prazzoli, Mara Paneroni, and Annalisa Carlucci. 2016. “Effects of Heated and Humidified High Flow Gases During High-Intensity Constant-Load Exercise on Severe Copd Patients with Ventilatory Limitation.” &lt;em&gt;Respiratory Medicine&lt;/em&gt; 118: 128–32.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cummings2003reporting&#34;&gt;
&lt;p&gt;Cummings, Peter, and Frederick P Rivara. 2003. “Reporting Statistical Information in Medical Journal Articles.” &lt;em&gt;Archives of Pediatrics &amp;amp; Adolescent Medicine&lt;/em&gt; 157 (4): 321–24.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-curran2004guidelines&#34;&gt;
&lt;p&gt;Curran-Everett, Douglas, and Dale J Benos. 2004. “Guidelines for Reporting Statistics in Journals Published by the American Physiological Society.” Am Physiological Soc.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hollestein2015guidelines&#34;&gt;
&lt;p&gt;Hollestein, LM, and Tamar Nijsten. 2015. “Guidelines for Statistical Reporting in the British Journal of Dermatology.” &lt;em&gt;British Journal of Dermatology&lt;/em&gt; 173 (1): 3–5.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lang2013basic&#34;&gt;
&lt;p&gt;Lang, Thomas A, and Douglas G Altman. 2013. “Basic Statistical Reporting for Articles Published in Biomedical Journals: The ‘Statistical Analyses and Methods in the Published Literature’ or the Sampl Guidelines”.” &lt;em&gt;Handbook, European Association of Science Editors&lt;/em&gt; 256: 256.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-o1978should&#34;&gt;
&lt;p&gt;O’Fallon, JR, SD Dubey, DS Salsburg, JH Edmonson, A Soffer, and T Colton. 1978. “Should There Be Statistical Guidelines for Medical Research Papers?” &lt;em&gt;Biometrics&lt;/em&gt;, 687–95.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Importing Flat Files Into R</title>
      <link>https://solon-karapanagiotis.com/post/importflatfiles/importing-flat-files-into-r/</link>
      <pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://solon-karapanagiotis.com/post/importflatfiles/importing-flat-files-into-r/</guid>
      <description>


&lt;p&gt;There are many tutorials for importing data into R focusing on a specific function/package. This one focuses on 3 different packages. You will learn how to import all common formats of flat file data with base R functions and the dedicated &lt;code&gt;readr&lt;/code&gt; and &lt;code&gt;data.table&lt;/code&gt; packages. I first present these three packages and finish with a comparison table between them.&lt;/p&gt;
&lt;div id=&#34;task&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Task&lt;/h2&gt;
&lt;p&gt;Import a flat file into R: create an R object that contains the data from a flat file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-a-flat-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is a flat file?&lt;/h2&gt;
&lt;p&gt;A flat file can be a plain text file that contains table data. A form of flat file is one in which table data is gathered in lines with the value from each table cell separated by a comma and each row represented with a new line. This type of flat file is also known as a comma-separated values (CSV) file. An alternative is a tab-delimited file where each field value is separated from the next using tabs.&lt;/p&gt;
&lt;p&gt;The following sections describe various options for importing flat files. The ultimate goal is to convey, “translate”, them into an R &lt;strong&gt;data.frame&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-are-we-going-to-import&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What are we going to import?&lt;/h2&gt;
&lt;p&gt;For illustration purposes we use the &lt;a href=&#34;http://perso.telecom-paristech.fr/~eagan/class/as2013/inf229/labs/datasets&#34;&gt;Happiness&lt;/a&gt; dataset. It is based on the European quality of life survey with questions related to income, life satisfaction or perceived quality of society. The file is quite small but enough to sharpen your importing skills. It provides the average rating for the question “How happy would you say you are these days?”. Rating 1 (low) to 10 (high) by country and gender.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Country Gender Mean   N.
## 1       AT   Male  7.3  471
## 2          Female  7.3  570
## 3            Both  7.3 1041
## 4       BE   Male  7.8  468
## 5          Female  7.8  542
## 6            Both  7.8 1010
## 7       BG   Male  5.8  416
## 8          Female  5.8  555
## 9            Both  5.8  971
## 10      CY   Male  7.8  433&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-get-going-the-utils&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let’s get going… the &lt;code&gt;utils&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;We start with the &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/&#34;&gt;&lt;code&gt;utils&lt;/code&gt;&lt;/a&gt; package. This package is loaded by default when you start your R session. This means that you can access its functions without further due. Here, we are interested in three of them:
&lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.table()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.csv()&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.delim()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;reading-data-with-read.table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reading data with &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.table()&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Reads a file in table format and creates an R &lt;strong&gt;data.frame&lt;/strong&gt; from it, with cases corresponding to rows and variables to columns. Let’s see how it works for our dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;happiness &amp;lt;- read.table(&amp;quot;happiness.csv&amp;quot;)
head(happiness)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       V1
## 1 Country,Gender,Mean,N=
## 2        AT,Male,7.3,471
## 3        ,Female,7.3,570
## 4         ,Both,7.3,1041
## 5        BE,Male,7.8,468
## 6        ,Female,7.8,542&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not what we wanted?! This data frame contains 108 rows and 1 column instead of 105 rows and 4 columns. That’s because additional arguments need to be specified in order to tell R what it has to deal with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;happiness &amp;lt;- read.table(file = &amp;quot;happiness.csv&amp;quot;,     # path to flat file 
                        header = TRUE,              # first row lists variables&amp;#39; names
                        sep = &amp;quot;,&amp;quot;,                  # field separator is a comma
                        stringsAsFactors = FALSE)   # not import strings as categorical variables&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look now&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(happiness)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Country Gender Mean   N.
## 1      AT   Male  7.3  471
## 2         Female  7.3  570
## 3           Both  7.3 1041
## 4      BE   Male  7.8  468
## 5         Female  7.8  542
## 6           Both  7.8 1010&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(happiness)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    105 obs. of  4 variables:
##  $ Country: chr  &amp;quot;AT&amp;quot; &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;BE&amp;quot; ...
##  $ Gender : chr  &amp;quot;Male&amp;quot; &amp;quot;Female&amp;quot; &amp;quot;Both&amp;quot; &amp;quot;Male&amp;quot; ...
##  $ Mean   : num  7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ...
##  $ N.     : int  471 570 1041 468 542 1010 416 555 971 433 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By specifying &lt;code&gt;header = TRUE&lt;/code&gt; R sees the that the first line contains the names of the variables. With &lt;code&gt;stringsAsFactors = FALSE&lt;/code&gt; we specify that we wanted &lt;code&gt;Country&lt;/code&gt; and &lt;code&gt;Gender&lt;/code&gt; to be character variables. The &lt;code&gt;sep = &#34;,&#34;&lt;/code&gt; identifies the field separator to be a comma. There are many more arguments you can specify and each one can take many values!
For further details, consult the &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;R documentation&lt;/a&gt; or type &lt;code&gt;help(read.table)&lt;/code&gt; on the console.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: In order to use &lt;code&gt;read.table()&lt;/code&gt;, in same manner, you need to give the full path name of the target file if it’s not in your working directory. You can use the &lt;a href=&#34;http://rfunction.com/archives/1001&#34;&gt;R Function of the Day&lt;/a&gt;, namely &lt;code&gt;setwd(&#34;&amp;lt;location of your dataset&amp;gt;&#34;)&lt;/code&gt;, to change your working directory. The same is valid for any other function we are going to encounter in this tutorial. Alternatively, you can specify the location of the flat file inside &lt;code&gt;read.table()&lt;/code&gt;. Keep in mind that the specification of the file is platform dependent (Windows, Unix/Linux and OSX).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read.table(file = &amp;quot;&amp;lt;location of your dataset&amp;gt;&amp;quot;, ...) &lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Another option is to use &lt;a href=&#34;http://www.rdocumentation.org/packages/base/functions/file.path&#34;&gt;&lt;code&gt;file.path()&lt;/code&gt;&lt;/a&gt;. It constructs the path to a file from components in a platform-independent way.
For example,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path &amp;lt;- file.path(&amp;quot;~&amp;quot;, &amp;quot;datasets&amp;quot;, &amp;quot;happiness.csv&amp;quot;)     

happiness &amp;lt;- read.table(file = path,    
                        header = TRUE,             
                        sep = &amp;quot;,&amp;quot;,                  
                        stringsAsFactors = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Comment&lt;/strong&gt;
The &lt;code&gt;stringsAsFactors&lt;/code&gt; argument is true by default which means that character variables are imported into R as factors, the data type to store categorical variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;happiness_2 &amp;lt;- read.table(file = &amp;quot;happiness.csv&amp;quot;,    
                        header = TRUE,             
                        sep = &amp;quot;,&amp;quot;,                  
                        stringsAsFactors = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At first sight you do not notice anything different and you shouldn’t! But for R it’s a big deal! For character variables each element is a string of one or more characters. On the other hand, factor variables are stored, internally, as numeric variables together with their levels. This has major impact in computations that R maybe has to carry out later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(happiness_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    105 obs. of  4 variables:
##  $ Country: Factor w/ 36 levels &amp;quot;&amp;quot;,&amp;quot;AT&amp;quot;,&amp;quot;BE&amp;quot;,&amp;quot;BG&amp;quot;,..: 2 1 1 3 1 1 4 1 1 6 ...
##  $ Gender : Factor w/ 3 levels &amp;quot;Both&amp;quot;,&amp;quot;Female&amp;quot;,..: 3 2 1 3 2 1 3 2 1 3 ...
##  $ Mean   : num  7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ...
##  $ N.     : int  471 570 1041 468 542 1010 416 555 971 433 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-data-with-read.csv&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reading data with &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.csv()&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It is a wrapper around &lt;code&gt;read.table()&lt;/code&gt;. This means that &lt;code&gt;read.csv()&lt;/code&gt; calls &lt;code&gt;read.table()&lt;/code&gt; behind the scenes but with different default arguments. More specifically, the defaults are &lt;code&gt;header = TRUE&lt;/code&gt; and &lt;code&gt;sep = &#34;,&#34;&lt;/code&gt;. These match with the standardized CSV format, where &lt;code&gt;,&lt;/code&gt; is used as a separator and usually the first line contains the names of the columns. Therefore, it saves you time since you need to specify less arguments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read.csv(file = &amp;quot;happiness.csv&amp;quot;,   
                        stringsAsFactors = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is equivalent to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read.table(file = &amp;quot;happiness.csv&amp;quot;,    
                        header = TRUE,             
                        sep = &amp;quot;,&amp;quot;,                  
                        stringsAsFactors = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-data-with-read.delim&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reading data with &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.delim()&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It is also a wrapper of &lt;code&gt;read.table()&lt;/code&gt;. Now the default arguments match with tab-delimited files. More specifically, the defaults are &lt;code&gt;header = TRUE&lt;/code&gt; and &lt;code&gt;sep = &#34;\t&#34;&lt;/code&gt;, since &lt;code&gt;\t&lt;/code&gt; is the field separator in tab-delimited files.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read.delim(file = &amp;quot;happiness.txt&amp;quot;,    
                        stringsAsFactors = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is equivalent to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read.table(file = &amp;quot;happiness.txt&amp;quot;,    
                        header = TRUE,             
                        sep = &amp;quot;\t&amp;quot;,                  
                        stringsAsFactors = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both these functions make our lives easier since less arguments need to be specified.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; Locale differences.
The standard field delimiters for CSV files are commas. On US versions, the comma is set as default for the “List Separator”, which is okay for CSV files. But on European versions this character is reserved as the Decimal Symbol and the “List Separator” is set by default to the semicolon.
Why you should care?&lt;/p&gt;
&lt;p&gt;Suppose you try to import the European CSV version &lt;code&gt;happiness_eu.csv&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(happiness_eu)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Country.Gender.Mean.N.
## 1        AT,Male,7.3,471
## 2        ,Female,7.3,570
## 3         ,Both,7.3,1041
## 4        BE,Male,7.8,468
## 5        ,Female,7.8,542
## 6         ,Both,7.8,1010&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R performs the operation but clearly not the one we wanted. It’s a data frame with 105 rows but a single variable! To deal with such problems you can use the &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.csv2()&lt;/code&gt;&lt;/a&gt; function. The defaults are &lt;code&gt;sep = &#34;;&#34;&lt;/code&gt; and &lt;code&gt;dec = &#34;,&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;happiness_eu &amp;lt;- read.csv2(file = &amp;quot;happiness_eu.csv&amp;quot;,  
                        stringsAsFactors = FALSE)
head(happiness_eu)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Country Gender Mean   N.
## 1      AT   Male  7.3  471
## 2         Female  7.3  570
## 3           Both  7.3 1041
## 4      BE   Male  7.8  468
## 5         Female  7.8  542
## 6           Both  7.8 1010&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, there is &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.delim2()&lt;/code&gt;&lt;/a&gt;. The logic is the same.&lt;/p&gt;
&lt;p&gt;To summarize, the &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.table()&lt;/code&gt;&lt;/a&gt; is to read delimited data files. Some variants are: &lt;code&gt;read.csv()&lt;/code&gt; and &lt;code&gt;read.delim()&lt;/code&gt;, which have different default values and are tailored for CSV and tab-delimited files, respectively.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.csv()&lt;/code&gt;&lt;/a&gt; default values are: &lt;code&gt;header = T&lt;/code&gt;, &lt;code&gt;sep = ???,???&lt;/code&gt;, &lt;code&gt;dec = ???.???&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;In &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.csv2()&lt;/code&gt;&lt;/a&gt; default values are: &lt;code&gt;header = T&lt;/code&gt;, &lt;code&gt;sep = ???;???&lt;/code&gt;, &lt;code&gt;dec = ???,???&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;In &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.delim()&lt;/code&gt;&lt;/a&gt; default values are: &lt;code&gt;header = T&lt;/code&gt;, &lt;code&gt;sep = ???\t???&lt;/code&gt;, &lt;code&gt;dec = ???.???&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;In &lt;a href=&#34;http://www.rdocumentation.org/packages/utils/functions/read.table&#34;&gt;&lt;code&gt;read.delim2()&lt;/code&gt;&lt;/a&gt; default values are: &lt;code&gt;header = T&lt;/code&gt;, &lt;code&gt;sep = ???\t???&lt;/code&gt;, &lt;code&gt;dec = ???,???&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;##&lt;a href=&#34;https://cran.r-project.org/web/packages/readr/readr.pdf&#34;&gt;&lt;code&gt;readr&lt;/code&gt;&lt;/a&gt; … an alternative to import flat files&lt;/p&gt;
&lt;p&gt;An alternative to the &lt;code&gt;utils&lt;/code&gt; package is the &lt;a href=&#34;https://cran.r-project.org/web/packages/readr/readr.pdf&#34;&gt;&lt;code&gt;readr&lt;/code&gt;&lt;/a&gt;. Compared
the &lt;code&gt;read.table&lt;/code&gt; family of functions, it is faster, easier to use and with a consistent naming scheme. We start by installing and loading it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;readr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s import our dataset. In &lt;code&gt;readr&lt;/code&gt; you can use &lt;a href=&#34;https://cran.r-project.org/web/packages/readr/readr.pdf&#34;&gt;&lt;code&gt;read_delim()&lt;/code&gt;&lt;/a&gt; for flat files. It can be considered the correspondent to &lt;code&gt;read.table()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;happiness_readr &amp;lt;- read_delim(&amp;quot;happiness.csv&amp;quot;,        # path to flat file 
                              delim = &amp;quot;,&amp;quot;)            # character that separates fields in the file&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   Country = col_character(),
##   Gender = col_character(),
##   Mean = col_double(),
##   `N=` = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(happiness_readr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [105 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
##  $ Country: chr [1:105] &amp;quot;AT&amp;quot; NA NA &amp;quot;BE&amp;quot; ...
##  $ Gender : chr [1:105] &amp;quot;Male&amp;quot; &amp;quot;Female&amp;quot; &amp;quot;Both&amp;quot; &amp;quot;Male&amp;quot; ...
##  $ Mean   : num [1:105] 7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ...
##  $ N=     : num [1:105] 471 570 1041 468 542 ...
##  - attr(*, &amp;quot;spec&amp;quot;)=
##   .. cols(
##   ..   Country = col_character(),
##   ..   Gender = col_character(),
##   ..   Mean = col_double(),
##   ..   `N=` = col_double()
##   .. )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(happiness_readr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   Country Gender  Mean  `N=`
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 AT      Male     7.3   471
## 2 &amp;lt;NA&amp;gt;    Female   7.3   570
## 3 &amp;lt;NA&amp;gt;    Both     7.3  1041
## 4 BE      Male     7.8   468
## 5 &amp;lt;NA&amp;gt;    Female   7.8   542
## 6 &amp;lt;NA&amp;gt;    Both     7.8  1010&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice, that the output is the same as when using the &lt;code&gt;read.table()&lt;/code&gt;, previously. But we did not have to specify &lt;code&gt;header=TRUE&lt;/code&gt; because by default &lt;code&gt;read_delim()&lt;/code&gt; expects the first row to contain the column names. This is done through the &lt;code&gt;col_names&lt;/code&gt; argument, set equal to true by default. Also, strings are never automatically converted to factors. Hence, &lt;code&gt;stringsAsFactors = FALSE&lt;/code&gt; is not necessary. To control the types of the columns &lt;code&gt;readr&lt;/code&gt; uses the &lt;code&gt;col_types&lt;/code&gt; argument. Let’s see how these two work.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;col_names&lt;/code&gt; is true by default meaning that it will use the the first row of data as column names. If your file does not have column names you can set &lt;code&gt;col_names = FALSE&lt;/code&gt; and columns will be numbered sequentially.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(read_delim(&amp;quot;happiness2.csv&amp;quot;, delim = &amp;quot;,&amp;quot;,
                              col_names = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   X1 = col_character(),
##   X2 = col_character(),
##   X3 = col_double(),
##   X4 = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   X1    X2        X3 X4   
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1 &amp;#39;AT   Male     7.3 471&amp;#39; 
## 2 &amp;#39;     Female   7.3 570&amp;#39; 
## 3 &amp;#39;     Both     7.3 1041&amp;#39;
## 4 &amp;#39;BE   Male     7.8 468&amp;#39; 
## 5 &amp;#39;     Female   7.8 542&amp;#39; 
## 6 &amp;#39;     Both     7.8 1010&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; Instead of assigning the output of &lt;code&gt;read_delim()&lt;/code&gt; to a variable I directly use the &lt;code&gt;head()&lt;/code&gt; function to print the first 6 lines of the data frame. It is equivalent to&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;happiness_delim &amp;lt;- read_delim(&amp;quot;happiness2.csv&amp;quot;, delim = &amp;quot;,&amp;quot;,
                              col_names = FALSE)
head(happiness_delim)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also manually set the column names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(read_delim(&amp;quot;happiness2.csv&amp;quot;, delim = &amp;quot;,&amp;quot;,
                              col_names = c(&amp;quot;Country&amp;quot;, &amp;quot;Gender&amp;quot;, &amp;quot;Mean&amp;quot;, &amp;quot;N&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   Country = col_character(),
##   Gender = col_character(),
##   Mean = col_double(),
##   N = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   Country Gender  Mean N    
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1 &amp;#39;AT     Male     7.3 471&amp;#39; 
## 2 &amp;#39;       Female   7.3 570&amp;#39; 
## 3 &amp;#39;       Both     7.3 1041&amp;#39;
## 4 &amp;#39;BE     Male     7.8 468&amp;#39; 
## 5 &amp;#39;       Female   7.8 542&amp;#39; 
## 6 &amp;#39;       Both     7.8 1010&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As mentioned, there is &lt;code&gt;col_types&lt;/code&gt; to control the column classes. If you leave the default value &lt;code&gt;readr&lt;/code&gt; heuristically inspects the first 100 rows to guess the type of each column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sapply(happiness_readr, class)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Country      Gender        Mean          N= 
## &amp;quot;character&amp;quot; &amp;quot;character&amp;quot;   &amp;quot;numeric&amp;quot;   &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to override the default column types you can also specify them manually. An option would be&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;happiness_readr2 &amp;lt;- read_delim(&amp;quot;happiness.csv&amp;quot;, delim = &amp;quot;,&amp;quot;, 
                               col_types = &amp;quot;ccni&amp;quot;)
sapply(happiness_readr2, class)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Country      Gender        Mean          N= 
## &amp;quot;character&amp;quot; &amp;quot;character&amp;quot;   &amp;quot;numeric&amp;quot;   &amp;quot;integer&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt; = character&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d&lt;/code&gt; = double&lt;/li&gt;
&lt;li&gt;&lt;code&gt;i&lt;/code&gt; = integer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;l&lt;/code&gt; = logical&lt;/li&gt;
&lt;li&gt;&lt;code&gt;_&lt;/code&gt; = skip&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let see how &lt;code&gt;skip&lt;/code&gt; works&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(read_delim(&amp;quot;happiness.csv&amp;quot;, delim = &amp;quot;,&amp;quot;, 
                               col_types = &amp;quot;ccn_&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   Country Gender  Mean
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 AT      Male     7.3
## 2 &amp;lt;NA&amp;gt;    Female   7.3
## 3 &amp;lt;NA&amp;gt;    Both     7.3
## 4 BE      Male     7.8
## 5 &amp;lt;NA&amp;gt;    Female   7.8
## 6 &amp;lt;NA&amp;gt;    Both     7.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice the fourth column has been skipped.&lt;/p&gt;
&lt;p&gt;Yet another way of setting the types of the imported columns is using collectors. Collector functions can be passed in a &lt;a href=&#34;http://www.rdocumentation.org/packages/base/functions/list&#34;&gt;&lt;code&gt;list()&lt;/code&gt;&lt;/a&gt; to the &lt;code&gt;col_types&lt;/code&gt; argument of &lt;code&gt;read_&lt;/code&gt; functions to tell them how to interpret values in a column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;car &amp;lt;- col_character()
fac &amp;lt;- col_factor(levels = c(&amp;quot;Male&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Both&amp;quot;))
num &amp;lt;- col_number()
int &amp;lt;- col_integer()

str(read_delim(&amp;quot;happiness.csv&amp;quot;, delim = &amp;quot;,&amp;quot; , 
                           col_types = list(car, fac, num, int)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [105 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
##  $ Country: chr [1:105] &amp;quot;AT&amp;quot; NA NA &amp;quot;BE&amp;quot; ...
##  $ Gender : Factor w/ 3 levels &amp;quot;Male&amp;quot;,&amp;quot;Female&amp;quot;,..: 1 2 3 1 2 3 1 2 3 1 ...
##  $ Mean   : num [1:105] 7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ...
##  $ N=     : int [1:105] 471 570 1041 468 542 1010 416 555 971 433 ...
##  - attr(*, &amp;quot;spec&amp;quot;)=
##   .. cols(
##   ..   Country = col_character(),
##   ..   Gender = col_factor(levels = c(&amp;quot;Male&amp;quot;, &amp;quot;Female&amp;quot;, &amp;quot;Both&amp;quot;), ordered = FALSE, include_na = FALSE),
##   ..   Mean = col_number(),
##   ..   `N=` = col_integer()
##   .. )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For a complete list of collector functions, you can take a look at the &lt;a href=&#34;https://cran.r-project.org/web/packages/readr/readr.pdf&#34;&gt;&lt;code&gt;collector&lt;/code&gt;&lt;/a&gt; documentation.&lt;/p&gt;
&lt;p&gt;If you are working on large datasets you may prefer handling the data in smaller parts. In &lt;code&gt;readr&lt;/code&gt; you can achieve this with the combination of &lt;code&gt;skip&lt;/code&gt; and &lt;code&gt;n_max&lt;/code&gt; arguments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(read_delim(&amp;quot;happiness.csv&amp;quot;, delim = &amp;quot;,&amp;quot;,
                skip=2, n_max= 4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Missing column names filled in: &amp;#39;X1&amp;#39; [1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   X1 = col_character(),
##   Female = col_character(),
##   `7.3` = col_double(),
##   `570` = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 4
##   X1    Female `7.3` `570`
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 &amp;lt;NA&amp;gt;  Both     7.3  1041
## 2 BE    Male     7.8   468
## 3 &amp;lt;NA&amp;gt;  Female   7.8   542
## 4 &amp;lt;NA&amp;gt;  Both     7.8  1010&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We skipped two rows and then read four lines. There is something wrong though! Since the &lt;code&gt;col_names&lt;/code&gt; is true by default the first line is used for the column names. Therefore, we need to manually specify the column names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(read_delim(&amp;quot;happiness2.csv&amp;quot;, delim = &amp;quot;,&amp;quot;,
                              col_names = c(&amp;quot;Country&amp;quot;,&amp;quot;Gender&amp;quot;, &amp;quot;Mean&amp;quot;, &amp;quot;N&amp;quot;), 
                              skip=2, n_max= 4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ── Column specification ────────────────────────────────────────────────────────
## cols(
##   Country = col_character(),
##   Gender = col_character(),
##   Mean = col_double(),
##   N = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 4
##   Country Gender  Mean N    
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1 &amp;#39;       Both     7.3 1041&amp;#39;
## 2 &amp;#39;BE     Male     7.8 468&amp;#39; 
## 3 &amp;#39;       Female   7.8 542&amp;#39; 
## 4 &amp;#39;       Both     7.8 1010&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like the &lt;code&gt;utils&lt;/code&gt; package &lt;code&gt;readr&lt;/code&gt; provides alternatives to &lt;code&gt;read_delim()&lt;/code&gt;. The &lt;a href=&#34;https://cran.r-project.org/web/packages/readr/readr.pdf&#34;&gt;&lt;code&gt;read_csv()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/readr/readr.pdf&#34;&gt;&lt;code&gt;read_tsv()&lt;/code&gt;&lt;/a&gt; are used for CSV files and tab-delimited files, respectively. The functions of both packages are presented below. Notice the &lt;code&gt;_&lt;/code&gt; is used in &lt;code&gt;readr&lt;/code&gt; instead of the &lt;code&gt;.&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;utils&lt;/th&gt;
&lt;th&gt;readr&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;read.table()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;read_delim()&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;read.csv()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;read_csv()&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;read.delim()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;read.tsv&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data.table-yet-another-alternative-to-read-data-into-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;a href=&#34;http://www.rdocumentation.org/packages/data.table&#34;&gt;&lt;code&gt;data.table&lt;/code&gt;&lt;/a&gt; … yet another alternative to read data into R&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;data.table&lt;/code&gt; package is designed mainly for fast data manipulation. It also features a powerful function to import your data into R, the &lt;a href=&#34;http://www.rdocumentation.org/packages/data.table/functions/fread&#34;&gt;&lt;code&gt;fread()&lt;/code&gt;&lt;/a&gt;. Once more you need to install and load the package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;data.table&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see how it works with two versions of our dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(fread(&amp;quot;happiness.csv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Country Gender Mean   N=
## 1:      AT   Male  7.3  471
## 2:         Female  7.3  570
## 3:           Both  7.3 1041
## 4:      BE   Male  7.8  468
## 5:         Female  7.8  542
## 6:           Both  7.8 1010&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(fread(&amp;quot;happiness2.csv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     V1     V2  V3    V4
## 1: &amp;#39;AT   Male 7.3  471&amp;#39;
## 2:   &amp;#39; Female 7.3  570&amp;#39;
## 3:   &amp;#39;   Both 7.3 1041&amp;#39;
## 4: &amp;#39;BE   Male 7.8  468&amp;#39;
## 5:   &amp;#39; Female 7.8  542&amp;#39;
## 6:   &amp;#39;   Both 7.8 1010&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember that the first row of &lt;code&gt;happiness2.csv&lt;/code&gt; does not contain the column names. That’s not a problem for &lt;code&gt;fread()&lt;/code&gt; as it automatically assignees names to the columns. As in this case, often simply specifying the path to the file is enough to successfully import your flat file using &lt;code&gt;fread&lt;/code&gt;. Moreover, it can infer the column types and separators.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(fread(&amp;quot;happiness.csv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;data.table&amp;#39; and &amp;#39;data.frame&amp;#39;:   105 obs. of  4 variables:
##  $ Country: chr  &amp;quot;AT&amp;quot; &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;BE&amp;quot; ...
##  $ Gender : chr  &amp;quot;Male&amp;quot; &amp;quot;Female&amp;quot; &amp;quot;Both&amp;quot; &amp;quot;Male&amp;quot; ...
##  $ Mean   : num  7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ...
##  $ N=     : int  471 570 1041 468 542 1010 416 555 971 433 ...
##  - attr(*, &amp;quot;.internal.selfref&amp;quot;)=&amp;lt;externalptr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two more useful arguments of &lt;code&gt;fread()&lt;/code&gt; are &lt;code&gt;drop&lt;/code&gt; and &lt;code&gt;select&lt;/code&gt;. They enable you to drop or select variables of interest in your flat file. Suppose I want to select the 2nd and 3rd column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(fread(&amp;quot;happiness.csv&amp;quot;, select = c(2,3)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Gender Mean
## 1:   Male  7.3
## 2: Female  7.3
## 3:   Both  7.3
## 4:   Male  7.8
## 5: Female  7.8
## 6:   Both  7.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(fread(&amp;quot;happiness.csv&amp;quot;, select = c(&amp;quot;Gender&amp;quot;,&amp;quot;Mean&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Gender Mean
## 1:   Male  7.3
## 2: Female  7.3
## 3:   Both  7.3
## 4:   Male  7.8
## 5: Female  7.8
## 6:   Both  7.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(fread(&amp;quot;happiness.csv&amp;quot;, drop = c(1,4)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Gender Mean
## 1:   Male  7.3
## 2: Female  7.3
## 3:   Both  7.3
## 4:   Male  7.8
## 5: Female  7.8
## 6:   Both  7.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is equivalent to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(fread(&amp;quot;happiness.csv&amp;quot;, drop = c(&amp;quot;Country&amp;quot;,&amp;quot;N=&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In short, &lt;code&gt;fread()&lt;/code&gt; saves you work by automatically guessing the delimiter, whether or not the file has a header, how many lines to skip by default, providing an easy way to select variables and more. Nevertheless, if you wish to specify them, you can do it, along with other arguments. Check the &lt;a href=&#34;http://www.rdocumentation.org/packages/data.table/functions/fread&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comment&lt;/strong&gt;
You might have noticed by now that the &lt;code&gt;fread()&lt;/code&gt; function produces data frames that look slightly different when you print them out. That’s because another class is assigned to the resulting data frames, namely &lt;code&gt;data.table&lt;/code&gt; and &lt;code&gt;data.frame&lt;/code&gt;. &lt;code&gt;read_delim()&lt;/code&gt; creates an object with three classes: &lt;code&gt;tbl_df&lt;/code&gt;, &lt;code&gt;tbl&lt;/code&gt; and &lt;code&gt;data.frame&lt;/code&gt;. The printout of such data.table objects is different. Well, it allows for a different treatment of the printouts, for example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;when-to-use-read.table-read_delim-or-fread&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;When to use &lt;code&gt;read.table()&lt;/code&gt;, &lt;code&gt;read_delim()&lt;/code&gt; or &lt;code&gt;fread()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;In a nutshell, the main differences between these functions are : the &lt;code&gt;read_&lt;/code&gt; functions from the &lt;code&gt;readr&lt;/code&gt; package have more consistent naming scheme for the parameters (e.g. &lt;code&gt;col_names&lt;/code&gt; and &lt;code&gt;col_types&lt;/code&gt;) than &lt;code&gt;read.&lt;/code&gt; and all functions work exactly the same way regardless of the current locale (to override the US-centric defaults, use &lt;code&gt;locale()&lt;/code&gt;). It is also faster! But &lt;code&gt;fread()&lt;/code&gt; is even faster! And it saves you work by automatically guessing parameters. This &lt;a href=&#34;https://cran.r-project.org/web/packages/readr/README.html&#34;&gt;README&lt;/a&gt; goes in more detail.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Speed&lt;/th&gt;
&lt;th&gt;Auto-detection&lt;/th&gt;
&lt;th&gt;Locale&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;read.table()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;fast&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;read_delim()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;faster&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fread()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;fastest&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With some loss of generality a few suggestions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for large files (many MB-GB) &lt;code&gt;fread()&lt;/code&gt; will be the fastest (with a few &lt;a href=&#34;http://stackoverflow.com/questions/32263566/comparing-speed-of-fread-vs-read-table-for-reading-the-first-1m-rows-out-of-100&#34;&gt;exceptions&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;the consistency and independence of the actual locale makes &lt;code&gt;readr&lt;/code&gt; a good candidate for everyday use&lt;/li&gt;
&lt;li&gt;if you are new to all these using &lt;code&gt;read.table()&lt;/code&gt; will allow you to develop intuition on how R works.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
