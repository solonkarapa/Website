[{"authors":null,"categories":null,"content":"I\u0026rsquo;m a researcher at the MRC Biostatistics Unit, University of Cambridge. I\u0026rsquo;m interested in many areas across statistics, machine learning, and their interactions. My research focuses on tailored model development, that is targeted model building for a specific task of interest, mostly prediction.\nI\u0026rsquo;m also developing and applying tools leveraging genomics to detect cancer earlier, which will ultimately lead to more personalized treatment for patients. Toward these goals, I draw from a wide range of disciplines, including molecular biology, computational biology, and medical oncology. More specifically, I’m looking at novel ways to incorporate liquid biopsies into the management of cancer. Liquid biopsies - the analysis of tumours using biomarkers circulating in fluids such as the blood - have the potential to change the way cancer is diagnosed, monitored, and treated.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://solon-karapanagiotis.com/author/solon-karapanagiotis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/solon-karapanagiotis/","section":"authors","summary":"I\u0026rsquo;m a researcher at the MRC Biostatistics Unit, University of Cambridge. I\u0026rsquo;m interested in many areas across statistics, machine learning, and their interactions. My research focuses on tailored model development, that is targeted model building for a specific task of interest, mostly prediction.","tags":null,"title":"Solon Karapanagiotis","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://solon-karapanagiotis.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":["algorithm","reporting","model evaluation"],"content":" The article by Matsuo et al. (2019) appeared in my newsletter. It is another attempt to sell deep-learning (DL) as a promising alternative to traditional survival analysis. To this end, they compare the performance of their DL model to Cox proportional hazard regression (CPH) when predicting survival for women with newly diagnosed cervical cancer. In fact, they compare a series of cox-based regression models (CPH, Cox Lasso, Random Survival Forest, and Cox Boost) to their proposed DL model. Not surprisingly, they conclude the DL model shows better performance.\nNevertheless, the paper is full of misconceptions and misreporting making their conclusions debatable. Let’s start with a few of their claims:\nThey claim, “Unlike CPH and its variants, deep-learning approaches can model nonlinear risk functions …”. This is not true; both Random Survival Forests, and Cox Boost allow for nonlinear functions of the risk factors. (In fact, even CPH allows for nonlinear functions, albeit they need to be pre-specified.) They continue arguing that, “deep-learning models […] easily can handle censoring in survival data”. Again, not entirely true; all the models mentioned above handle censored data. Third, they claim “… the performance of the deep-learning neural network model will perform better when large feature sets are used”. This is generally true - using more features can improve performance- but it is also clinically useless and practically infeasible. It could be quite costly and time-consuming to collect a continuously increasing number of features to achieve fractional increases in any performance metric.\nBesides all these, I have two more serious concerns: (1) about using the mean absolute deviation (MAD) as a performance metric and (2) the problem of algorithmic convergence.\nMAD is not properly defined The authors define the mean absolute deviation as “the absolute difference between the original survival time (ground truth) and the model’s predicted survival time measured in months.” This definition is ambiguous since a time-to-event model outputs the probability a patient survives (or dies) up to a timepoint. Of course, we could calculate the estimated time the outcome is expected to occur with high probability. But for some subjects this can be larger than the study follow-up time. In other words, not all subjects will necessarily experience the outcome within the follow-up time. This creates two problems.\nFirst, we are extrapolating time. There is no way of knowing how the survival curves act beyond the sampled survival times. Second, if we do not want to extrapolate time what happens to the censored individuals? These are people that experience the event after the last follow-up evaluation or are lost to follow-up, hence their event time is unobserved. It is unclear how the data from censored individuals have been analysed. Of course, one option is to simply remove all the censored individuals. However, this will likely bias the results. There are two general approaches to overcome this issue: weighting and imputation. Both methods have been explored in survival settings, for example, by Graf et al. (1999), Schemper and Henderson (2000) and Gerds and Schumacher (2006).\n To convergence or not to converge … does it matter? Another concern is about the convergence of the CPH model. To my surprise the authors report a MAD of 316 months! for the CPH model (Table 2), which they attribute to failed convergence. Fair enough, but this raises two other questions - why did it the algorithm fail in the fist place and why was it not fixed? More importantly, since the algorithm did not converge, its output cannot be trusted.\nUsually, when we write an algorithm, we are interested in knowing if the solution the algorithm provides is the correct one for the problem it solves. This can sometimes come in the form of a convergence. In iterative algorithms (such as the one used for CPH), every step generates a different error. And what the algorithm tries to do is to minimize that error so it ever gets smaller and smaller. We say that the algorithm converges if it sequence of errors converges. If the algorithm does not converge implies that a different solution can give us a lower error. In other words, our solution is not optimal.\nAs a result, we cannot trust the parameter estimates from an algorithm that has not converged and even less other derived quantities such as standard errors or p-values (which are reported in Table 4 and discussed throughout the paper). So why the authors interpret the results in the main text as “the deep-learning model had significantly better predictions compared with the CPH model, with\u0026gt;10-fold difference between the 2 analytic approaches (mean absolute error for CPH vs deep-learning: 316.2 vs 29.3)”?\nIt is disheartening to know such work passes both the editorial team and peer-review process and gets published.\n References Gerds, Thomas A, and Martin Schumacher. 2006. “Consistent Estimation of the Expected Brier Score in General Survival Models with Right-Censored Event Times.” Biometrical Journal 48 (6): 1029–40.  Graf, Erika, Claudia Schmoor, Willi Sauerbrei, and Martin Schumacher. 1999. “Assessment and Comparison of Prognostic Classification Schemes for Survival Data.” Statistics in Medicine 18 (17-18): 2529–45.  Matsuo, Koji, Sanjay Purushotham, Bo Jiang, Rachel S Mandelbaum, Tsuyoshi Takiuchi, Yan Liu, and Lynda D Roman. 2019. “Survival Outcome Prediction in Cervical Cancer: Cox Models Vs Deep-Learning Model.” American Journal of Obstetrics and Gynecology 220 (4): 381–e1.  Schemper, Michael, and Robin Henderson. 2000. “Predictive Accuracy and Explained Variation in Cox Regression.” Biometrics 56 (1): 249–55.    ","date":1673308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673308800,"objectID":"e1f04e757707daf43e30de3b1b1f8477","permalink":"https://solon-karapanagiotis.com/post/convergence/convergence/","publishdate":"2023-01-10T00:00:00Z","relpermalink":"/post/convergence/convergence/","section":"post","summary":"The article by Matsuo et al. (2019) appeared in my newsletter. It is another attempt to sell deep-learning (DL) as a promising alternative to traditional survival analysis. To this end, they compare the performance of their DL model to Cox proportional hazard regression (CPH) when predicting survival for women with newly diagnosed cervical cancer.","tags":["algorithm","reporting","model evaluation"],"title":"How to publish a model that hasn’t converged","type":"post"},{"authors":["Stavrinides V","et al"],"categories":null,"content":"","date":1670889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670889600,"objectID":"9b229e2ba0a1b4fb0e3ed605289a6022","permalink":"https://solon-karapanagiotis.com/publication/mri_prostate2/","publishdate":"2022-12-13T00:00:00Z","relpermalink":"/publication/mri_prostate2/","section":"publication","summary":"**Background** The effects of regional histopathologic changes on prostate MRI scans have not been accurately quantified in men with an elevated prostate-specific antigen (PSA) level and no previous biopsy. **Purpose** To assess how Gleason grade, maximum cancer core length (MCCL), inflammation, prostatic intraepithelial neoplasia (PIN), or atypical small acinar proliferation within a Barzell zone affects the odds of MRI visibility. **Materials and Methods** In this secondary analysis of the Prostate MRI Imaging Study (PROMIS; May 2012 to November 2015), consecutive participants who underwent multiparametric MRI followed by a combined biopsy, including 5-mm transperineal mapping (TPM), were evaluated. TPM pathologic findings were reported at the whole-prostate level and for each of 20 Barzell zones per prostate. An expert panel blinded to the pathologic findings reviewed MRI scans and declared which Barzell areas spanned Likert score 3–5 lesions. The relationship of Gleason grade and MCCL to zonal MRI outcome (visible vs nonvisible) was assessed using generalized linear mixed-effects models with random intercepts for individual participants. Inflammation, PIN, and atypical small acinar proliferation were similarly assessed in men who had negative TPM results. **Results** Overall, 161 men (median age, 62 years [IQR, 11 years]) were evaluated and 3179 Barzell zones were assigned MRI status. Compared with benign areas, the odds of MRI visibility were higher when a zone contained cancer with a Gleason score of 3+4 (odds ratio [OR], 3.1; 95% CI: 1.9, 4.9; P ","tags":null,"title":"Regional Histopathology and Prostate MRI Positivity: A Secondary Analysis of the PROMIS Trial","type":"publication"},{"authors":null,"categories":["probability","qwerty"],"content":" QWERTY has become the dominant keyboard standard, used by billions of people every day. The basic QWERTY form was developed in 1873 and was based around four rows with eleven characters in each row.\nQWERTY takes its name from the first six letter of the second line (see image).\n There has been a lot of debate on the nature of QWERTY, whether the specific keyboard design was by choice or chance?\n A common view is the letters QWERTY were assembled (on purpose) in one row so the salesman could impress the customers by quickly typing “typewriter” which was the name of the brand producing the hardware - “the Sholes and Glidden Type Writer”. Effectively a sales trick!\n Using the six letters in QWERTY we can type the word “typewriter”.\n Kay (2013) labels this view as Myth 1.\nThey used probability theory to investigate whether this feature of the keyboard exists “by intent or accident”. To do this, they calculated the probability the seven letters that make up “typewriter” falling on one line. This probability is 0.0002, so small, which indicates it was a design choice.\nCrucially, this calculation is based on the assumption that the designer had chosen in advance to place 10 letters at the top row of the keyboard1.\nHere, I’m looking how this probability changes for other values of letters at the top row.\nThe calculation in Kay (2013) First, I briefly go through the calculations presented in Kay (2013). The problem is parallel to sampling without replacement from an (imaginary) urn. The designer has chosen \\(K=10\\) letters to be assigned at the top row of the keyboard and the rest \\(R=26-10=16\\) to the other rows (there are \\(N=26\\) letters in the alphabet).\nWe are interested in the probability the \\(k=7\\) letters needed to form the word “typewriter” finish at the top row. This probability is described by a hypergeometric distribution.\nThe hypergeometric distribution describes the probability of \\(k=7\\) “successes” in \\(n=7\\) draws, without replacement, from a finite population of size \\(N = 26\\) letters that contains exactly \\(K = 10\\) objects with that feature, wherein each draw is either a success or a failure. Using the usual urn-style language of “green” and “red” marbles we have:\n \\(K = 10\\) green marbles (i.e. 10 letters to be assigned at the top row) \\(N = 26\\) (i.e. the letters of the alphabet) \\(R = 16\\) red marbles (i.e. the rest of the letters to be assigned to the other rows) \\(k = 7\\) letters that form the word “typerwriter”  We then draw \\(n = 7\\) marbles without replacement. What is the probability that exactly \\(k=7\\) are green? This is given by\n\\[P(X=k) = \\frac{ {K\\choose{k}} {{N-k}\\choose{n-k}}}{N\\choose n}\\]\nFor \\(k=7, n=7, N=26, K= 10\\), this probability is 0.00018 which is quite small. Hence, it indicates this was a design choice rather than by chance.\nBut this calculation is based on the assumption: the designer decided the number of letters for the top row to be 10 (\\(K=10\\)).\n An updated calculation Another approach is to change the number of letters in the top row and see how this probability changes.\nNow we are interested in the probability of drawing “\\(k=7\\) green marbles in \\(n=7\\) draws for different choices of \\(K\\)”. The plot shows the probability of \\(k=7\\) for different values of \\(K\\). We start with \\(K = 6\\) which gives probability = 0, as there is no way to draw 7 green marbles when there are only 6 of them! The red dot corresponds to \\(K=10\\) which has been used in the paper by Kay. For \\(K=19\\) which corresponds to the quite extreme scenario of having 19 letters at the top row, the probability the 7 “typerwriter” letters are found there becomes 8%. Still small but appreciable.\nThe calculations above are based on sampling the letters in “typewriter” at any order. We can also calculate the probability of drawing “Exactly \\(k=7\\) green marbles in \\(n=7\\) draws, in the specific QWERTY order”. This corresponds to the horizontal dashed line in the plot.\nOverall, adding more letters at the top row increases the probability substantially. For example, the probability increases by 9900% (!) when going from \\(K=10\\) to \\(16\\) letters. Nevertheless, in absolute terms even for \\(K =19\\) it is so small that we can conclude it is unlikely that these letters would appear together by chance.\nFor a more historical perspective and other myths on QWERTY - read this article.\nlibrary(purrr) N \u0026lt;- 26 K \u0026lt;- seq(6, N - 7, by = 1) prbs \u0026lt;- map_dbl(K, ~ dhyper(x = 7, m = .x, n = N - .x, k = 7)) df \u0026lt;- data.frame(K, prbs) library(ggplot2) library(dplyr) # prb of exact \u0026quot;qwerty\u0026quot; word sampled pr_exact \u0026lt;- (1/26)*(1/25)*(1/24)*(1/23)*(1/22)**(1/21)*(1/20)*(1/19) ggplot(df) + geom_point(aes(x = K, y = prbs)) + labs(y = \u0026quot;Probability\u0026quot;, x = \u0026quot;K\u0026quot;) + geom_point(data = df %\u0026gt;% filter(K == \u0026quot;10\u0026quot;), aes(x = K, y = prbs), col = \u0026#39;red\u0026#39;) + geom_hline(yintercept = pr_exact, linetype = \u0026quot;dashed\u0026quot;) + scale_x_continuous(breaks = K, labels = K) + theme_bw()  References Kay, Neil M. 2013. “Rerun the Tape of History and QWERTY Always Wins.” Research Policy 42 (6-7): 1175–85.     In fact, later versions changed this arrangement to 10 (top), 9 (middle), 7 (bottom), a more balanced ordering - this the modern QWERTY.↩︎\n   ","date":1665100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665100800,"objectID":"69c12b6063f382d0e5540500f043a378","permalink":"https://solon-karapanagiotis.com/post/qwerty/probability/","publishdate":"2022-10-07T00:00:00Z","relpermalink":"/post/qwerty/probability/","section":"post","summary":"QWERTY has become the dominant keyboard standard, used by billions of people every day. The basic QWERTY form was developed in 1873 and was based around four rows with eleven characters in each row.","tags":["probability","qwerty"],"title":"QWERTY-nomics, how did QWERTY came to be?","type":"post"},{"authors":null,"categories":["book overview"],"content":" The most common discussion around Earth concerns how we are shaping our planet. Origins is all about “how the Earth made us”.\n Origins: How the Earth Shaped Human History, Lewis Dartnell, Vintage Publishing (2020)\n What is the connecting link between the voting patterns in south-eastern US states and an ocean formed millions of years ago? The Earth’s rotation and the built of Taj Mahal? How is the formation of coal deposits 300 million years ago influence the political map of today’s Britain?\nAll these parallels between events are spread across Dartnell’s book, a thought-provoking introduction to our origins. The premise of the book is that these events are driven by the Earth’s climate which created a sort of domino effect for human evolution and progress to today’s society.\nIn a nutshell, planetary forces, caused by shifts of the Earth relative to the Sun and its orbital path, drove our evolution. Different processes (e.g. tectonic drift, climate change, landscape formation) facilitated both the emergence of humanity and its subsequent history. In fact, this is supported by more recent evidence: a simulation of the past two million years of Earth’s climate provides evidence that temperature and other planetary forces influenced early human migration — and possibly contributed to the emergence of the modern-day human species (Timmermann et al. 2022).\nAlthough such a deductive step (i.e. planetary forces caused our evolution) is interesting (and not new (Dart and Salmons 1925)), the more intriguing aspect of the book is going through each of these climate processes and looking their connections with our history.\nThree examples Why did we domesticate only a small proportion of animals?\n A spike in the planet’s termperature (methane-related carbon release) about 50 million years ago drove rapid evolution in many animals and especially the emergence of new orders of mammals. These dispersed across Asia, Europe and North America. The subsequent cooling down created the ecosystems (vast grasslands) these mammals came to dominate and diversify in many species such as the ancestors of cows and sheep. But these animal species were not evenly distributed across the planet. For example, the five most important mammals - sheep, goat, pig, cow and horse - were present only in Eurasia. After the last ice age humans settled down in Eurasia. We simply domesticated the animals we found around us!\n What is the connection between the voting patterns in south-eastern US states and an ocean formed millions of years ago?\n The south-eastern states of the US traditionaly vote for the Democrats. More specifically, there is a clear blue, Democrat-voting band curving through North and South Carolina and Alabama. This band represents the regional political and socio-economic conditions of today which relate to the an ocean formed millions of years ago! A few million years ago the area of the south-eastern US states was flooded. Material was deposited in the seafloor and when the sea fell again the ancient seafloor sediments rended this area agriculrutally productive. In particular, the cultivation of cotton became quite popular here during the Industrial Revolution. The cultivation was carried out by slaves. In fact, the term “Black belt” described the population in these regions, a dense concentration of African-Americans. Later on, as slavery was abolished the former slaves continued to work on the same cotton plantations, but now as freedmen. But, as cotton prices plumented the economy of the region struggled. As a result, people started to move to more industrial cities. But, over the years, despite this internal migration, many African-Americans remained in the region. More recently, without any industrial developments, these states suffer from socio-economic problems (poverty, poor healthcare, etc). Hence, the electorate tends to favour the policies of the Democratic Party. This is a clear connection between the soil and geology and contremporary politics.\n How the Earth’s rotation allowed the built of Taj Mahal?\n During the Age of Exploration Portuguese navigators developed a critical innovation, known as the volta do mar – the turn, or return, of the sea. This innovation takes advantage of specific wind patterns to allow for more efficient sailing - that is, sail further and faster. As people became more familiar with such wind patterns they could explore further offshore. Knowing these winds (and ocean) currents allowed the Europeans to discover and explore unknown places - Colombus sailed to America and back - and establish the trading routes during the 15th century. These patterns are created by the Sun warming up the air and the rotation of the Earth. Together these two forces produce the major wind zones of the planet which in turn drive the ocean currents. These currents allowed the Spanish to transport silver from South America, handled through European merchants, and ultimately financing a monumental building project in India - the Taj Mahal.\n  Implications for today The book touches upon another hot topic: energy production.\nOver the course of history we achieved energy production through muscle mass (ours and animals’) (and agriculture) - that is, using solar energy to grow crop and breed animals and converting them to energy. We then relied on wind and water. After that, coal and oil transformed and accelerated our progress. But we ended up emitting too much carbon (and other compounds) and changing the climate. What do we do now? Shall we return to age-old practices of using wind and water? Or solar energy?\nFarms of solar panels produce electricity directly, and hydroelectric dams and wind turbines are identical in principle to waterwheels and mills, but all have limitations. They require the Sun to shine, the wind to blow and the water to flow. But, what about nuclear fusion?\nThe book advocates in favour of nuclear power as it solves the problem of relying on natural elements (Sun, wind, river). In addition, nuclear power produces small amounts of carbon dioxide (The Economist, 2022), it is safe (Hannah Ritchie, 2020) and, most importantly, it occupies the least space among other energy sources (Hannah Ritchie, 2022).\nNuclear-power plants seem to be promising. But, first, we need to get them easier to build (The Economist, 2022).\n References Dart, Raymond A, and A Salmons. 1925. “Australopithecus Africanus: The Man-Ape of South Africa.” A Century of Nature: Twenty-One Discoveries That Changed Science and the World, 10–20.  Timmermann, Axel, Kyung-Sook Yun, Pasquale Raia, Jiaoyang Ruan, Alessandro Mondanaro, Elke Zeller, Christoph Zollikofer, et al. 2022. “Climate Effects on Archaic Human Habitats and Species Successions.” Nature 604 (7906): 495–501.    ","date":1659398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659398400,"objectID":"0c5c0d77e5ef1b2fb129b983615aa392","permalink":"https://solon-karapanagiotis.com/post/origins_review/book_review/","publishdate":"2022-08-02T00:00:00Z","relpermalink":"/post/origins_review/book_review/","section":"post","summary":"The most common discussion around Earth concerns how we are shaping our planet. Origins is all about “how the Earth made us”.\n Origins: How the Earth Shaped Human History, Lewis Dartnell, Vintage Publishing (2020)","tags":["book overview"],"title":"Origins - Book Overview","type":"post"},{"authors":null,"categories":["puzzle"],"content":" This week’s puzzle goes as follows:\n My son’s book of mathematical magic tricks includes this one:\n  Think of a whole number from 1 to 50. Add 9 and double the result. Multiply by 6 and add 12. Subtract 60. Divide by 12 and subtract 4. Subtract your original number. And the answer is (drumroll…) 1!    Unfortunately, when he is reading, my son sometimes repeats a line and sometimes he skips one. That happened with this trick. He did one of the lines twice and then missed the final instruction to subtract this original number. Yet, by an increadible fluke, he still managed to end up on the number 1.\n Let’s start by introducing some notation for each step:\nThink of a whole number from 1 to 50 - we call this number \\(x\\) Add 9 and double the result - we call the result of this step, \\(q\\). That is, \\(q=((x + 9)*2)\\) Multiply by 6 and add 12 - we call this \\(r\\). That is, \\(r = (q*6)+12\\) Subtract 60 - we call this \\(s\\). Then, \\(s=r-60\\) Divide by 12 and subtract 4 - we call this \\(t\\). Then, \\(t=\\frac{s}{12} - 4\\) Subtract your original number - we call this \\(u\\). That is, \\(u=t-x\\). And, we know, \\(u=1\\) (from step 7).  The puzzle states that the son “missed the final instruction to subtract this original number.” If he had subtracted the original number (after repeating one of the previous steps) he would have gotten a negative number (or zero). So, we know, after repeating a step that \\(t’ \\leq 0\\), where \\(t’\\) is the result to step 5, after repeating one of the previous steps.\nNow, looking at steps 2 through 5, the only ones that can give us a negative number are steps 4 and 5. Steps 2 and 3 involve the addition and multiplication of positive numbers. They cannot give us a negative result.\nWe are then left with steps 4 and 5 as potential candidates to have been repeated twice. In fact, whatever the result is after steps 2 and 3 we know that either:\n [repeat step 4 twice and add step 5] = 1 (I call this proposition A) or\n [step 4 and repeat step 5 twice] = 1 (I call this proposition B).\n  We can check each of these propositions:\n Proposition A is: \\(\\frac{(r-60)-60}{12} - 4 = 1\\). I have simply repeated step 4 twice and then added step 5. Solving for \\(r\\) gives us \\(r = 180\\). We can then backtrack, solving for \\(q\\) from steps 2 and 3 to get \\(x =5\\). So \\(x = 5\\) is a potential solution.\n Proposition B is: \\(\\frac{(s/12 - 4)}{12} - 4 = 1\\) gives us \\(s=768\\). From steps 2-4 this implies that \\(x=59\\), which is outside the range given in step 1. So this is not a valid solution.\n  Summarizing, the only way to get a result equal to 1 at step 6, after repeating one step twice and then missing the final instruction to subtract the original number, is to choose \\(x=5\\) at step 1.\nBrute force solution Another way to get the solution is to code steps 2 through 6, repeating each step twice and checking the result for all numbers from 1 to 50. This is what the magic_trick() function does.\n# helper functions - each one calculates one of the steps 2-5 f2 \u0026lt;- function(x) (x + 9) * 2 # calculates step 2 f3 \u0026lt;- function(x) (x * 6) + 12 # calculates step 3 f4 \u0026lt;- function(x) x - 60 # calculates step 4 f5 \u0026lt;- function(x) (x / 12) - 4 # calculates step 5 magic_trick \u0026lt;- function(x, rep = 0){ # x: a whole number from 1 to 50 # rep: the step 2-5 to be repeated twice. If rep = 0 or 1. - no repetition if(rep == 2){ f \u0026lt;- f5(f4(f3(f2(f2(x))))) } else if(rep == 3){ f \u0026lt;- f5(f4(f3(f3(f2(x))))) } else if(rep == 4){ f \u0026lt;- f5(f4(f4(f3(f2(x))))) } else if(rep == 5) { f \u0026lt;- f5(f5(f4(f3(f2(x))))) } else { f \u0026lt;- f5(f4(f3(f2(x)))) - x } return(f) } The code below calculates the final solution to the puzzle for each original number (x_values) and for each step repeated twice or not repeated at all (repeated).\nThe first (top left) panel corresponds to the solution for the original set-up - no step is repeated. For all \\(x\\) the result is always one. The second and third panels correspond to repeating steps 2 and 3, respectively. The results confirm our intuition from before - all solutions are positive and well above one. The last panel shows the results when repeating step 5. All of them are negative for the range 1 to 50. Finally, the fourth panel gives the solution to the puzzle. There is only one value of \\(x\\), namely \\(x = 5\\), that gives one as the final result.\nlibrary(purrr) library(dplyr) library(ggplot2) x_values \u0026lt;- seq(1, 50, by = 1) # step 1: whole number from 1 to 50. repeated \u0026lt;- c(0, 2, 3, 4, 5) # step to repeat. 0 corresponds to the original puzzle vals \u0026lt;- expand.grid(x_values, repeated) res \u0026lt;- map2_dbl(vals$Var1, vals$Var2, magic_trick) df \u0026lt;- data.frame(x = rep(x_values, 5), repeated = rep(repeated, each = 50), res = res) %\u0026gt;% mutate(repeated = paste0(\u0026quot;step \u0026quot;, repeated, \u0026quot; repeated\u0026quot;)) ggplot(df) + geom_point(aes(x = x, y = res)) + facet_wrap(. ~ repeated, scales = \u0026quot;free\u0026quot;) + geom_hline(yintercept = 1, linetype = \u0026quot;dashed\u0026quot;, linewidth = 1.7, col = \u0026quot;#009E73\u0026quot;) + labs(y = \u0026quot;Result at step 7\u0026quot;) + theme_linedraw(12) + theme(strip.background = element_rect(fill = \u0026quot;white\u0026quot;)) + theme(strip.text.x = element_text(color = \u0026quot;black\u0026quot;))  ","date":1644451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644451200,"objectID":"c2da3492b0b290b5167bde0a1c54489e","permalink":"https://solon-karapanagiotis.com/post/new_scientist_puzzle_204/puzzle_204/","publishdate":"2022-02-10T00:00:00Z","relpermalink":"/post/new_scientist_puzzle_204/puzzle_204/","section":"post","summary":"This week’s puzzle goes as follows:\n My son’s book of mathematical magic tricks includes this one:\n  Think of a whole number from 1 to 50. Add 9 and double the result.","tags":["puzzle"],"title":"# 204 Puzzle from the New Scientist","type":"post"},{"authors":null,"categories":["data analysis"],"content":" Standard survival data measure the time span from some time origin until the occurrence of the event of interest. In the interpretation of results of survival analyses, competing risks can be an important problem. Competing risks occur when subjects can experience one or more events which ‘compete’ with the event of interest. In those cases, the competing risk hinders the observation of the event of interest or modifies the chance that this event occurs.\nHere, I use the data from Kjernsmo et al. (2020) on biological iridescence to study the impact of competing risks on the final conclusions. First, I introduce the topic of iridescence and its intriguing biological significance (Background). Then, I outline the main idea behind competing risks (About competing risks) and present the two main types of hazard functions. I then re-analyse the data from Kjernsmo et al. (2020) (Results) and finish with a few concluding remarks.\nBackground Biological iridescence (the vivid, shining colouring of many species) often serves to make individual animals more visible, and as a result, has been hypothesised to contribute to sexual selection. But the fact that it is found in non-reproductive stages makes the sexual selection hypothesis less likely. An alternative (and ) hypothesis is that can work as a form of protection, aiming “to conceal rather than reveal” .\nKjernsmo et al. (2020) provide evidence for this hypothesis by showing that iridescence provides a survival advantage making the prey less detectable, effectively acting as camouflage.\nThe authors use a coloured beetle species as a test case. The beetle’s wings sport a shiny, shifting and metallic green-blue appearance stemming from structural colour. They then use a series of elegantly simple experiments to test the camouflage hypothesis. First, they put together a collection of hundreds of beetle wing cases, including the iridescent and non-iridescent beetles in a variety of colours, and distributed them in a natural setting amid a variety of plant species. They found, surprisingly, that the iridescent specimens were more likely to survive predation by birds than the non-iridescent variety—even outperforming a leaf-green non-iridescent model that should have blended in with the background colours.\nThese conclusions are based on a mixed Cox model where the survival of the beetles was recorded at 2, 24, and 48 h. Predation by birds, which ate all or most of the metalwork, was scored as an event in the survival analysis. Predation by animals other than birds (non-birds), complete disappearance of a target, or survival to 48 h, were treated as censored values in the survival analysis1.\nEffectively this analysis ignores the competing risks. A competing risk is an event whose occurrence precludes the occurrence of the primary event of interest. Being eaten by other animals precludes to be eaten by birds! Here, I re-analyse the data taking into account the competing risks.\n About competing risks Competing risks concern the situation where more than one cause of failure is possible. It refers to situations where an event has occurred, which prevents occurrence of the primary event of interest. For instance, in this study, predation by non-birds prevents the occurrence of the primary event of interest, i.e. predation by birds. A common assumption is that upon removal of one cause of failure, the risk of failure of the remaining causes is unchanged. That is, the competing risks are assumed independent. While this may be a reasonable assumption in some settings, independent competing risks may be relatively rare in biological applications.\nWhen analyzing survival data in which competing risks are present, analysts frequently censor subjects when a competing event occurs (as done in this study). Thus, when the outcome is time to death attributable to birds, an analyst considers an insect as censored once it dies of non-bird causes (spiders etc). However, censoring insects at the time of death attributable to non-bird causes may be problematic (see Putter, Fiocco, and Geskus (2007) for a review). The next section introduces two ways competing risks can be taken into account.\nThe Hazard Function A key concept in survival analysis is that of the hazard function. In the absence of competing risks, the hazard function is defined as\n\\[\\lambda(t) = \\lim_{\\Delta t \\to 0} \\frac{Prob(t \\leq T \u0026lt; t + \\Delta t | T \u0026gt; t)}{\\Delta t}\\] where \\(T\\) denotes the time from baseline until the occurrence of the event of interest. The hazard function, which is a function of time, describes the instantaneous rate of occurrence of the event of interest in subjects who are still at risk of the event. In a setting in which the outcome is, say, all-cause mortality, the hazard function at a given point in time would describe the instantaneous rate of death in subjects who were alive at that point in time.\nCompeting risks implies that a subject can experience one of a set of different events - an insect can be eaten by a bird (event 1) or by a non-bird (event 2). In this case, 2 different types of hazard functions are of interest: the cause-specific hazard function and the subdistribution hazard function. The cause-specific hazard function is\n\\[\\lambda^{CS}(t) = \\lim_{\\Delta t \\to 0} \\frac{Prob(t \\leq T \u0026lt; t + \\Delta t, \\color{blue}{E = k} | T \u0026gt; t)}{\\Delta t}\\] The cause-specific hazard function denotes the instantaneous rate of occurrence of the \\(k^{th}\\) event (blue term) in subjects who are currently event free (i.e. in subjects who have not yet experienced any of the different types of events). If one were considering 2 types of events, death attributable to birds and death attributable to non-birds, then the cause-specific hazard of bird death denotes the instantaneous rate of bird death in insects which have not yet experienced either event (i.e., in insects that are still “alive”). The subdistribution hazard function is\n\\[\\lambda^{SD}(t) = \\lim_{\\Delta t \\to 0} \\frac{Prob(t \\leq T \u0026lt; t + \\Delta t, \\color{blue}{E = k} | T \u0026gt; t \\color{blue}{\\cup (T \u0026lt; t \\cap E \\neq k)})}{\\Delta t}\\] It denotes the instantaneous risk of failure from the \\(k^{th}\\) event in subjects who have not yet experienced an event of type \\(k\\) (blue term). Note that this risk set includes those who are currently event free as well as those who have previously experienced a competing event. This differs from the risk set for the cause-specific hazard function, which only includes those who are currently event free. Using the same example as above, the subdistribution hazard of predation by birds denotes the instantaneous rate of bird death in insects who are still “alive” (i.e. who have not yet experienced either event) or who have previously died of non-bird predation. There is a distinct cause-specific hazard function for each of the distinct types of events and a distinct subdistribution hazard function for each of the distinct types of events.\nNote, the difference between the two hazard functions is in the risk set. As a result, for the cause-specific hazard, the risk set decreases at each time point at which there is a failure of another cause. For subdistribution hazard insects who fail from another cause remain in the risk set.\nHere, I’m interested in modelling the effect of covariates on both hazards and see if that leads us to different conclusions.\n  Results The data can be found here2. First, I transform the data for competing risks analysis. I use the following three event type indicators: 1 for bird death, 2 for non-bird death and 0 for censored observations.\n# data transformation for competing risks # data is the loaded data-frame data_compete \u0026lt;- data %\u0026gt;% mutate(BirdPredated = case_when( (Notes == \u0026quot;SPIDER\u0026quot; | Notes == \u0026quot;ANTS\u0026quot; | Notes == \u0026quot;SLUG\u0026quot; | Notes == \u0026quot;WASP\u0026quot;) ~ 2, TRUE ~ as.numeric(BirdPredated))) I use the survival package to fit the two cause-specific models: cox1 for bird death and cox2 for non-bird death.\nlibrary(survival) # Cause-specific hazard for bird death cox1 \u0026lt;- coxph(Surv(Time, BirdPredated == 1) ~ Treatment, data = data_compete, x = TRUE) # Cause-specific hazard for non-bird death cox2 \u0026lt;- coxph(Surv(Time, BirdPredated == 2) ~ Treatment, data = data_compete, x = TRUE) I then use the cmprsk package to fit the two subdistribution models: crr1 for bird death and crr2 for non-bird death.\nlibrary(cmprsk) # necessary pre-processing Treatment \u0026lt;- model.matrix(~ data_compete[, \u0026quot;Treatment\u0026quot;])[,-1] cov_mat \u0026lt;- Treatment # subdistribution hazard bird death crr1 \u0026lt;- crr(data_compete$Time, fstatus = data_compete$BirdPredated, cov1 = cov_mat, failcode = 1) # subdistribution hazard non-bird death crr2 \u0026lt;- crr(data_compete$Time, fstatus = data_compete$BirdPredated, cov1 = cov_mat, failcode = 2) I plot the hazard ratios (HR) with 95% confidence intervals for each treatment. We see that treatment affects the relative cause-specific hazard of bird death (red)3 but not of non-bird death (blue). Similarly, treatment has a significant effect on the relative incidence of bird death (green), but not of non-bird death (yellow). Together these indicate that, contrary to non-bird predators, birds are less sensitive to iridescent targets. Interestingly, though, treatment has a more accentuated effect on the cause-specific hazard (red) of bird death than the cumulative incidence (green) of bird death. Likely, the effects are qualitatively the same, which not be the case.\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead.  Which one to use? This example demonstrates that the two approaches may yield different results. This can be explained by the different composition of the risk sets. In the cause-specific model for bird death, insects who died from a non-bird cause were censored and thus removed from the risk sets after their time of death, whereas they were kept in the risk sets after death in the subdistribution model.\nAs a result, the cause-specific hazard ratio (\\(HR^{CS}\\)) and the subdistribution HR (\\(HR^{SD}\\)) do not have the same interpretation. For example4, the \\(HR^{CS}\\) of 1.65 means that static rainbow insects (‘Stat’ in plot - red), had a hazard of dying 1.65 times higher than iridescent insects, among insects who were alive and did not die from non-bird predators. The \\(HR^{SD}\\) higher than one (\\(HR^{SD}\\) = 1.45) means that the cumulative incidence of death is higher in static rainbow insects (‘Stat’ in plot - green) when compared with iridescent ones. However, the numerical value of 1.45 is not straightforward to interpret since it reflects the mortality rate ratio among insects who are alive or have died from non-bird predators. So, the \\(HR^{SD}\\) is in fact a different quantity than an \\(HR^{CS}\\), representing a ratio in a non-existing population including those who experienced the competing event.\nThis quantity is mainly of interest for prediction. That is why the the subdistribution hazard ratio may be thought of as a measure of ‘prognostic association’, i.e. best suited to quantifying predictive relationships. This suggests that subdistribution hazards models should be used for developing clinical prediction models. Conversely, the cause-specific hazard ratio may be thought of as a measure of ‘aetiological association’, i.e. best suited to quantifying causal relationships and may be more appropriate for addressing questions of aetiology. (see Noordzij et al. (2013) for a comprehensive review and Feakins et al. (2018) for an application on cardiovascular and cancer mortality).\n References Feakins, Benjamin G, Emily C McFadden, Andrew J Farmer, and Richard J Stevens. 2018. “Standard and Competing Risk Analysis of the Effect of Albuminuria on Cardiovascular and Cancer Mortality in Patients with Type 2 Diabetes Mellitus.” Diagnostic and Prognostic Research 2 (1): 1–9.  Kjernsmo, Karin, Heather M Whitney, Nicholas E Scott-Samuel, Joanna R Hall, Henry Knowles, Laszlo Talas, and Innes C Cuthill. 2020. “Iridescence as Camouflage.” Current Biology 30 (3): 551–55.  Noordzij, Marlies, Karen Leffondré, Karlijn J van Stralen, Carmine Zoccali, Friedo W Dekker, and Kitty J Jager. 2013. “When Do We Need Competing Risks Methods for Survival Analysis in Nephrology?” Nephrology Dialysis Transplantation 28 (11): 2670–77.  Putter, Hein, Marta Fiocco, and Ronald B Geskus. 2007. “Tutorial in Biostatistics: Competing Risks and Multi-State Models.” Statistics in Medicine 26 (11): 2389–2430.     Predation by other animals included spiders, which sucked the fluids out and left a hollow exoskeleton, slugs, which left slime trails, and ants, which chopped off small pieces of the mealworm.↩︎\n I use Kjernsmo_et_al_Experiment1_data.txt↩︎\n These are almost identical results as in the original paper (Kjernsmo et al. (2020)), even though I don’t use any random effects.↩︎\n These figures can be obtained from ‘summary(cox1)’ and ‘summary(crr1)’.↩︎\n   ","date":1643241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643241600,"objectID":"2c23673bff57b359aabe67c3625d47e0","permalink":"https://solon-karapanagiotis.com/post/iridescence/iridescence/","publishdate":"2022-01-27T00:00:00Z","relpermalink":"/post/iridescence/iridescence/","section":"post","summary":"Standard survival data measure the time span from some time origin until the occurrence of the event of interest. In the interpretation of results of survival analyses, competing risks can be an important problem.","tags":["data analysis"],"title":"Iridescence as camouflage - A comment on competing risks","type":"post"},{"authors":null,"categories":["puzzle","permutations"],"content":" Another puzzle from the New Scientist - #134: Can you work out which keyboard to hack?. It goes as follows:\n James Blond edges along the corridors of the supervillain’s base, and comes to two locked doors, each with a keypad that requires a four-digit code. He will need to get through one of the doors, but there is no time to guess a four-digit code – the number of possible combinations is staggering!\n  But wait! Some of the buttons on the keypads are visibly worn down, while others look as if they have never been pressed.\n  One door has a keypad with four worn buttons, the other has three. Blond only has time to try one door, and he will have to try all the possible combinations.\n  Which of the two keypads will give him fewer combinations to try – the one with four worn buttons, or the one with three?\n Here we are interested in the order of the four numbers, that is all possible permutations of four digits.\nFor the door with the four worn buttons (let’s call it Door 1) we can calculate all the permutations as follows:\nPick one of the four numbers (there are four choices in this step). Pick one of the remaining three numbers (there are three choices). Pick one of the remaining two numbers (two choices). Stick the last number on the end.  By multiplying these choices together to get our result: \\(4 \\times 3 \\times 2 (\\times 1) = 24\\) possible permutations.\nThe actual formula behind the calculation is \\(n!/(n-r)!\\), where \\(n\\) is the number of things to choose from, and we choose \\(r\\) of them, with no repetitions (we need to choose each number only once). For us, \\(n= r= 4\\) since we have \\(n=4\\) digits to choose from and we choose \\(r=4\\) which gives \\(4!/(0!) = 4!/1 = 4 \\times 3 \\times 2 \\times 1 = 24\\).\nThe factorial function (symbol: \\(!\\)) just means to multiply a series of descending natural numbers. Example: \\(7! = 7 × 6 × 5 × 4 × 3 × 2 × 1 = 5,040\\). Note, it is generally agreed that \\(0! = 1\\).\nThe reasoning for the door with three worn buttons (Door 2) is slightly more involved. First, I introduce the letters \\(A, B\\) and \\(C\\) to denote each of the three worn buttons on the keypad- it doesn’t matter which is which. We can arrange the three letters in \\(3!\\) ways. This follows the same reasoning as before. Now, let’s assume the following order\n\\({\\color{BrickRed}*}, A, *, B, *, C, {\\color{BrickRed}*}\\)\nwhere the asterisks indicate positions where the letters can be repeated. For example, if \\(A\\) is repeated then I simply replace the asterisks with \\(A\\). We then get\n\\(A, A, B, C\\) (I replaced the first asterisk)\n\\(A, A, B, C\\) (I replaced the second asterisk)\n\\(A, B, A, C\\) (I replaced the third asterisk)\n\\(A, B, C, A\\) (I replaced the fourth asterisk)\nNote the first two are actually the same. So, in effect, two asterisks (shown in red above) are superfluous. This reduces the positions the letters can be repeated to 2. And since we can repeat any of the 3 letters (\\(A\\) or \\(B\\) or \\(C\\)), the number of possible permutations is \\(3! \\times 2 \\times 3 = 36\\).\nHence, James should go for Door 1 with the four worn digits as he will need at most 24 attempts compared to 36 attempts for Door 2.\nUpdate 15/10/21 The solution from the New Scientist.\n ","date":1634256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634256000,"objectID":"4c7e92f828c4d08245e03eb421a17ca7","permalink":"https://solon-karapanagiotis.com/post/new_scientist_puzzle_134/puzzle_134/","publishdate":"2021-10-15T00:00:00Z","relpermalink":"/post/new_scientist_puzzle_134/puzzle_134/","section":"post","summary":"Another puzzle from the New Scientist - #134: Can you work out which keyboard to hack?. It goes as follows:\n James Blond edges along the corridors of the supervillain’s base, and comes to two locked doors, each with a keypad that requires a four-digit code.","tags":["puzzle","permutations"],"title":"# 134 Puzzle from the New Scientist","type":"post"},{"authors":null,"categories":["puzzle","probability"],"content":" Solution to #131 “The Paradise Club” puzzle from the New Scientist. This week (No 3352 - 18 September 2021) the New Scientist posted the following puzzle:\n Down at The Paradise Club, Gus and Bart take it in turns to roll a pair of dice. The first person to score his favourite score for two dice wins, which means being treated to a drink by the other (the loser). They each favour a different prime number as a score with two dice and it so happens that their chances of getting their favourite score is the same for each.\n  What is that probability? If Gus goes first what are his chances of being bought a drink?\n The maximum score (i.e. sum) when rolling two dice is 12. The prime numbers from 2 (the minimum score) to 12 are\n\\[2, 3, 5, 7, 11\\]\nEach score can be the result of the following dice combinations\n2: {1, 1}\n3: {2, 1}\n5: {2, 3}, {4, 1}\n7: {4, 3}, {5, 2}, {6, 1}\n11: {6, 5}\nFor example, a score of 2 can only be achieved if both dice are {1, 1} (1 combination in total). A score of 3 can be achieved either with {2, 1} or {1, 2} (2 combinations in total). A score of 5 can be achieved with {2, 3} or {4, 1} or {3, 2} or {1, 4} (4 combinations in total).\nThe table below shows the total number of combinations for each score\n  Score Total Combinations    2 1  3 2  5 4  7 6  11 2    The puzzle states that the chances of getting their favourite score is the same for Gus and Bart. Since each combination is equiprobable1 the only scores that result in the same chances are 3 and 11.\nNow we need to find that probability. We can calculate it as\nProbability = Number of favourable combinations/total number of combinations\nWe already know the number of favourable combinations (i.e. the number of combinations that will give the win) - it is 2. There are 2 ways to get a score of 3 or 11 (see table). The total number of combinations is 36. There are 36 possible outcomes when we roll two dice (6 from the first and 6 from the second die).\nHence, the probability of getting their favourite score is \\(2/36 \\text{ or } 1/18\\) and it is the same for both.\nThen, we are asked: “If Gus goes first what are his chances of being bought a drink?” That is, what are his chances of winning? Gus wins if he rolls his favourite score or both Bart and Gus fail - in which case they continue the game. Let W denote the event that Gus wins. The probability of W is\nP(W) = P(Gus rolls his favourite score ) + P(both fail)\nThis is usually called the addition rule2. We already know the first term, the probability of rolling his favourite score is \\(1/18\\). The second term can be decomposed as\nP(both fail) = P(Bart fails and Gus fails) =\nP(Bart fails given that Gus fails) P(Gus fails)\nP(Bart fails given that Gus fails) (1 - P(W))\nTo go from the first to the second line I used the multiplication rule3. Also, P(Gus fails) = 1 - P(Gus wins) = 1 - P(W). The first term in the last line (P(Bart fails given that Gus fails)) is equal to the probability that Bart fails, which is 1-P(Bart rolls his favourite score) = 1 - 1/18 = 17/18. So rewriting our previous equation we have\nP(W) = P(Gus rolls his favourite score) + P(both fail)\nP(W) = P(Gus rolls his favourite score) + P(Bart fails given that Gus fails) (1 - P(W))\nP(W) = 1/18 + 17/18(1 - P(W))\nP(W) = 18/35\nThe probability of Gus winning, if he goes first, is 18/35.\nUpdate 27/09/21 The solution from the New Scientist.\n  I have assumed that each die is fair, that is the occurrence of each number is equally likely.↩︎\n The addition rule states that for two events A and B, the probability that either one or both events occur is P(A or B) = P(A) + P(B) - P(A and B). The last term, P(A and B), is the probability that both events occur. In our case, it is zero because we cannot have a score of 3 and 11 at the same time, they cannot occur together when rolling a pair of dice.↩︎\n The multiplication rule states that the probability that both events occur is P(A and B) = P(A) P(B given A) or P(B) P(A given B). P(A given B) means the probability that event A occurs given event B has occurred.↩︎\n   ","date":1632182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632182400,"objectID":"0ae45ebe25161424f0216ce9bc072874","permalink":"https://solon-karapanagiotis.com/post/new_scientist_puzzle_131/puzzle_131/","publishdate":"2021-09-21T00:00:00Z","relpermalink":"/post/new_scientist_puzzle_131/puzzle_131/","section":"post","summary":"Solution to #131 “The Paradise Club” puzzle from the New Scientist. This week (No 3352 - 18 September 2021) the New Scientist posted the following puzzle:\n Down at The Paradise Club, Gus and Bart take it in turns to roll a pair of dice.","tags":["puzzle","probability"],"title":"# 131 Puzzle from the New Scientist","type":"post"},{"authors":null,"categories":["ML","data vis"],"content":" Shall I display my data using a table or a graph? The usual answer is: it depends. Mostly, it depends on who the audience is and how the data will be used. I agree, but Alaa et al, 2021 may have gone a bit too far using tables.\nI’ll start with a brief summary of the paper.\nIt is about the development of Adjutorium - a machine learning algorithm for breast cancer prognostication. The authors motivate the development of Adjutorium by stating that a widely used model (PREDICT v2.1) under-performs in specific subgroups of patients. They then compare the accuracy of Adjutorium in predicting all-cause and breast cancer-specific mortality at 3, 5 and 10 years from baseline with PREDICT v2.1. In addition, they compare Adjutorium to an in-house Cox proportional hazards (Cox PH) regression model. They use a series of measures to assess the three models, AUC-ROC, Harrel’s C-index and Uno’s C-index. They conclude that “Adjutorium uniformly outperformed PREDICT v2.1 and the conventional Cox PH model in predicting all-cause and breast cancer-specific mortality”.\nThis statement is mostly based on Table 1. But, the table is cramped with so many values that is difficult to draw any conclusions - unless you spend hours on it.\nI argue that the main message they are trying to convey is not contained in the actual values, which would justify this tabular form, but in the “shape” of the values. They want to reveal the relationships among the three models. That is why I believe a graph would communicate the message more efficiently. So, below I plot the bottom panel (external validation cohort) of their Table 1.\nThe horizontal lines show the performance of Adjutorium. In general, Adjutorium performs better. The improvement in performance is more evident for the cancer-specific mortality (right panel).\nInterestingly though, the conclusions depend the choice of performance measure. For example, using the AUC-ROC and Uno’s C-index the simpler Cox PH model predicts all-cause mortality equally well to Adjutorium.\nIn general, I find graphs more informative - it is easier to see trends in the data when it is displayed visually compared to when it is displayed numerically in a table.\n","date":1629417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629417600,"objectID":"5d1c473e6d94e799a4c624082e8ff741","permalink":"https://solon-karapanagiotis.com/post/table_vs_graph/ml/","publishdate":"2021-08-20T00:00:00Z","relpermalink":"/post/table_vs_graph/ml/","section":"post","summary":"Shall I display my data using a table or a graph? The usual answer is: it depends. Mostly, it depends on who the audience is and how the data will be used.","tags":["ML","data vis"],"title":"Table vs graph","type":"post"},{"authors":["Solon Karapanagiotis","Umberto Benedetto","Sach Mukherjee","Paul D W Kirk","Paul J Newcombe"],"categories":null,"content":"","date":1628294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628294400,"objectID":"7f3bd5446ece7f162efde57959e7d2ff","permalink":"https://solon-karapanagiotis.com/publication/tailoredbayes/","publishdate":"2021-08-07T00:00:00Z","relpermalink":"/publication/tailoredbayes/","section":"publication","summary":"Risk prediction models are a crucial tool in healthcare. Risk prediction models with a binary outcome (i.e., binary classification models) are often constructed using methodology which assumes the costs of different classification errors are equal. In many healthcare applications, this assumption is not valid, and the differences between misclassification costs can be quite large. For instance, in a diagnostic setting, the cost of misdiagnosing a person with a life-threatening disease as healthy may be larger than the cost of misdiagnosing a healthy person as a patient. In this article, we present Tailored Bayes (TB), a novel Bayesian inference framework which “tailors” model fitting to optimize predictive performance with respect to unbalanced misclassification costs. We use simulation studies to showcase when TB is expected to outperform standard Bayesian methods in the context of logistic regression. We then apply TB to three real-world applications, a cardiac surgery, a breast cancer prognostication task, and a breast cancer tumor classification task and demonstrate the improvement in predictive performance over standard methods.","tags":null,"title":"Tailored Bayes: a risk modeling framework under unequal misclassification costs","type":"publication"},{"authors":null,"categories":["puzzle","probability"],"content":" Attempt to solve #115 “A random robot” puzzle from the New Scientist. This week (No 3336 - 29 May 2021) the New Scientist published the following puzzle:\n Roman the test robot is being given one final roam before being consigned to the scrapheap where he can rust in peace.\n  He has been programmed to make four equal length steps. For his first move, he can travel one step east, west, north or south. Each of his subsequent three steps must be at right angles to the previous move. The direction of each move is selected by a random number generator, with all four possibilities being equally probable.\n  What is the chance that Roman will finish where he started?\n I decided to solve the puzzle by first plotting how (and whether) Roman can finish where he started (see graph below). To do this, I divided the space based on the 4 cardinal directions he can take in one step. These are the coloured quadrants in the plot; his Start/Finish position is also given. For example, Roman will be travelling along the purple top-right quadrant if he starts by going either north or east.\nNext, within each quadrant he can travel clockwise or anti-clockwise. This is depicted with the black and grey arrows, respectively. For instance, if he goes north at the first step (purple quadrant) and then by travelling clockwise (black arrow), at right angles, he will finish where he started. Similarly, if he goes east at the first step (purple quadrant) and then by travelling anti-clockwise (grey arrow) he will again finish where he started. In effect, the purple quadrant corresponds to the path:\nnorth -\u0026gt; east -\u0026gt; south -\u0026gt; west\nor\neast -\u0026gt; north -\u0026gt; west -\u0026gt; south\nUsing the same reasoning, within each quadrant Roman can travel clockwise or anti-clockwise. This gives us 8 different paths (4 quadrants times 2 directions in each one) in total for Roman to start and finish at the same position.\nNow we need to calculate the probability of each of those paths. I’ll use the path:\nnorth -\u0026gt; east -\u0026gt; south -\u0026gt; west\nfor illustration. For his first move, he can choose any of the four cardinal directions with equal probability. So, the probability he chooses north is 1/4 (i.e. P(north)=1/4). After that, for each subsequent step he can only travel at right angels to the previous move. This limits his choices to two at each subsequent step. In addition, either choice is equally probable (as given by the problem). In this example, after going north, he can only go east or west with equal probability, so P(east) = 1/2. The same is valid of the next two steps. Hence, P(south) = 1/2 and P(west) = 1/2. To find the probability of the entire path we simply need to multiply the individual probabilities of the path. This is, P(path) = P(north) * P(east) * P(south) * P(west) = 1/32.\nFinally, we have 8 different paths so the total probability that Roman will finish where he started is 8*(1/32)=1/4.\nUpdate 04/06/21 Indeed, the answer is 1/4, see here for the New Scientist’s solution.\n ","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"6ba3d1a149afacc0b5d98c48e7b9359b","permalink":"https://solon-karapanagiotis.com/post/new_scientist_puzzle/puzzle_115/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/post/new_scientist_puzzle/puzzle_115/","section":"post","summary":"Attempt to solve #115 “A random robot” puzzle from the New Scientist. This week (No 3336 - 29 May 2021) the New Scientist published the following puzzle:\n Roman the test robot is being given one final roam before being consigned to the scrapheap where he can rust in peace.","tags":["puzzle","probability"],"title":"Puzzle from the New Scientist","type":"post"},{"authors":null,"categories":["Bayes","outliers","ML"],"content":" This post is concerned with a ubiquitous problem of outliers. They are infamous for degrading the performance of many models/algorithms. As a result, ongoing attempts try to accommodate them by deriving robust estimators. Unfortunately, these estimators have drawbacks such as being less efficient. In this post, I approach the problem from a Bayesian viewpoint. I illustrate how the issue of outliers connects with our prior beliefs about the data collection procedure. This leads me to show how a simple but flexible Bayesian model allows us to accommodate outliers without inheriting the drawbacks of other estimators.\nDisclaimer: This post is heavily inspired by the work of Jaynes (2003).\nThe problem Imagine we are interested in a quantity \\(\\theta\\), which is unknown. The subsequent, logical step is to try to quantify our uncertainty about \\(\\theta\\) by collecting some data. That is, we are trying to measure \\(\\theta\\). But the data collection procedure (or apparatus) is always imperfect and so having \\(n\\) independent measurements of \\(\\theta\\), we have \\(n\\) different results ($x_1, …, x_n $). How are we going to proceed on estimating \\(\\theta\\), what is the “best” estimate to use? If the \\(n\\) data points are “close” together the problem of drawing conclusion about \\(\\theta\\) is not very difficult. But if they are not nicely clustered: one value, \\(x_j\\), lies far away from the other \\(n-1\\) values? How are we going to deal with this outlier1?\n The dilemma Two opposite views have been expressed:\nThe outlier should not have been included in the data. The data have been contaminated and the outlier needs to be removed otherwise we may get erroneous conclusions. The outlier may be the most important datapoint we have so it must be taken into account in the analysis. In other words, it may be desirable to describe the population including all observations. For only in that way do we describe what is actually happening (Dixon 1950).  These viewpoints reflect different prior information about the data collection procedure. The first view is reasonable if we believe a priori the data collection procedure is unreliable. That is, any now and then and without warning we can get an erroneous measurement. The second view is reasonable if we have absolute confidence in the data collection procedure. Then the outlier is an important result and ignoring it may harm us.\nClearly these are extreme positions, and in real-life the researcher is in a intermediate position. If they knew the apparatus is unreliable they would have choose not to collect data in the first place or improve the apparatus. Of course, in some situations we are obliged to use whatever “apparatus” we have access to. So the question arises can we formalise an intermediate position?\n Robustness Such an intermediate position is the idea of robustness. Researchers sometimes use various “robust” procedures, which protect against the possibility (or presence) of outliers. These techniques do not directly examine the outliers but accommodate them at no serious inconvenience (Barnett and Lewis 1974). Certain estimators, especially the mean and least squares estimators, are particularly vulnerable to outliers, or have low breakdown values2.\nFor this reason, researchers turn to robust or high breakdown methods to provide alternative estimators for these important aspects of the data. A common robust estimation method for univariate distributions involves the use of a trimmed mean, which is calculated by temporarily eliminating extreme observations at both ends of the sample (very high and low values) (Anscombe 1960). Alternatively, researchers may choose to compute a Windsorized mean, for which the highest and lowest observations are temporarily censored, and replaced with adjacent values from the remaining data.\nThe issue arises from the fact that robust qualities - however defined - must be bought at a price: poorer performance when the model is correct. This is usually reported by some trade-off between the conflicting requirements of robustness and accuracy.\nAs an example, lets look at the median which is often cited as a robust estimator. The downside of the median is that it is less efficient than the mean. This is because it does not take into account the precise value of each observation and hence does not use all information available in the data. The standard error of the median (\\(\\sigma_{median}\\)) for large samples and normal distributions is:\n\\[ \\sigma_{median} \\approx 1.25 \\frac{\\sigma}{\\sqrt{N}} = 1.25 \\sigma_{mean}\\]\nwhere \\(\\sigma\\) is the population standard deviation and \\(N\\) the sample size. Thus, the standard error of the median is about \\(25\\%\\) larger than that for the mean (Maindonald and Braun 2006, Chapter 4). Hence, the median is less efficient estimator when the model in correct, i.e the data come from normal distributions. Later, I will show that Bayesian analysis automatically delivers robustness whenever it is desirable without throwing away relevant information. But first I introduce how the apparatus generates data.\n The model Following Box and Tiao (1968) I assume that the apparatus produces good and bad measurements. So we have a “good” sampling distribution\n\\[G(x|\\theta)\\]\nparametrized by \\(\\theta\\). The “bad” sampling distribution\n\\[B(x|\\xi)\\]\npossibly containing an uninteresting parameter \\(\\xi\\). Data from \\(B(x|\\xi)\\) are useless or worse for estimating \\(\\theta\\), since their occurrence probability has nothing to do with \\(\\theta\\). Our sample consists of \\(n\\) observations\n\\[D = (x_1 \\dots x_n)\\] The trouble is we do not know which is which. However, we may be able to guess since a datapoint far away from the tails of \\(G(x|\\theta)\\) can be suspected of being bad. Let’s define\n\\[\\begin{equation} q_i = \\begin{cases} 1 \u0026amp; \\text{if the ith datapoint is good} \\\\ 0 \u0026amp; \\text{if it is bad,} \\end{cases} \\end{equation}\\]\nwith joint prior probabilities\n\\[p(q_1 \\dots q_n)\\]\nto the \\(2^n\\) sequences of good and bad.\nConsider the most common case where our prior information about the good and bad observations is invariant on the particular trial at which they occur. That is, the probability of any sequence of \\(n\\) good/bad observations depends only on the numbers \\(r\\), \\(n-r\\) of good and bad ones. Then, under de Finetti’s representation theorem (De Finetti 1972)\n\\[\\begin{equation} p(q_1 \\dots q_n) = \\int_{0}^{1} u^r (1-u)^{n-r} dg(u) \\tag{1} \\end{equation}\\]\nThe theorem above is equivalent to assuming that \\(q_i\\) are independent Bern(\\(u\\)) (Bernoulli) random variables with \\(u\\), given a prior distribution \\(g(u)\\). Consequently, our sampling distribution can be written as a probability mixture of the good and bad distributions\n\\[\\begin{equation} p(x|\\theta,\\xi,u) = u G(x|\\theta) + (1-u) B(x|\\xi) \\tag{2} \\end{equation}\\]\n\\(\\theta\\) can be thought of the parameter of interest while (\\(\\xi,u\\)) are nuisance parameters. In the next section, I show how a simple, flexible Bayesian solution allows for robustness. Throughout I assume \\(u\\) is unknown, which is in line with real-life scenarios.\n The solution Let \\(p(\\theta,\\xi,u)\\) be the joint prior density for the parameters. Under Bayes theorem their joint posterior density, given the data \\(D\\), becomes\n\\[p(\\theta,\\xi,u|D) \\propto L(\\theta,\\xi,u) p(\\theta,\\xi,u),\\]\nand from (2),\n\\[\\begin{equation} L(\\theta,\\xi,u) = \\prod_{i=1}^{n} \\Big[ u G(x|\\theta) + (1-u) B(x|\\xi) \\Big] \\tag{3} \\end{equation}\\]\nis the likelihood. The marginal posterior density for the parameter of interest \\(\\theta\\) is\n\\[\\begin{equation} p(\\theta|D) = \\int \\int p(\\theta,\\xi,u|D) d\\xi du. \\tag{4} \\end{equation}\\]\nAnother formulation of (4) is\n\\[ p(\\theta|D) = \\frac{p(\\theta) \\bar{L}(\\theta)} {\\int p(\\theta) \\bar{L}(\\theta) d\\theta}\\]\nwhere \\(p(\\theta)\\) is the marginal prior density for \\(\\theta\\) and \\(\\bar{L}(\\theta)\\) is the quasi-likelihood defined as\n\\[\\begin{equation} \\bar{L}(\\theta) = \\int \\int L(\\theta,\\xi,u) h(\\xi,u|\\theta) d\\xi du. \\tag{5} \\end{equation}\\]\nwhich results from decomposing the prior joint density \\(p(\\theta,\\xi,u)\\) into\n\\[p(\\theta,\\xi,u) = h(\\xi,u|\\theta) p(\\theta)\\]\nwhere \\(h(\\xi,u|\\theta)\\) is the joint prior for \\((\\xi,u)\\) given \\(\\theta\\). Substituting (3) into (5), we have\n\\[\\begin{equation} \\begin{split} \\bar{L}(\\theta) = \\int \\int h(\\xi,u|\\theta) d\\xi du \\Big[ u^n L(\\theta) + u^{n-1} (1-u) \\sum_{j=1}^n B(x_j|\\xi) L_j(\\theta) \\\\ + n^{n-2} (1-u)^2 \\sum_{j\u0026lt; k} B(x_j|\\xi) B(x_k|\\xi) L_{jk}(\\theta) + \\dots \\\\ + (1-u)^n B(x_1|\\xi) \\dots B(x_n|\\xi) \\Big] \\end{split} \\tag{6} \\end{equation}\\]\nwhere\n\\[\\begin{equation} \\begin{split} L(\\theta) = \\prod_{i = 1}^n G(x_i|\\theta) \\\\ L_j(\\theta) = \\prod_{i \\neq j} G(x_i|\\theta) \\\\ L_{jk}(\\theta) = \\prod_{i \\neq j,k} G(x_i|\\theta) \\dots \\end{split} \\end{equation}\\]\nare a sequence of likelihood functions for the good distributions in which we use all the data, all except \\(x_j\\), all except \\(x_j\\) and \\(x_k\\) etc. Note that the coefficient of \\(L(\\theta)\\) in (6),\n\\[\\begin{equation} \\int \\int h(\\xi,u|\\theta) u^n d\\xi du = \\int h(u|\\theta)u^n du, \\tag{7} \\end{equation}\\]\nis the probability that all the data \\(D\\) are good conditional on \\(\\theta\\)3. This is in the form (1), in which the function \\(g(u)\\) is the prior \\(h(u|\\theta)\\). Likewise, the coefficient of \\(L_j(\\theta)\\) is\n\\[ \\int \\int h(\\xi,u|\\theta) u^{n-1} (1-u) B(x_j|\\xi) d\\xi du =\\\\ \\int u^{n-1} (1-u) du \\int B(x_j|\\xi) h(\\xi,u|\\theta) d\\xi.\\]\nFollowing the same reasoning, this is the probability, given \\(\\theta\\), that the jth datapoint would be bad and would have the value \\(x_j\\) and the other data would be good. Putting \\(\\bar{L}(\\theta)\\) into words\n\\[\\begin{equation} \\begin{array}{l@{}l} \\bar{L}(\\theta) \u0026amp;{} = \\text{prob(all the data are good)} \\times \\text{(likelihood using all the data)} \\\\ \u0026amp;{} + \\sum_j \\text{prob(only $x_j$ bad)} \\times \\text{(likelihood using all the data except $x_j$)} \\\\ \u0026amp;{} + \\dots \\\\ \u0026amp;{} + \\text{prob(all the data are bad)}. \\end{array} \\label{quasiinwords} \\end{equation}\\]\nIn short, \\(\\bar{L}(\\theta)\\) is a weighted average of likelihoods resulting from every possible assumption about each datapoint \\(x_j\\), weighted by the prior probabilities of those assumptions.\n An example Suppose we are interested in a location parameter, and have a sample of 10 observations. But one datapoint \\(x_j\\) moves away from the cluster of the others. How will this datapoint affect our conclusions about \\(\\theta\\)? The answer depends on the model we specify. If we assume the sampling distribution \\(G(x|\\theta)\\) to be Gaussian i.e. \\(x \\sim N(\\theta, \\sigma)\\), and our prior for \\(\\theta\\) wide, then the Bayesian estimate will remain equal to the sample average and our datapoint \\(x_j\\) will pull the estimate far away from the average indicated by the nine other data values. However, this analysis assumes that we know in advance that \\(u =1\\), all the data are good i.e. come from \\(G\\). In such a case the study of datapoint \\(x_j\\) may be of significance since it gives us information about \\(\\theta\\). The rejection of \\(x_j\\) would then be fault. On the other hand, if we believe that \\(x_j\\) should be thrown out, then we don’t actually believe in our assumption that \\(u = 1\\) strongly enough to adhere to it in the presence of the this surprising datapoint. A model like (2) would then be more realistic.\n Connection with adversarial training in Machine Learning In fact, model (2) is the cornerstone of adversarial training in Machine Learning (ML). In adversarial training, the basic idea is to simply create and then incorporate adversarial data into the training process. The researcher then evaluates how robust is the output of the model to such perturbations of the input data. The entire area of adversarial ML studies ways to create robust learning algorithms that withstand such perturbations. The area of adversarial ML arose after observing that standard learning methods degrade rapidly in the presence of perturbations (Kurakin, Goodfellow, and Bengio 2016).\nThe formal study of robust estimation was initiated by (Huber 1964, 1965) who considered estimation procedures under the \\(\\epsilon\\)-contamination model, where samples are obtained from a mixture model of the form:\n\\[\\begin{equation} P_{\\epsilon} = (1 - \\epsilon) P + \\epsilon Q, \\label{Huber_contamination} \\end{equation}\\]\nwhere \\(P\\) is the uncontaminated target distribution, \\(Q\\) is an arbitrary outlier distribution and \\(\\epsilon\\) is the expected fraction of contamination. The distribution \\(Q\\) allows for arbitrary contamination, which may correspond to gross corruptions or more subtle deviations from the assumed model. This is exactly our model in (2).\nSummarising, the Bayesian solution can capture our prior knowledge about how the data are being generated. Allowing for a more flexible Bayesian model gives desirable qualities of robustness automatically. As a result, we may be able to bypass the need to derive robust estimators which, as we saw, come with drawbacks. This fact could be used in adversarial ML applications.\n References Anscombe, Frank J. 1960. “Rejection of Outliers.” Technometrics 2 (2): 123–46.\n Barnett, Vic, and Toby Lewis. 1974. Outliers in Statistical Data. Wiley.\n Box, George EP, and George C Tiao. 1968. “A Bayesian Approach to Some Outlier Problems.” Biometrika 55 (1): 119–29.\n De Finetti, Bruno. 1972. “Probability, Induction, and Statistics.”\n Dixon, Wilfred J. 1950. “Analysis of Extreme Values.” The Annals of Mathematical Statistics 21 (4): 488–506.\n Grubbs, Frank E. 1969. “Procedures for Detecting Outlying Observations in Samples.” Technometrics 11 (1): 1–21.\n Huber, Peter J. 1964. “Robust Estimation of a Location Parameter.” Ann. Math. Statist. 35 (1): 73–101. https://doi.org/10.1214/aoms/1177703732.\n ———. 1965. “A Robust Version of the Probability Ratio Test.” Ann. Math. Statist. 36 (6): 1753–8. https://doi.org/10.1214/aoms/1177699803.\n Jaynes, Edwin T. 2003. Probability Theory: The Logic of Science. Cambridge University Press.\n Kurakin, Alexey, Ian Goodfellow, and Samy Bengio. 2016. “Adversarial Machine Learning at Scale.” arXiv Preprint arXiv:1611.01236.\n Maindonald, John, and John Braun. 2006. Data Analysis and Graphics Using R: An Example-Based Approach. Vol. 10. Cambridge University Press.\n Serfling, Robert. 2011. “Asymptotic Relative Efficiency in Estimation.” International Encyclopedia of Statistical Science 23 (13): 68–72.\n    I define an outlier as an observation which seems “to deviate markedly from the other members of the data sample in which it appears.” (Grubbs 1969)?↩︎\n The breakdown point of an estimator is the proportion of incorrect observations (e.g. arbitrarily large observations) an estimator can handle before giving an incorrect (e.g., arbitrarily large) result. See Serfling (2011) for a formal definition.↩︎\n In (7) I assume that \\(u\\) and \\(\\xi\\) are independent. That is, \\(h(\\xi,u) = h(\\xi) h(u)\\), which a reasonable assumption.↩︎\n   ","date":1621641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621641600,"objectID":"e603f484edee079deca42e6cc1681971","permalink":"https://solon-karapanagiotis.com/post/outliers/outliers/","publishdate":"2021-05-22T00:00:00Z","relpermalink":"/post/outliers/outliers/","section":"post","summary":"This post is concerned with a ubiquitous problem of outliers. They are infamous for degrading the performance of many models/algorithms. As a result, ongoing attempts try to accommodate them by deriving robust estimators.","tags":["Bayes","outliers","ML"],"title":"Outliers: Which prior are you using?","type":"post"},{"authors":["Stavrinides V","et al"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"0f9dc1ec373f8edc46017e5761cb5677","permalink":"https://solon-karapanagiotis.com/publication/mri_prostate/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/mri_prostate/","section":"publication","summary":"**Background** False positive multiparametric magnetic resonance imaging (mpMRI) phenotypes prompt unnecessary biopsies. The Prostate MRI Imaging Study (PROMIS) provides a unique opportunity to explore such phenotypes in biopsy-naïve men with raised prostate-specific antigen (PSA) and suspected cancer. **Objective** To compare mpMRI lesions in men with/without significant cancer on transperineal mapping biopsy (TPM).**Design, setting, and participants** PROMIS participants (n=235) underwent mpMRI followed by a combined biopsy procedure at University College London Hospital, including 5-mm TPM as the reference standard. Patients were divided into four mutually exclusive groups according to TPM findings: (1) no cancer, (2) insignificant cancer, (3) definition 2 significant cancer (Gleason ≥3 + 4 of any length and/or maximum cancer core length ≥4 mm of any grade), and (4) definition 1 significant cancer (Gleason ≥4 + 3 of any length and/or maximum cancer core length ≥6 mm of any grade). **Outcome measurements and statistical analysis** Index and/or additional lesions present in 178 participants were compared between TPM groups in terms of number, conspicuity, volume, location, and radiological characteristics. **Results and limitations** Most lesions were located in the peripheral zone. More men with significant cancer had two or more lesions than those without significant disease (67% vs 37%; p ","tags":null,"title":"False Positive Multiparametric Magnetic Resonance Imaging Phenotypes in the Biopsy-naïve Prostate: Are They Distinct from Significant Cancer-associated Lesions? Lessons from PROMIS","type":"publication"},{"authors":null,"categories":["reporting","model evaluation","R"],"content":"  Overview Mitani and co-authors’ present a deep-learning algorithm trained with retinal images and participants’ clinical data from the UK Biobank to estimate blood-haemoglobin levels and predict the presence or absence of anaemia (Mitani et al. 2020). A major limitation of the study is the inadequate evaluation of the algorithm. I will show how a naïve classification (i.e. classify everybody as healthy) performs much better than their deep-learning approach, despite their model having AUC of around 80%. I will then explain why this is the case and finish with some thoughts on how (clinical) predictions models should be evaluated.\n Introduction The goal of the paper was to investigate whether anaemia can be detected via machine-learning algorithms trained using retinal images, study participants’ metadata or the combination of both.\nFirst, the authors develop a deep-learning algorithm to predict haemoglobin concentration (Hb) (which is the most reliable indicator of anaemia) and then three others to predict anaemia itself. They develop a deep convolutional neural network classification model to directly predict whether a patient is anaemic (rather than predicting Hb). They used the World Health Organization Hb cut-off values to label each participant as not having anaemia, mild, moderate or severe anaemia. One of the models was trained to classify: normal versus mild, moderate or severe. For concreteness, I’ll focus on this model but the reasoning below is valid for the others as well. Also, I focus on the combined model (images and metadata) because it showed the best performance (AUC of 0.88).\nThe authors present a detailed analysis of the data and their model. Nevertheless, a crucial point is missing. Is the model useful? If it is implemented tomorrow will it result in better care? This is important, especially since the authors argue their model potentially enables automated anaemia screening (see Discussion). This is slightly far-fetched in my opinion given they only evaluated their algorithm on a test set; with unsatisfactory results as I argue below. Algorithms need to be compared with human experts (i.e. ophthalmologists in this case), followed by extensive field testing to prove their trustworthiness and usefulness (Spiegelhalter 2020).\n The Issue As with many other models out there the authors have fallen in the trap of evaluating the absolute performance of their model when the important metric is the relative performance with respect to the actual reality of retinal screening. It is exactly the same idea when in clinical trials the experimental treatment is compared with the standard of care (or placebo). We want to see if the new treatment (deep-learning model classification in this case) is better than simply classifying every participant as having anaemia or not. This should be the benchmark. I call these naive classifications and I will demonstrate the model does not perform better than the naive rule of classifying everybody as healthy.\nFirst, some preliminary stuff. The performance of a model can be represented in a confusion matrix with four categories (see table below). True positives (TP) are positive examples that are correctly labelled as positives, and False positives (FP) are negative examples that are labelled incorrectly as positive. Likewise, True negatives (TN) are negatives labelled correctly as negative, and false negatives (FN) refer to positive examples labelled incorrectly as negative.\n Table 1: Confusion matrix showing correct classifications (in red) and incorrect (in blue)     Truth       positive  negative      Predict  positive  TP  FP     negative  FN  TN     Let’s use the information from the study to construct our confusion matrix using the data from Table 1 (last column; validation dataset) and Table 2 (column 1 and anaemia combined model). There are in total 10949 negatives (non anaemic) out of 11388 participants. The reported specificity is 0.7 and sensitivity is 0.875. Using these we can calculate the number of true negatives and true positives as follows:\n Specificity = true negative rate (TNR) = TN/#negative, so TN = 0.7*10949 = 7664. Sensitivity = true positive rate (TPR) = TP/ #positive, so TP = 0.875*439 = 384.  So the confusion matrix is\n   Truth       positive  negative      Predict  positive  384  3285     negative  55  7664     Looking at the table we see that in total 3285+55 = 3340 subjects have been misclassified. That is 29% misclassification rate.\nNow, let’s construct the confusion matrix for my naïve classification: “classify everybody as negative”,\n   Truth       positive  negative      Predict  positive  0  0     negative  439  10949     In total, 439 subjects have been misclassified. This is 4% misclassification rate.\nThis means the naive classification achieves 86% better performance!\n ROC (and AUC) is to blame Why was this not spotted by the authors? Probably because the model evaluation was based on the receiver operating curve (ROC) and area under the ROC (AUC). ROC curves can present an overly optimistic view of a model’s performance if there is a large skew in the class distribution. In this study the ratio positive (i.e. anaemic) to negative (i.e. not anaemic) participants is 439/10949 = 0.04 (see Table 2, validation column)!\nROC curves (and AUC) have the (un-)attractive property of being insensitive to changes in class distribution (Fawcett 2006). That is, if the proportion of positive to negative instances changes in a dataset, the ROC curves (and AUC) will not change. This is because ROC plots are based upon TPR and FPR which do not depend on class distributions. Increasing the number of positive samples by 10x would increase both TP and FN by 10x, which would not change the TPR at any threshold. Similarly, increasing the number of negative samples by 10x would increase both TN and FP by 10x, which would not change the FPR at any threshold. Thus, both the shape of the ROC curve and the AUC are insensitive to the class distribution. On the contrary, any performance metric that uses values from both columns will be inherently sensitive to class skews, for instance the misclassification rate.\nLet’s make this more concrete with a simple simulation example. I simulate one covariate, \\(X\\), which follows a standard Gaussian distribution in negative cases: \\(X \\sim N(0, 1)\\). Among positives, it follows \\(X \\sim N(1.5, 1)\\). The event rate (i.e. prevalence) is varied to be \\(20\\%\\) (I call this scenario 1) and \\(2\\%\\) (scenario 2). (The \\(2\\%\\) is close to the one observed in the study \\(\\approx 4\\%\\)). Then, I derive true risks (\\(R\\)) based on the event rate (\\(ER\\)) and the density of the covariate distributions for positives (\\(D_p\\)) and negatives (\\(D_{n}\\)) at the covariate values:\n\\[ R = \\frac{ER × D_p}{[ER × D_p] + [(1 − ER) × D_{n}]}.\\]\nI simulate two large samples of 5000 and 50000 and plot the ROC. The two plots below are almost identical despite the fact that scenario 2 has 10x more negative examples than scenario 1.\nlibrary(pROC) library(tibble) sim_data \u0026lt;- function(n_positives, n_negatives){# simulates dataset as described above and calculates the ROC # input arguments: the number of positives and negatives y \u0026lt;- c(rep(0, n_negatives), rep(1, n_positives)) # binary response x \u0026lt;- c(rnorm(n_negatives), rnorm(n_positives, mean = 1.5)) # simulate covariate df \u0026lt;- data.frame(y = y, x = x) ER \u0026lt;- mean(df$y) # event rate Dp \u0026lt;- dnorm(df$x, mean = 1.5, sd = 1) # covariate density for positives Dn \u0026lt;- dnorm(df$x, mean = 0, sd = 1) # covariate density for negatives true_risk \u0026lt;- (ER * Dp)/((ER * Dp) + ((1 - ER) * Dn)) # true risks roc_sim \u0026lt;- roc(df$y, true_risk) # calculates ROC curve df \u0026lt;- tibble(FPR = 1 - roc_sim$specificities, # false positive rate TPR = roc_sim$sensitivities) # true positive rate return(df) } n.sims \u0026lt;- 20 # times simulation is repeated n.positives \u0026lt;- 1000 # number of positives n.negatives \u0026lt;- 4000 # number of negatives library(purrr) library(dplyr) # scenario 1 multiplier \u0026lt;- 1 # the multiplier adjusts the number of the negatives - so I can have the event rate I want sims1 \u0026lt;- n.sims %\u0026gt;% rerun(sim_data(n.positives, n.negatives * multiplier)) %\u0026gt;% map(~ data.frame(.x)) %\u0026gt;% plyr::ldply(., data.frame, .id = \u0026quot;Name\u0026quot;) %\u0026gt;% mutate(sims = rep(1:n.sims, each = sum(n.positives + n.negatives * multiplier) + 1), Scenario = \u0026quot;Scenario 1\u0026quot;) # scenario 2 multiplier \u0026lt;- 10 sims2 \u0026lt;- n.sims %\u0026gt;% rerun(sim_data(n.positives, n.negatives * multiplier)) %\u0026gt;% map(~ data.frame(.x)) %\u0026gt;% plyr::ldply(., data.frame, .id = \u0026quot;Name\u0026quot;) %\u0026gt;% mutate(sims = rep(1:n.sims, each = sum(n.positives + n.negatives * multiplier) + 1), Scenario = \u0026quot;Scenario 2\u0026quot;) df_final \u0026lt;- rbind(sims1, sims2) library(ggplot2) ggplot(df_final) + geom_line(aes(x = FPR, y = TPR, group = sims, col = Scenario), alpha = 0.8) + facet_grid(~ Scenario)  Figure 1: ROC plots for each scenario; 20 repetitions each.   Unequal misclassification costs Of course, my naive classification can be easily debated by noting that the misclassification rate makes an inherent assumption, which is unlikely to be true in anaemia screening: it assumes that misclassifying someone with anaemia is of the same severity as misclassifying a healthy subject. This implies that one type of error is more costly (i.e. worse) than the other. In other words, the costs are asymmetric. I agree that most of time this is the case. This information should be taken into account when evaluating or fitting models.\nSome methods to account for differing consequences of correct and incorrect classification when evaluating models are the Weighted Net Reclassification Improvement (Pencina et al. 2011), Relative Utility (Baker et al. 2009), Net Benefit (Vickers and Elkin 2006) and the \\(H\\) measure (Hand 2009). Another option is to design models/algorithms that take misclassification costs into consideration. This area of research is called cost-sensitive learning (Elkan 2001).\nThis is another reason the ROC (AUC) is inadequate metric (in addition to the insensitivity in class imbalance). It ignores clinical differentials in misclassification costs and, therefore, risks finding a model worthwhile (or worthless) when patients and clinicians would consider otherwise. Strictly speaking, ROC weighs changes in sensitivity and specificity equally only where the curve slope equals one (Fawcett 2006). Other points assign different weights, determined by curve shape and without considering any clinically meaningful information. Thus, AUC can consider a model that increases sensitivity at low specificity superior to one that increases sensitivity at high specificity. However, in some situations, in disease screening for instance, better tests must increase sensitivity at high specificity to avoid numerous false positives.\n A way forward: estimate and validate probabilities Ultimately, the quality of algorithms is exposed to the nature of the performance metrics chosen. We must carefully choose the goals we ask these systems to optimize. Evaluation of models for use in healthcare should take the intended purpose of the model into account. Metrics such as AUC are rarely of any use in clinical practise. AUC represents how likely it is that the model will rank a pair of subjects; one with anaemia and one without, in the correct order, across all possible thresholds. More intuitively, AUC is the chance that a randomly selected participant with anaemia will be ranked above a randomly selected healthy participant. However, patients do not walk into the clinician’s room in pairs, and patients want their results, rather than the order of their results compared with another patient. They care about their individual risk of having a disease/condition (being anaemic in this case). Hence, the focus of modelling should be on estimating and validating risks/probabilities rather than the chance of correctly ranking a pair of patients.\nConsequently, model evaluation/comparison should focus (primarily) on calibration. Calibration refers to the agreement between observed and predicted probabilities. This means that for future cases predicted to be in class \\(A\\) with probability \\(p\\), a proportion of \\(p\\) cases will truly belong in class \\(A\\), and this should be true for all \\(p\\) in (0,1). In other words, for every 100 patients given a risk of \\(p\\)%, close to \\(p\\) have the event. Calibration approaches appropriate for representing prediction accuracy are crucial, especially when treatment decisions are made based on probability thresholds. Calibration of a model can be evaluated graphically by plotting expected against observed probabilities (Steyerberg et al. 2004) or using an aggregate score. The most widely used is the Brier score, which is given by the average over all squared differences between an observation and its predicted probability (Brier 1950). (It has nice properties (Gneiting and Raftery 2007; see e.g. Spiegelhalter 1986)).\nTo conclude, machine learning models in healthcare need rigorous evaluation. Beyond ethical, legal and moral issues, the technical/statistical fitness of the models needs thorough assessment. Statistical analysis should consider clinically relevant evaluation metrics. Motivated by the paper of Mitani et al. (2020) I have re-demonstrated why AUC is an irrelevant metric for clinical practise. This is because it is insensitive to class imbalances and integrates over all error regimes (under the best case scenario). This becomes increasingly important in predicting rare outcomes, where operating in a regime that corresponds to a high false positive rate may be impractical, because costly interventions might be applied in situations in which patients are unlikely to benefit. A way forward is to focus on evaluating predictions rather than (in addition to) classifications. That is, focus on estimating and evaluating probabilities. These convey more useful information to clinicians and patients in order to aid decision making.\n Further reading The limitations of the ROC (and AUC) have been discussed in\n Cook NR . Use and misuse of the receiver operating characteristic curve in risk prediction. Circulation (2007).\n Pencina, Michael J., et al. “Evaluating the added predictive ability of a new marker: from area under the ROC curve to reclassification and beyond.” Statistics in medicine (2008).\n Hand, David J. “Evaluating diagnostic tests: the area under the ROC curve and the balance of errors.” Statistics in medicine (2010).\n Hand, David J., and Christoforos Anagnostopoulos. “When is the area under the receiver operating characteristic curve an appropriate measure of classifier performance?.” Pattern Recognition Letters (2013).\n Halligan, Steve, Douglas G. Altman, and Susan Mallett. “Disadvantages of using the area under the receiver operating characteristic curve to assess imaging tests: a discussion and proposal for an alternative approach.” European radiology (2015).\n  On probability estimation and evaluation:\n Kruppa, Jochen, Andreas Ziegler, and Inke R. König. “Risk estimation and risk prediction using machine-learning methods.” Human genetics (2012).\n Kruppa, Jochen, et al. “Probability estimation with machine learning methods for dichotomous and multicategory outcome: Theory.” Biometrical Journal (2014).\n Kruppa, Jochen, et al. “Probability estimation with machine learning methods for dichotomous and multicategory outcome: Applications.” Biometrical Journal (2014).\n   References Baker, S. G., Cook, N. R., Vickers, A., and Kramer, B. S. (2009), “Using relative utility curves to evaluate risk prediction,” Journal of the Royal Statistical Society: Series A (Statistics in Society), Wiley Online Library, 172, 729–748.  Brier, G. W. (1950), “Verification of forecasts expressed in terms of probability,” Monthly weather review, 78, 1–3.  Elkan, C. (2001), “The foundations of cost-sensitive learning,” in International joint conference on artificial intelligence, Lawrence Erlbaum Associates Ltd, pp. 973–978.  Fawcett, T. (2006), “An introduction to ROC analysis,” Pattern recognition letters, Elsevier, 27, 861–874.  Gneiting, T., and Raftery, A. E. (2007), “Strictly proper scoring rules, prediction, and estimation,” Journal of the American statistical Association, Taylor \u0026amp; Francis, 102, 359–378.  Hand, D. J. (2009), “Measuring classifier performance: A coherent alternative to the area under the ROC curve,” Machine learning, Springer, 77, 103–123.  Mitani, A., Huang, A., Venugopalan, S., Corrado, G. S., Peng, L., Webster, D. R., Hammel, N., Liu, Y., and Varadarajan, A. V. (2020), “Detection of anaemia from retinal fundus images via deep learning,” Nature Biomedical Engineering, Nature Publishing Group, 4, 18–27.  Pencina, M. J., D’Agostino Sr, R. B., and Steyerberg, E. W. (2011), “Extensions of net reclassification improvement calculations to measure usefulness of new biomarkers,” Statistics in medicine, Wiley Online Library, 30, 11–21.  Spiegelhalter, D. (2020), “Should we trust algorithms?” Harvard Data Science Review, 2. https://doi.org/10.1162/99608f92.cb91a35a.  Spiegelhalter, D. J. (1986), “Probabilistic prediction in patient management and clinical trials,” Statistics in medicine, Wiley Online Library, 5, 421–433.  Steyerberg, E. W., Borsboom, G. J., Houwelingen, H. C. van, Eijkemans, M. J., and Habbema, J. D. F. (2004), “Validation and updating of predictive logistic regression models: A study on sample size and shrinkage,” Statistics in medicine, Wiley Online Library, 23, 2567–2586.  Vickers, A. J., and Elkin, E. B. (2006), “Decision curve analysis: A novel method for evaluating prediction models,” Medical Decision Making, Sage Publications Sage CA: Thousand Oaks, CA, 26, 565–574.    ","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582156800,"objectID":"d02f5cf360cbef5a25ac3b0ce830fda1","permalink":"https://solon-karapanagiotis.com/post/auc_post/model-evaluation-auc/","publishdate":"2020-02-20T00:00:00Z","relpermalink":"/post/auc_post/model-evaluation-auc/","section":"post","summary":"Overview Mitani and co-authors’ present a deep-learning algorithm trained with retinal images and participants’ clinical data from the UK Biobank to estimate blood-haemoglobin levels and predict the presence or absence of anaemia (Mitani et al.","tags":["(ML) reporting","model evaluation","R"],"title":"Naive classification beats deep-learning","type":"post"},{"authors":null,"categories":["probability","R"],"content":" It is usually taught in statistics classes that Binomial probabilities can be approximated by Poisson probabilities, which are generally easier to calculate. This approximation is valid “when \\(n\\) is large and \\(np\\) is small,” and rules of thumb are sometimes given.\nIn this post I’ll walk through a simple proof showing that the Poisson distribution is really just the Binomial with \\(n\\) (the number of trials) approaching infinity and \\(p\\) (the probability of success in each trail) approaching zero. I’ll then provide some numerical examples to investigate how good is the approximation.\nProof The Binomial distribution describes the probability that there will be \\(x\\) successes in a sample of size \\(n\\), chosen with replacement from a population where the probability of success is \\(p\\).\nLet \\(X \\sim Binomial(n, p)\\), that is\n\\[\\begin{equation} \\tag{1} P(X = x) = {n\\choose x} p^x (1-p)^{n-x}, \\end{equation}\\]\nwhere \\(x= 0, 1, \\dots, n\\). Define the number\n\\[\\lambda = np\\]\nThis is the rate of success. That’s the number of trials \\(n\\)—however many there are—times the chance of success \\(p\\) for each of those trials. If we repeat the experiment every day, we will be getting \\(\\lambda\\) successes per day on average.\nSolving for \\(p\\), we get:\n\\[ p = \\frac{\\lambda}{n}\\] We then substitute this into (1), and take the limit as \\(n\\) goes to infinity\n\\[ \\lim_{n \\to \\infty}P(X = x) = \\lim_{n \\to \\infty} \\frac{n!}{x!(n-x)!} \\bigg( \\frac{\\lambda}{n} \\bigg)^x \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^{n-x}\\]\nI then collect the constants (terms that don’t depend on \\(n\\)) in front and split the last term into two\n\\[\\begin{equation} \\tag{2} \\frac{\\lambda^x}{x!} \\lim_{n \\to \\infty} \\color{blue}{\\frac{n!}{(n-x)!} \\bigg( \\frac{1}{n} \\bigg)^x} \\color{red}{ \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^n } \\color{green}{\\bigg( 1-\\frac{\\lambda}{n} \\bigg)^{-x}} \\end{equation}\\]\nNow let’s take the limit of this right-hand side one term at a time.\nWe start with the blue term  \\[\\color{blue}{ \\lim_{n \\to \\infty} \\frac{n!}{(n-x)!} \\bigg( \\frac{1}{n} \\bigg)^x }\\] The numerator and denominator can be expanded as follows\n\\[\\color{blue}{ \\lim_{n \\to \\infty} \\frac{(n)(n-1)(n-2)\\dots(n-x)(n-x-1)\\dots (1)}{(n-x)(n-x-1)(n-x-2)\\dots (1)}\\bigg( \\frac{1}{n} \\bigg)^x }\\]\nThe \\((n-x)(n-x-1)\\dots(1)\\) terms cancel from both the numerator and denominator, leaving the following\n\\[\\color{blue}{ \\lim_{n \\to \\infty} \\frac{(n)(n-1)(n-2)(n-x+1)}{n^x} }\\] This can be rewrited as\n\\[\\color{blue}{ \\lim_{n \\to \\infty} \\frac{n}{n} \\frac{(n-1)}{n} \\frac{(n-2)}{n} \\frac{(n-x+1)}{n} }\\] This is because there were \\(x\\) terms in both the numerator and denominator. Clearly, every one of these \\(x\\) terms approaches 1 as \\(n\\) approaches infinity. So we know this just simplifies to one. So we’re done with the first step.\nNow we focus on the red term of (2)  \\[\\color{red}{ \\lim_{n \\to \\infty} \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^n }\\]\nRecall the definition of \\(e= 2.7182\\dots\\) is\n\\[ \\lim_{a \\to \\infty} \\bigg(1 + \\frac{1}{a}\\bigg)^a\\] Our goal here is to find a way to manipulate our expression to look more like the definition of \\(e\\), which we know the limit of. Let’s define a number \\(a\\) as\n\\[ a = -\\frac{n}{\\lambda}\\]\nSubstituting it into our expression we get\n\\[ \\color{red}{ \\lim_{n \\to \\infty} \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^n = \\lim_{n \\to \\infty} \\bigg( 1+\\frac{1}{a} \\bigg)^{-a\\lambda} = e^{-\\lambda} }\\] So we’ve finished with the middle term.\nThe third term of (2) is  \\[\\color{green}{ \\lim_{n \\to \\infty} \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^{-x} }\\] As \\(n\\) approaches infinity, this term becomes \\(1^{-x}\\) which is equal to one. And that takes care of our last term.\nPutting these together we can re-write (2) as\n\\[ \\frac{\\lambda^x}{x!} \\lim_{n \\to \\infty} \\color{blue}{ \\frac{n!}{(n-x)!} \\bigg( \\frac{1}{n} \\bigg)^x} \\color{red}{ \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^n} \\color{green}{ \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^{-x} } = \\frac{\\lambda^x}{x!} \\color{red}{ e^{-\\lambda} }\\] which is the probability mass function of a Poisson random variable \\(Y\\), i.e\n\\[P(Y = y) = \\frac{\\lambda^y}{y!} e^{-\\lambda}\\]\nwhere \\(y = 0, 1, \\dots\\). So we have shown that the Poisson distribution is a special case of the Binomial, in which the number of trials grows to infinity and the chance of success in any trial approaches zero. And that completes the proof.\nCasella and Berger (2002) provide a much shorter proof based on moment generating functions.\nA natural question is how good is this approximation? It turns out it is quite good even for moderate \\(p\\) and \\(n\\) as we’ll see with a few numerical examples.\n Code A rule of thumb says for the approximation to be good:\n “The sample size \\(n\\) should be equal to or larger than 20 and the probability of a single success, \\(p\\), should be smaller than or equal to 0.05. If \\(n\\) \u0026gt; 100, the approximation is excellent if \\(np\\) is also \u0026lt; 10.”\n Let’s try a few scenarios. I have slightly modified the code from here.\n# plots the pmfs of Binomial and Poisson pl \u0026lt;- function(n, p, a, b) { clr \u0026lt;- rainbow(15)[ceiling(c(10.68978, 14.24863))] lambda \u0026lt;- n * p mx \u0026lt;- max(dbinom(a:b, n, p)) plot( c(a:b, a:b), c(dbinom(a:b, n, p), dpois(a:b, lambda)), type = \u0026quot;n\u0026quot;, main = paste(\u0026quot;Poisson Approx. to Binomial, n=\u0026quot;, n, \u0026quot;, p=\u0026quot;, p, \u0026quot;, lambda=\u0026quot;, lambda), ylab = \u0026quot;Probability\u0026quot;, xlab = \u0026quot;x\u0026quot;) points((a:b) - .15, dbinom(a:b, n, p), type = \u0026quot;h\u0026quot;, col = clr[1], lwd = 10) points((a:b) + .15, dpois(a:b, lambda), type = \u0026quot;h\u0026quot;, col = clr[2], lwd = 10) legend(b - 3.5, mx, legend = c(\u0026quot;Binomial(x,n,p)\u0026quot;, \u0026quot;Poisson(x,lambda)\u0026quot;), fill = clr, bg = \u0026quot;white\u0026quot;) } I start with the recommendation: \\(n\\) = 20, \\(p\\) = 0.05. This gives \\(\\lambda= 1\\). Already the approximation seems reasonable.\npl(20, 0.05, 0, 10) For \\(n\\) = 10, \\(p\\) = 0.3 it doesn’t seem to work very well.\npl(10, 0.3, 0, 10) But if we increase \\(n\\) and decrease \\(p\\) in order to come home with the same \\(\\lambda\\) value things improve.\npl(100, 0.03, 0, 10) Lastly, for 1000 trials the distributions are indistinguishable.\npl(1000, 0.003, 0, 10)  References Casella, George, and Roger L Berger. 2002. Statistical Inference. Vol. 2. Duxbury Pacific Grove, CA.\n   ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"420fc3eee2f69b74d0c4b10af0c4ab08","permalink":"https://solon-karapanagiotis.com/post/approx_binomial/approximating-binomial-with-poisson/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/approx_binomial/approximating-binomial-with-poisson/","section":"post","summary":"It is usually taught in statistics classes that Binomial probabilities can be approximated by Poisson probabilities, which are generally easier to calculate. This approximation is valid “when \\(n\\) is large and \\(np\\) is small,” and rules of thumb are sometimes given.","tags":["probability","R"],"title":"Approximating Binomial with Poisson","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://solon-karapanagiotis.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["probability"],"content":" I present a solution to a modification of the “hardest logic puzzle ever” using probability theory.\nBackground “The hardest logic puzzle” was originally presented by Boolos (1996) and since then it has been amended several times in order to make it harder (see Rabern and Rabern 2008; Novozhilov 2012).\nThe puzzle: Three gods A, B, and C are called, in some order, True, False, and Random. True always speaks truly, False always speaks falsely, but whether Random speaks truly or falsely is a completely random matter. Your task is to determine the identities of A, B, and C by asking three yes-no questions; each question must be put to exactly one god. The gods understand English, but will answer all questions in their own language, in which the words for “yes” and “no” are “da” and “ja,” in some order. You do not know which word means which. \nBoolos (1996) then provides the following guidelines:\n1. It could be that some god gets asked more than one question (and hence that some god is not asked any question at all).\n2. What the second question is, and to which god it is put, may depend on the answer to the first question. (And of course similarly for the third question.)\n3. Whether Random speaks truly or not should be thought of as depending on the flip of a coin hidden in his brain: if the coin comes down heads, he speaks truly; if tails, falsely.\n4. Random will answer da or ja when asked any yes-no question.\nRabern and Rabern (2008) proposed to modify the third point above with the following: “Whether Random answers ‘da’ or ‘ja’ should be thought of as depending on the flip of a coin hidden in his brain: if the coin comes down heads, he answers ‘yes’; if tails, ‘no’.”\nBoolos’ article includes multiple ways of solving the problem. Rabern and Rabern (2008) give a simpler solution. The main ideas for the solutions can be found here and here.\n My solution My solution is based on the long-run frequency interpretation of probability. It involves two steps. At the first step we will identify the Random god and in step 2 we distinguish between the True and False gods.\nStep 1\nImagine the following scenario: you keep asking the same question to each god. The question is different for each god. Under the interpretation of probability as long-run frequency both True and False will always give the same answer. For example, if A is the True god he will always answer “da” or “ja” and similarly the False god will always answer the opposite. The crucial point is that Random will change between “da” and “ja” because his answers are random, “they depend on the flip of a coin hidden in his brain”. Suppose you ask your question to Random ten times, and assuming the coin in his head is fair (i.e., P(heads) = P(tails) = 0.5) then the probability that all his answers are same ( “da” or “ja” ) is \\(0.5^{10}\\), that is highly unlikely. In fact, you do not need to pre-specify how many times you ask the question, since the moment a given god switches from “da” to “ja” or vice-versa you know he is Random. Having identified Random we proceed to distinguish between True and False. An example question to each one is “are you True”?\nStep 2\nFor simplicity let’s assume C is Random. Now, we only need to identify one more god. Let’s use god A for illustration. All possibilities regarding god A and the word “da” are given below:\nA is True and “da” means “yes”, A is True and “da” means “no”, A is False and “da” means “yes”, A is False and “da” means “no”,  Then ask A the following question:\nQ1: Is C Random?\nAnd B:\nQ2: Is A True?\nFor each scenario above we end up with the following pattern:\n If scenario (1) the answers are “da” and “ja” for Q1 and Q2 respectively. If scenario (2) the answers are “ja” and “da”. If scenario (3) the answers are “ja” and “da”. If scenario (4) the answers are “da” and “da”.  Looking more carefully at the answers we distinguish 3 distinct patterns for the answers:\n P1: “da”and “ja”, P2: “ja” and “da” and P3: “da” and “da”.  Furthermore, P1 and P3 are unique, they appear only once. That means if the gods answer Q1 and Q2 using P1 or P3 we have identified them and the game is over! For example, if they answer with P3 then scenario (4) was correct: A is False and “da” means “no” and consequently B is True and “ja” means “yes”. If they answer using P2 then we need a further question because both scenarios (2) and (3) may be right. We can ask A: (repeat the 1st question)\nQ3: Are you True?\nNow, if scenario (2) the answer is “ja” and if scenario (3) the answer is “da”.\nUsing this approach we have also identified the meanings of “da” and “ja”.\n Comments The modification I used was allowing each question to be put to more than one god. In step 1 the question “Are you True?” was put to all gods and was repeated in step 2 as Q3. So technically I have solved the puzzle using only three questions in total, but allowing myself to repeat the same questions to more than one god.\nBoolos (1996) provided his solution in the same article in which he introduced the puzzle. He states that the “first move is to find a god that you can be certain is not Random, and hence is either True or False”. My approach does the reverse; first identifies the Random god and then the True and False gods.\n References Boolos, George. 1996. “The Hardest Logic Puzzle Ever.” The Harvard Review of Philosophy 6 (1): 62–65.\n Novozhilov, Nikolay. 2012. “The Hardest Logic Puzzle Ever Becomes Even Tougher.” arXiv Preprint arXiv:1206.1926.\n Rabern, Brian, and Landon Rabern. 2008. “A Simple Solution to the Hardest Logic Puzzle Ever.” Analysis 68 (2): 105–12.\n   ","date":1533254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533254400,"objectID":"485f2f95dcc4320e57b7608405c5a95b","permalink":"https://solon-karapanagiotis.com/post/hardest_puzzle/the-hardest-logic-puzzle/","publishdate":"2018-08-03T00:00:00Z","relpermalink":"/post/hardest_puzzle/the-hardest-logic-puzzle/","section":"post","summary":"I present a solution to a modification of the “hardest logic puzzle ever” using probability theory.\nBackground “The hardest logic puzzle” was originally presented by Boolos (1996) and since then it has been amended several times in order to make it harder (see Rabern and Rabern 2008; Novozhilov 2012).","tags":["probability"],"title":"Another solution to the 'The Hardest Logic Puzzle Ever' using probability","type":"post"},{"authors":null,"categories":["data analysis"],"content":" Recently, I came across this very interesting article published in Science about how plastic waste is associated with disease on coral reefs (J. B. Lamb et al. 2018). The main conclusions are\ncontact with plastic increases the probability of disease,\n the morphological structure of the reefs is associated with the probability of being in contact with plastic with more complex ones being more likely to be affected by plastic,\n the plastic levels correspond to estimates of mismanaged plastic waste into the ocean.  Overall, this study provides evidence how plastic waste negatively affects coral reefs, making them more susceptible to diseases. The authors made available both the datasets they used and the code (both can be downloaded from J. Lamb et al. 2018 - an excellent example of reproducible research). The methods section is straightforward to follow (see Supplementary Materials). My comment is about the 2nd point above, and more specifically the methodology that led to this conclusion (see Fig. 4 of the article). The issue is the authors interpret the models they are using wrongly. Let me explain …\nTheir model is a simple generalised linear mixed model (GLMM) - binomial error distribution and logistic link. The outcome is the disease prevalence (binary) among coral reefs with different morphology. The morphology assignments were massive, tabular, and branching (3-level categorical covariate). The morphological assignments were treated as fixed factors and the site as random (in order to take into account the correlation between reefs due to their geographical position). The model is\n\\[ logit(Disease Presense_{ik}) = \\sum_j \\beta_j x_{ik} + b_i \\]\nwhere \\(j_{1:3} = \\{massive, tabular, branching\\}\\) and \\(b_i\\) are the reef-specific intercepts. Such a \\(b_i\\) represents the deviation of the intercept of a specific reef from the average intercept in the group to which that reef belongs, i.e deviation from \\(\\beta_1\\), \\(\\beta_2\\) or \\(\\beta_3\\). The model is fitted only for reefs unaffected by plastic waste. The output is given in Fig. 4(B) in the paper. The conclusion is the disease risk increases from massive to branching and tabular reefs when not in contact with plastic debris (Fig. 4(B) and table S13).\nThe issue with this figure is the authors give a population-average interpretation of the coefficients. In GLLMs the fixed effects have a site-specific interpretation but not a population-average one. Let us now consider the logistic random-intercepts model above. The conditional means \\(E[Disease Presense_{ik}|b_i]\\) are given by\n\\[ E[Disease Presense_{ik}|b_i] = \\frac{\\exp(\\sum_j \\beta_j x_{ik} + b_i)}{1 + \\exp(\\sum_j \\beta_j x_{ik} + b_i)} \\] where \\(E[.]\\) is the expectation operator. The above model assumes logistic change in prevalence of disease for each morphology, all having different intercepts \\(\\beta_0 + b_i\\). The average reef, i.e, the reef with intercept \\(b_i = 0\\), has disease probability given by\n\\[ E[Disease Presense_{ik}|b_i = 0] = \\frac{\\exp(\\sum_j \\beta_j x_{ik} + 0)}{1 + \\exp(\\sum_j \\beta_j x_{ik} + 0)} \\]\nwhich is what the authors have calculated and produced Fig. 4. In other words, the authors have calculated the probability of disease for an “average” reef. They proceed interpreting this as marginal effect, which is wrong.\nThe issue arises due to the conditional interpretation, conditionally upon level of random effects, of the \\(\\beta\\)s in a GLMM model. And this is due to the fact that \\(E[g(Y )] \\neq g[E(Y)]\\) unless \\(g\\) is linear, which is not the case for this model. In what follows I fit the same model and demonstrate how the conclusions change when conditioning of different levels of the random coefficients. The code the authors use is\n# GLMM, Baseline Disease levels for different growth forms, Asia Pacific -------- library(lme4) Normal.Disease.Growth = glmer(Disease ~ -1 + Growth2+(1|Reef_Name), data = Plastic[which(Plastic$Plastic==0),], family = \u0026#39;binomial\u0026#39;, control = glmerControl(optimizer =\u0026quot;bobyqa\u0026quot;)) # As a sidenote: This code uses a Laplace approximation (nAGQ = 1 - the default) on the integral over the random effects space. \u0026quot;Values greater than 1 produce greater accuracy in the evaluation of the log-likelihood at the expense of speed\u0026quot;. The authors of the package suggest values up to 25 (see the documentation).  The following reproduces Fig. 4(B) of the publication.\n# PDF, Baseline disease levels by growth form ----------------------------- NormalDisease.by.Growth = data.frame(Tabular = rnorm(100000, mean = -3.1332, sd = .1549), Massive = rnorm(100000, mean = -3.8153, sd = .1095), Branching = rnorm(100000, mean = -3.5534, sd = .1103)) NormalDisease.by.Growth$Tabularbt = plogis(NormalDisease.by.Growth$Tabular) NormalDisease.by.Growth$Massivebt = plogis(NormalDisease.by.Growth$Massive) NormalDisease.by.Growth$Branchingbt = plogis(NormalDisease.by.Growth$Branching) NormalDisease.by.Growth = gather(NormalDisease.by.Growth, Growth, Estimate, Tabularbt:Branchingbt) library(ggplot2) ggplot(aes(x = Estimate*100), data = NormalDisease.by.Growth) + geom_density(aes(y = ..scaled.., fill = Growth)) + scale_x_continuous(limits = c(0, 10)) + ylab(\u0026quot;\u0026quot;) + labs(fill = \u0026quot;Morphology\u0026quot;) It is evident from the code that they plot the fixed effects estimates with their standard errors. This plot ignores the random effects and it only takes into consideration the variation of the fixed coefficients \\(\\beta_j\\). To get an idea for the variability of the random effects I simulate them from the model and plot them. Points that are distinguishable from zero (i.e. the confidence band based on level does not cross the red line) are highlighted. We see substantial variation on the random effects estimates with many “outliers” with both high and low averages that need to be accounted for.\nlibrary(merTools) sim_rfs_Normal.Disease \u0026lt;- REsim(Normal.Disease.Growth, n.sims = 200) plotREsim(sim_rfs_Normal.Disease) What the authors are effectively doing in Fig. 4(B) (see density plot above) is presenting the results for reefs with \\(b_i = 0\\) which corresponds to the red horizontal line. Let’s see how the density plot changes when we condition on more “extreme” reefs. I use the 0.1 and 0.9 quantiles.\nquantile0.9 \u0026lt;- REquantile(Normal.Disease.Growth, quantile = 0.9, groupFctr = \u0026quot;Reef_Name\u0026quot;) #which(sim_rfs_Normal.Disease$groupID == quantile0.9) quantile0.1 \u0026lt;- REquantile(Normal.Disease.Growth, quantile = 0.1, groupFctr = \u0026quot;Reef_Name\u0026quot;) #which(sim_rfs_Normal.Disease$groupID == quantile0.1) NormalDisease.by.Growth_quantile0.9 = data.frame( Tabular = rnorm(100000, mean = -3.1332 + 1.495773, sd = .1549), Massive = rnorm(100000, mean = -3.8153 + 1.495773, sd = .1095), Branching = rnorm(100000, mean = -3.5534 + 1.495773, sd = .1103)) NormalDisease.by.Growth_quantile0.9$Tabularbt = plogis(NormalDisease.by.Growth_quantile0.9$Tabular) NormalDisease.by.Growth_quantile0.9$Massivebt = plogis(NormalDisease.by.Growth_quantile0.9$Massive) NormalDisease.by.Growth_quantile0.9$Branchingbt = plogis(NormalDisease.by.Growth_quantile0.9$Branching) NormalDisease.by.Growth_quantile0.9 = gather(NormalDisease.by.Growth_quantile0.9, Growth, Estimate, Tabularbt:Branchingbt) NormalDisease.by.Growth_quantile0.1 = data.frame( Tabular = rnorm(100000, mean = -3.1332 - 1.689569, sd = .1549), Massive = rnorm(100000, mean = -3.8153 - 1.689569, sd = .1095), Branching = rnorm(100000, mean = -3.5534 - 1.689569, sd = .1103)) NormalDisease.by.Growth_quantile0.1$Tabularbt = plogis(NormalDisease.by.Growth_quantile0.1$Tabular) NormalDisease.by.Growth_quantile0.1$Massivebt = plogis(NormalDisease.by.Growth_quantile0.1$Massive) NormalDisease.by.Growth_quantile0.1$Branchingbt = plogis(NormalDisease.by.Growth_quantile0.1$Branching) NormalDisease.by.Growth_quantile0.1 = gather(NormalDisease.by.Growth_quantile0.1, Growth, Estimate, Tabularbt:Branchingbt) NormalDisease.by.Growth$ID = \u0026quot;average\u0026quot; NormalDisease.by.Growth_quantile0.1$ID = \u0026quot;0.1quantile\u0026quot; NormalDisease.by.Growth_quantile0.9$ID = \u0026quot;0.9quantile\u0026quot; overall \u0026lt;- rbind(NormalDisease.by.Growth, NormalDisease.by.Growth_quantile0.1, NormalDisease.by.Growth_quantile0.9) ggplot(aes(x = Estimate*100, col = ID), data = overall) + geom_density(aes(y = ..scaled.., fill = Growth), alpha = 0.9, size = 1.3) + scale_fill_brewer(palette = \u0026quot;Spectral\u0026quot;) + #scale_fill_manual(values = c(\u0026quot;#D55E00\u0026quot;, \u0026quot;#009E73\u0026quot;, \u0026quot;#0072B2\u0026quot;)) + scale_color_manual(values = c(\u0026quot;#000000\u0026quot;, \u0026quot;dodgerblue\u0026quot;, \u0026quot;darkmagenta\u0026quot;)) + scale_x_continuous(limits = c(0, 10)) + ylab(\u0026quot;\u0026quot;) + labs(col = \u0026quot;R effect\u0026quot;, fill = \u0026quot;Morphology\u0026quot;) It is evident both the center and the variability of the distributions change depending whether we look an “average” coral reef (purple line), a reef towards the upper extreme (blue line) or the lower extreme (black line). So the conclusions should be something along the lines: the increase in disease likelihood with plastic debris depends also on inherit/unobserved characteristics of the reefs, captured by the random effects, in addition to their morphology.\nOf course, what I have presented above is still conditional interpretation of the parameters. Ideally, we want the marginal population-average interpretation which is obtained from averaging over the random effects. This allows to take into account both the residual (observation-level) variance, the uncertainty in the variance parameters for the grouping factors added to the uncertainty in the fixed coefficients. See for example the predictInterval() function of the merTools package.\nReferences Lamb, JB, BL Willis, EA Fiorenza, CS Couch, R Howard, DN Rader, JD True, et al. 2018. “Data from: Plastic Waste Associated with Disease on Coral Reefs.” Science. Dryad Digital Repository. https://doi.org/10.5061/dryad.mp480.\n Lamb, Joleah B, Bette L Willis, Evan A Fiorenza, Courtney S Couch, Robert Howard, Douglas N Rader, James D True, et al. 2018. “Plastic Waste Associated with Disease on Coral Reefs.” Science 359 (6374): 460–62.\n   ","date":1526774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526774400,"objectID":"8b599fc00a28470359c907b53baf5d5f","permalink":"https://solon-karapanagiotis.com/post/reefs/plastic-waste-and-disease-on-coral-reefs/","publishdate":"2018-05-20T00:00:00Z","relpermalink":"/post/reefs/plastic-waste-and-disease-on-coral-reefs/","section":"post","summary":"Recently, I came across this very interesting article published in Science about how plastic waste is associated with disease on coral reefs (J. B. Lamb et al. 2018). The main conclusions are","tags":["data analysis","misinterpretation","random effects models"],"title":"Plastic waste and disease on coral reefs - Another misinterpretation of a statistical model","type":"post"},{"authors":null,"categories":["guidelines","reporting"],"content":" Poor quality statistical reporting in the biomedical literature is not uncommon. Here is another example by Cirio et al. (2016). The study itself is well planed, executed and reported. The aim was to assess whether heated and humidified high flow gases delivered through nasal cannula (HFNC) improve exercise performance in severe chronic obstructive pulmonary disease (COPD) patients. It all started when I saw their Fig.1. Here is my attempt to reproduce it\n Figure 1: Effect of the HFNC on exercise capacity compared to a control condition. Tlim = exercise duration.  In total there are 12 patients tested twice; once under the control test and once under the HFNC test. The outcome of interest is the endurance time (Tlim; y axis). This is practically how long each test lasted. The authors hypothesized that HFNC would improve exercise performance, that is the test would last longer. This was the case since Tlim increased for all subjects under the HFNC test (see figure 1). Moreover, this increase reached statistical significance (p-value = 0.015) - ready to publish! Looking at the plot I was pondered about the “outlying” patient (red dot). His/her Tlim increased by a whooping 400 seconds! This is huge compared to the other patients. Then I wondered how would the results change if we excluded him/her from the analysis? And here is where the problems start.\nThere is no way from the text to figure out which test was used to produce the p-value of 0.015. Is is a paired t-test or a Wilcoxon test? (they mention both in the statistical analysis section). So it is impossible to evaluate and/or try to reproduce the results.\nHaving abandoned the idea of being able to reproduce the analysis I started thinking about reporting guidelines, hence the title of this post. I thought the journal must have guidelines for reporting statistical analysis. No, it does not and unfortunately, most of the biomedical journals don’t have such guidelines even though 40 years ago O’Fallon and colleges recommended that “Standards governing the content and format of statistical aspects should be developed to guide authors in the preparation of manuscripts” (O’Fallon et al. 1978). Since then many have repeated the message. A few sporadic attempts are usually editorials such as Cummings and Rivara (2003), Curran-Everett and Benos (2004) and Arifin et al. (2016).\nRecently, Lang and Altman (2013) published a comprehensive set of statistical reporting guidelines suitable for medical journals - the SAMPL guidelines. “The SAMPL guidelines are designed to be included in a journal’s Instructions for Authors”. So the journals just need to refer to them! As there are many general reporting guidelines based on the study design as such CONSORT, STROBE, PRISMA etc (see http://www.equator-network.org/) that authors in many journals must adhere to, I believe the SAMPL guidelines is a big step forward on reporting statistics. The only journal (that I know of) that suggests the use of the SAMPL guidelines is the British Journal of Dermatology (Hollestein and Nijsten 2015). (I’ll keep adding to this list).\nNow that the guidelines exist, let’s make use of them.\nReferences Arifin, Wan Nor, Abdullah Sarimah, Bachok Norsa’adah, Yaacob Najib Majdi, Ab Hamid Siti-Azrin, Musa Kamarul Imran, Abd Aziz Aniza, and Lin Naing. 2016. “Reporting Statistical Results in Medical Journals.” The Malaysian Journal of Medical Sciences: MJMS 23 (5): 1.\n Cirio, Serena, Manuela Piran, Michele Vitacca, Giancarlo Piaggi, Piero Ceriana, Matteo Prazzoli, Mara Paneroni, and Annalisa Carlucci. 2016. “Effects of Heated and Humidified High Flow Gases During High-Intensity Constant-Load Exercise on Severe Copd Patients with Ventilatory Limitation.” Respiratory Medicine 118: 128–32.\n Cummings, Peter, and Frederick P Rivara. 2003. “Reporting Statistical Information in Medical Journal Articles.” Archives of Pediatrics \u0026amp; Adolescent Medicine 157 (4): 321–24.\n Curran-Everett, Douglas, and Dale J Benos. 2004. “Guidelines for Reporting Statistics in Journals Published by the American Physiological Society.” Am Physiological Soc.\n Hollestein, LM, and Tamar Nijsten. 2015. “Guidelines for Statistical Reporting in the British Journal of Dermatology.” British Journal of Dermatology 173 (1): 3–5.\n Lang, Thomas A, and Douglas G Altman. 2013. “Basic Statistical Reporting for Articles Published in Biomedical Journals: The ‘Statistical Analyses and Methods in the Published Literature’ or the Sampl Guidelines”.” Handbook, European Association of Science Editors 256: 256.\n O’Fallon, JR, SD Dubey, DS Salsburg, JH Edmonson, A Soffer, and T Colton. 1978. “Should There Be Statistical Guidelines for Medical Research Papers?” Biometrics, 687–95.\n   ","date":1526342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526342400,"objectID":"48baa9638f24c77831706a3c80545d5f","permalink":"https://solon-karapanagiotis.com/post/stat_reporting/on-statistical-reporting-in-biomedical-journals/","publishdate":"2018-05-15T00:00:00Z","relpermalink":"/post/stat_reporting/on-statistical-reporting-in-biomedical-journals/","section":"post","summary":"Poor quality statistical reporting in the biomedical literature is not uncommon. Here is another example by Cirio et al. (2016). The study itself is well planed, executed and reported. The aim was to assess whether heated and humidified high flow gases delivered through nasal cannula (HFNC) improve exercise performance in severe chronic obstructive pulmonary disease (COPD) patients.","tags":["guidelines","(stat) reporting"],"title":"On statistical reporting in biomedical journals","type":"post"},{"authors":null,"categories":["R","data"],"content":" There are many tutorials for importing data into R focusing on a specific function/package. This one focuses on 3 different packages. You will learn how to import all common formats of flat file data with base R functions and the dedicated readr and data.table packages. I first present these three packages and finish with a comparison table between them.\nTask Import a flat file into R: create an R object that contains the data from a flat file.\n What is a flat file? A flat file can be a plain text file that contains table data. A form of flat file is one in which table data is gathered in lines with the value from each table cell separated by a comma and each row represented with a new line. This type of flat file is also known as a comma-separated values (CSV) file. An alternative is a tab-delimited file where each field value is separated from the next using tabs.\nThe following sections describe various options for importing flat files. The ultimate goal is to convey, “translate”, them into an R data.frame.\n What are we going to import? For illustration purposes we use the Happiness dataset. It is based on the European quality of life survey with questions related to income, life satisfaction or perceived quality of society. The file is quite small but enough to sharpen your importing skills. It provides the average rating for the question “How happy would you say you are these days?”. Rating 1 (low) to 10 (high) by country and gender.\n## Country Gender Mean N. ## 1 AT Male 7.3 471 ## 2 Female 7.3 570 ## 3 Both 7.3 1041 ## 4 BE Male 7.8 468 ## 5 Female 7.8 542 ## 6 Both 7.8 1010 ## 7 BG Male 5.8 416 ## 8 Female 5.8 555 ## 9 Both 5.8 971 ## 10 CY Male 7.8 433  Let’s get going… the utils We start with the utils package. This package is loaded by default when you start your R session. This means that you can access its functions without further due. Here, we are interested in three of them: read.table(), read.csv(), and read.delim().\nReading data with read.table() Reads a file in table format and creates an R data.frame from it, with cases corresponding to rows and variables to columns. Let’s see how it works for our dataset.\nhappiness \u0026lt;- read.table(\u0026quot;happiness.csv\u0026quot;) head(happiness) ## V1 ## 1 Country,Gender,Mean,N= ## 2 AT,Male,7.3,471 ## 3 ,Female,7.3,570 ## 4 ,Both,7.3,1041 ## 5 BE,Male,7.8,468 ## 6 ,Female,7.8,542 Not what we wanted?! This data frame contains 108 rows and 1 column instead of 105 rows and 4 columns. That’s because additional arguments need to be specified in order to tell R what it has to deal with.\nhappiness \u0026lt;- read.table(file = \u0026quot;happiness.csv\u0026quot;, # path to flat file header = TRUE, # first row lists variables\u0026#39; names sep = \u0026quot;,\u0026quot;, # field separator is a comma stringsAsFactors = FALSE) # not import strings as categorical variables Let’s take a look now\nhead(happiness) ## Country Gender Mean N. ## 1 AT Male 7.3 471 ## 2 Female 7.3 570 ## 3 Both 7.3 1041 ## 4 BE Male 7.8 468 ## 5 Female 7.8 542 ## 6 Both 7.8 1010 str(happiness) ## \u0026#39;data.frame\u0026#39;: 105 obs. of 4 variables: ## $ Country: chr \u0026quot;AT\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;BE\u0026quot; ... ## $ Gender : chr \u0026quot;Male\u0026quot; \u0026quot;Female\u0026quot; \u0026quot;Both\u0026quot; \u0026quot;Male\u0026quot; ... ## $ Mean : num 7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ... ## $ N. : int 471 570 1041 468 542 1010 416 555 971 433 ... By specifying header = TRUE R sees the that the first line contains the names of the variables. With stringsAsFactors = FALSE we specify that we wanted Country and Gender to be character variables. The sep = \",\" identifies the field separator to be a comma. There are many more arguments you can specify and each one can take many values! For further details, consult the R documentation or type help(read.table) on the console.\n Note: In order to use read.table(), in same manner, you need to give the full path name of the target file if it’s not in your working directory. You can use the R Function of the Day, namely setwd(\"\u0026lt;location of your dataset\u0026gt;\"), to change your working directory. The same is valid for any other function we are going to encounter in this tutorial. Alternatively, you can specify the location of the flat file inside read.table(). Keep in mind that the specification of the file is platform dependent (Windows, Unix/Linux and OSX).\n read.table(file = \u0026quot;\u0026lt;location of your dataset\u0026gt;\u0026quot;, ...)   Another option is to use file.path(). It constructs the path to a file from components in a platform-independent way. For example,\n path \u0026lt;- file.path(\u0026quot;~\u0026quot;, \u0026quot;datasets\u0026quot;, \u0026quot;happiness.csv\u0026quot;) happiness \u0026lt;- read.table(file = path, header = TRUE, sep = \u0026quot;,\u0026quot;, stringsAsFactors = FALSE) Comment The stringsAsFactors argument is true by default which means that character variables are imported into R as factors, the data type to store categorical variables.\nhappiness_2 \u0026lt;- read.table(file = \u0026quot;happiness.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot;, stringsAsFactors = TRUE) At first sight you do not notice anything different and you shouldn’t! But for R it’s a big deal! For character variables each element is a string of one or more characters. On the other hand, factor variables are stored, internally, as numeric variables together with their levels. This has major impact in computations that R maybe has to carry out later.\nstr(happiness_2) ## \u0026#39;data.frame\u0026#39;: 105 obs. of 4 variables: ## $ Country: Factor w/ 36 levels \u0026quot;\u0026quot;,\u0026quot;AT\u0026quot;,\u0026quot;BE\u0026quot;,\u0026quot;BG\u0026quot;,..: 2 1 1 3 1 1 4 1 1 6 ... ## $ Gender : Factor w/ 3 levels \u0026quot;Both\u0026quot;,\u0026quot;Female\u0026quot;,..: 3 2 1 3 2 1 3 2 1 3 ... ## $ Mean : num 7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ... ## $ N. : int 471 570 1041 468 542 1010 416 555 971 433 ...  Reading data with read.csv() It is a wrapper around read.table(). This means that read.csv() calls read.table() behind the scenes but with different default arguments. More specifically, the defaults are header = TRUE and sep = \",\". These match with the standardized CSV format, where , is used as a separator and usually the first line contains the names of the columns. Therefore, it saves you time since you need to specify less arguments.\nread.csv(file = \u0026quot;happiness.csv\u0026quot;, stringsAsFactors = FALSE) which is equivalent to\nread.table(file = \u0026quot;happiness.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot;, stringsAsFactors = FALSE)  Reading data with read.delim() It is also a wrapper of read.table(). Now the default arguments match with tab-delimited files. More specifically, the defaults are header = TRUE and sep = \"\\t\", since \\t is the field separator in tab-delimited files.\nread.delim(file = \u0026quot;happiness.txt\u0026quot;, stringsAsFactors = FALSE) which is equivalent to\nread.table(file = \u0026quot;happiness.txt\u0026quot;, header = TRUE, sep = \u0026quot;\\t\u0026quot;, stringsAsFactors = FALSE) Both these functions make our lives easier since less arguments need to be specified.\nNote Locale differences. The standard field delimiters for CSV files are commas. On US versions, the comma is set as default for the “List Separator”, which is okay for CSV files. But on European versions this character is reserved as the Decimal Symbol and the “List Separator” is set by default to the semicolon. Why you should care?\nSuppose you try to import the European CSV version happiness_eu.csv.\nhead(happiness_eu) ## Country.Gender.Mean.N. ## 1 AT,Male,7.3,471 ## 2 ,Female,7.3,570 ## 3 ,Both,7.3,1041 ## 4 BE,Male,7.8,468 ## 5 ,Female,7.8,542 ## 6 ,Both,7.8,1010 R performs the operation but clearly not the one we wanted. It’s a data frame with 105 rows but a single variable! To deal with such problems you can use the read.csv2() function. The defaults are sep = \";\" and dec = \",\".\nhappiness_eu \u0026lt;- read.csv2(file = \u0026quot;happiness_eu.csv\u0026quot;, stringsAsFactors = FALSE) head(happiness_eu) ## Country Gender Mean N. ## 1 AT Male 7.3 471 ## 2 Female 7.3 570 ## 3 Both 7.3 1041 ## 4 BE Male 7.8 468 ## 5 Female 7.8 542 ## 6 Both 7.8 1010 Similarly, there is read.delim2(). The logic is the same.\nTo summarize, the read.table() is to read delimited data files. Some variants are: read.csv() and read.delim(), which have different default values and are tailored for CSV and tab-delimited files, respectively.\n In read.csv() default values are: header = T, sep = ???,???, dec = ???.??? In read.csv2() default values are: header = T, sep = ???;???, dec = ???,??? In read.delim() default values are: header = T, sep = ???\\t???, dec = ???.??? In read.delim2() default values are: header = T, sep = ???\\t???, dec = ???,???  ##readr … an alternative to import flat files\nAn alternative to the utils package is the readr. Compared the read.table family of functions, it is faster, easier to use and with a consistent naming scheme. We start by installing and loading it.\ninstall.packages(\u0026quot;readr\u0026quot;) library(readr) Let’s import our dataset. In readr you can use read_delim() for flat files. It can be considered the correspondent to read.table().\nhappiness_readr \u0026lt;- read_delim(\u0026quot;happiness.csv\u0026quot;, # path to flat file delim = \u0026quot;,\u0026quot;) # character that separates fields in the file ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Country = col_character(), ## Gender = col_character(), ## Mean = col_double(), ## `N=` = col_double() ## ) str(happiness_readr) ## tibble [105 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Country: chr [1:105] \u0026quot;AT\u0026quot; NA NA \u0026quot;BE\u0026quot; ... ## $ Gender : chr [1:105] \u0026quot;Male\u0026quot; \u0026quot;Female\u0026quot; \u0026quot;Both\u0026quot; \u0026quot;Male\u0026quot; ... ## $ Mean : num [1:105] 7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ... ## $ N= : num [1:105] 471 570 1041 468 542 ... ## - attr(*, \u0026quot;spec\u0026quot;)= ## .. cols( ## .. Country = col_character(), ## .. Gender = col_character(), ## .. Mean = col_double(), ## .. `N=` = col_double() ## .. ) head(happiness_readr) ## # A tibble: 6 x 4 ## Country Gender Mean `N=` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 AT Male 7.3 471 ## 2 \u0026lt;NA\u0026gt; Female 7.3 570 ## 3 \u0026lt;NA\u0026gt; Both 7.3 1041 ## 4 BE Male 7.8 468 ## 5 \u0026lt;NA\u0026gt; Female 7.8 542 ## 6 \u0026lt;NA\u0026gt; Both 7.8 1010 Notice, that the output is the same as when using the read.table(), previously. But we did not have to specify header=TRUE because by default read_delim() expects the first row to contain the column names. This is done through the col_names argument, set equal to true by default. Also, strings are never automatically converted to factors. Hence, stringsAsFactors = FALSE is not necessary. To control the types of the columns readr uses the col_types argument. Let’s see how these two work.\ncol_names is true by default meaning that it will use the the first row of data as column names. If your file does not have column names you can set col_names = FALSE and columns will be numbered sequentially.\nhead(read_delim(\u0026quot;happiness2.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, col_names = FALSE)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## X1 = col_character(), ## X2 = col_character(), ## X3 = col_double(), ## X4 = col_character() ## ) ## # A tibble: 6 x 4 ## X1 X2 X3 X4 ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026#39;AT Male 7.3 471\u0026#39; ## 2 \u0026#39; Female 7.3 570\u0026#39; ## 3 \u0026#39; Both 7.3 1041\u0026#39; ## 4 \u0026#39;BE Male 7.8 468\u0026#39; ## 5 \u0026#39; Female 7.8 542\u0026#39; ## 6 \u0026#39; Both 7.8 1010\u0026#39;  Note Instead of assigning the output of read_delim() to a variable I directly use the head() function to print the first 6 lines of the data frame. It is equivalent to\n happiness_delim \u0026lt;- read_delim(\u0026quot;happiness2.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, col_names = FALSE) head(happiness_delim) You can also manually set the column names.\nhead(read_delim(\u0026quot;happiness2.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, col_names = c(\u0026quot;Country\u0026quot;, \u0026quot;Gender\u0026quot;, \u0026quot;Mean\u0026quot;, \u0026quot;N\u0026quot;))) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Country = col_character(), ## Gender = col_character(), ## Mean = col_double(), ## N = col_character() ## ) ## # A tibble: 6 x 4 ## Country Gender Mean N ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026#39;AT Male 7.3 471\u0026#39; ## 2 \u0026#39; Female 7.3 570\u0026#39; ## 3 \u0026#39; Both 7.3 1041\u0026#39; ## 4 \u0026#39;BE Male 7.8 468\u0026#39; ## 5 \u0026#39; Female 7.8 542\u0026#39; ## 6 \u0026#39; Both 7.8 1010\u0026#39; As mentioned, there is col_types to control the column classes. If you leave the default value readr heuristically inspects the first 100 rows to guess the type of each column.\nsapply(happiness_readr, class) ## Country Gender Mean N= ## \u0026quot;character\u0026quot; \u0026quot;character\u0026quot; \u0026quot;numeric\u0026quot; \u0026quot;numeric\u0026quot; If you want to override the default column types you can also specify them manually. An option would be\nhappiness_readr2 \u0026lt;- read_delim(\u0026quot;happiness.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, col_types = \u0026quot;ccni\u0026quot;) sapply(happiness_readr2, class) ## Country Gender Mean N= ## \u0026quot;character\u0026quot; \u0026quot;character\u0026quot; \u0026quot;numeric\u0026quot; \u0026quot;integer\u0026quot; Where\n c = character d = double i = integer l = logical _ = skip  Let see how skip works\nhead(read_delim(\u0026quot;happiness.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, col_types = \u0026quot;ccn_\u0026quot;)) ## # A tibble: 6 x 3 ## Country Gender Mean ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 AT Male 7.3 ## 2 \u0026lt;NA\u0026gt; Female 7.3 ## 3 \u0026lt;NA\u0026gt; Both 7.3 ## 4 BE Male 7.8 ## 5 \u0026lt;NA\u0026gt; Female 7.8 ## 6 \u0026lt;NA\u0026gt; Both 7.8 Notice the fourth column has been skipped.\nYet another way of setting the types of the imported columns is using collectors. Collector functions can be passed in a list() to the col_types argument of read_ functions to tell them how to interpret values in a column.\ncar \u0026lt;- col_character() fac \u0026lt;- col_factor(levels = c(\u0026quot;Male\u0026quot;, \u0026quot;Female\u0026quot;, \u0026quot;Both\u0026quot;)) num \u0026lt;- col_number() int \u0026lt;- col_integer() str(read_delim(\u0026quot;happiness.csv\u0026quot;, delim = \u0026quot;,\u0026quot; , col_types = list(car, fac, num, int))) ## tibble [105 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Country: chr [1:105] \u0026quot;AT\u0026quot; NA NA \u0026quot;BE\u0026quot; ... ## $ Gender : Factor w/ 3 levels \u0026quot;Male\u0026quot;,\u0026quot;Female\u0026quot;,..: 1 2 3 1 2 3 1 2 3 1 ... ## $ Mean : num [1:105] 7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ... ## $ N= : int [1:105] 471 570 1041 468 542 1010 416 555 971 433 ... ## - attr(*, \u0026quot;spec\u0026quot;)= ## .. cols( ## .. Country = col_character(), ## .. Gender = col_factor(levels = c(\u0026quot;Male\u0026quot;, \u0026quot;Female\u0026quot;, \u0026quot;Both\u0026quot;), ordered = FALSE, include_na = FALSE), ## .. Mean = col_number(), ## .. `N=` = col_integer() ## .. ) For a complete list of collector functions, you can take a look at the collector documentation.\nIf you are working on large datasets you may prefer handling the data in smaller parts. In readr you can achieve this with the combination of skip and n_max arguments.\nhead(read_delim(\u0026quot;happiness.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, skip=2, n_max= 4)) ## Warning: Missing column names filled in: \u0026#39;X1\u0026#39; [1] ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## X1 = col_character(), ## Female = col_character(), ## `7.3` = col_double(), ## `570` = col_double() ## ) ## # A tibble: 4 x 4 ## X1 Female `7.3` `570` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 \u0026lt;NA\u0026gt; Both 7.3 1041 ## 2 BE Male 7.8 468 ## 3 \u0026lt;NA\u0026gt; Female 7.8 542 ## 4 \u0026lt;NA\u0026gt; Both 7.8 1010 We skipped two rows and then read four lines. There is something wrong though! Since the col_names is true by default the first line is used for the column names. Therefore, we need to manually specify the column names.\nhead(read_delim(\u0026quot;happiness2.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, col_names = c(\u0026quot;Country\u0026quot;,\u0026quot;Gender\u0026quot;, \u0026quot;Mean\u0026quot;, \u0026quot;N\u0026quot;), skip=2, n_max= 4)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Country = col_character(), ## Gender = col_character(), ## Mean = col_double(), ## N = col_character() ## ) ## # A tibble: 4 x 4 ## Country Gender Mean N ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026#39; Both 7.3 1041\u0026#39; ## 2 \u0026#39;BE Male 7.8 468\u0026#39; ## 3 \u0026#39; Female 7.8 542\u0026#39; ## 4 \u0026#39; Both 7.8 1010\u0026#39; Like the utils package readr provides alternatives to read_delim(). The read_csv() and read_tsv() are used for CSV files and tab-delimited files, respectively. The functions of both packages are presented below. Notice the _ is used in readr instead of the ..\n  utils readr    read.table() read_delim()  read.csv() read_csv()  read.delim() read.tsv      data.table … yet another alternative to read data into R The data.table package is designed mainly for fast data manipulation. It also features a powerful function to import your data into R, the fread(). Once more you need to install and load the package.\ninstall.packages(\u0026quot;data.table\u0026quot;) library(data.table) Let’s see how it works with two versions of our dataset.\nhead(fread(\u0026quot;happiness.csv\u0026quot;)) ## Country Gender Mean N= ## 1: AT Male 7.3 471 ## 2: Female 7.3 570 ## 3: Both 7.3 1041 ## 4: BE Male 7.8 468 ## 5: Female 7.8 542 ## 6: Both 7.8 1010 head(fread(\u0026quot;happiness2.csv\u0026quot;)) ## V1 V2 V3 V4 ## 1: \u0026#39;AT Male 7.3 471\u0026#39; ## 2: \u0026#39; Female 7.3 570\u0026#39; ## 3: \u0026#39; Both 7.3 1041\u0026#39; ## 4: \u0026#39;BE Male 7.8 468\u0026#39; ## 5: \u0026#39; Female 7.8 542\u0026#39; ## 6: \u0026#39; Both 7.8 1010\u0026#39; Remember that the first row of happiness2.csv does not contain the column names. That’s not a problem for fread() as it automatically assignees names to the columns. As in this case, often simply specifying the path to the file is enough to successfully import your flat file using fread. Moreover, it can infer the column types and separators.\nstr(fread(\u0026quot;happiness.csv\u0026quot;)) ## Classes \u0026#39;data.table\u0026#39; and \u0026#39;data.frame\u0026#39;: 105 obs. of 4 variables: ## $ Country: chr \u0026quot;AT\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;BE\u0026quot; ... ## $ Gender : chr \u0026quot;Male\u0026quot; \u0026quot;Female\u0026quot; \u0026quot;Both\u0026quot; \u0026quot;Male\u0026quot; ... ## $ Mean : num 7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ... ## $ N= : int 471 570 1041 468 542 1010 416 555 971 433 ... ## - attr(*, \u0026quot;.internal.selfref\u0026quot;)=\u0026lt;externalptr\u0026gt; Two more useful arguments of fread() are drop and select. They enable you to drop or select variables of interest in your flat file. Suppose I want to select the 2nd and 3rd column.\nhead(fread(\u0026quot;happiness.csv\u0026quot;, select = c(2,3))) ## Gender Mean ## 1: Male 7.3 ## 2: Female 7.3 ## 3: Both 7.3 ## 4: Male 7.8 ## 5: Female 7.8 ## 6: Both 7.8 Alternatively,\nhead(fread(\u0026quot;happiness.csv\u0026quot;, select = c(\u0026quot;Gender\u0026quot;,\u0026quot;Mean\u0026quot;))) ## Gender Mean ## 1: Male 7.3 ## 2: Female 7.3 ## 3: Both 7.3 ## 4: Male 7.8 ## 5: Female 7.8 ## 6: Both 7.8 or\nhead(fread(\u0026quot;happiness.csv\u0026quot;, drop = c(1,4))) ## Gender Mean ## 1: Male 7.3 ## 2: Female 7.3 ## 3: Both 7.3 ## 4: Male 7.8 ## 5: Female 7.8 ## 6: Both 7.8 which is equivalent to\nhead(fread(\u0026quot;happiness.csv\u0026quot;, drop = c(\u0026quot;Country\u0026quot;,\u0026quot;N=\u0026quot;))) In short, fread() saves you work by automatically guessing the delimiter, whether or not the file has a header, how many lines to skip by default, providing an easy way to select variables and more. Nevertheless, if you wish to specify them, you can do it, along with other arguments. Check the documentation.\nComment You might have noticed by now that the fread() function produces data frames that look slightly different when you print them out. That’s because another class is assigned to the resulting data frames, namely data.table and data.frame. read_delim() creates an object with three classes: tbl_df, tbl and data.frame. The printout of such data.table objects is different. Well, it allows for a different treatment of the printouts, for example.\n When to use read.table(), read_delim() or fread() In a nutshell, the main differences between these functions are : the read_ functions from the readr package have more consistent naming scheme for the parameters (e.g. col_names and col_types) than read. and all functions work exactly the same way regardless of the current locale (to override the US-centric defaults, use locale()). It is also faster! But fread() is even faster! And it saves you work by automatically guessing parameters. This README goes in more detail.\n   Speed Auto-detection Locale    read.table() fast NO YES  read_delim() faster NO NO  fread() fastest YES NO    With some loss of generality a few suggestions are:\n for large files (many MB-GB) fread() will be the fastest (with a few exceptions) the consistency and independence of the actual locale makes readr a good candidate for everyday use if you are new to all these using read.table() will allow you to develop intuition on how R works.   ","date":1523318400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523318400,"objectID":"243cfc121bb1400aff481e6cecbb96d4","permalink":"https://solon-karapanagiotis.com/post/importflatfiles/importing-flat-files-into-r/","publishdate":"2018-04-10T00:00:00Z","relpermalink":"/post/importflatfiles/importing-flat-files-into-r/","section":"post","summary":"There are many tutorials for importing data into R focusing on a specific function/package. This one focuses on 3 different packages. You will learn how to import all common formats of flat file data with base R functions and the dedicated readr and data.","tags":["import","data","R"],"title":"Importing Flat Files Into R","type":"post"},{"authors":["Solon Karapanagiotis","Paul D. P. Pharoah","Christopher H. Jackson","Paul J. Newcombe"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"1748b5665cbdb6ac88b607f717cf465e","permalink":"https://solon-karapanagiotis.com/publication/clinicalcancerresearch/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/clinicalcancerresearch/","section":"publication","summary":"**Purpose**: To compare PREDICT and CancerMath, two widely used prognostic models for invasive breast cancer, taking into account their clinical utility. Furthermore, it is unclear whether these models could be improved. **Experimental Design**: A dataset of 5729 women was used for model development. A Bayesian variable selection algorithm was implemented to stochastically search for important interaction terms among the predictors. The derived models were then compared in three independent datasets (n = 5534). We examined calibration, discrimination and performed decision curve analysis. **Results**: CancerMath demonstrated worse calibration performance compared to PREDICT in oestrogen receptor (ER)-positive and ER-negative tumours. The decline in discrimination performance was -4.27% (-6.39 - -2.03) and -3.21% (-5.9 - -0.48) for ER-positive and ER-negative tumours, respectively. Our new models matched the performance of PREDICT in terms of calibration and discrimination, but offered no improvement. Decision curve analysis showed predictions for all models were clinically useful for treatment decisions made at risk thresholds between 5% and 55% for ER-positive tumours and at thresholds of 15% to 60% for ER-negative tumours. Within these threshold ranges, CancerMath provided the lowest clinical utility amongst all the models. **Conclusions**: Survival probabilities from PREDICT offer both improved accuracy and discrimination over CancerMath. Using PREDICT to make treatment decisions offers greater clinical utility than CancerMath over a range of risk thresholds. Our new models performed as well as PREDICT, but no better, suggesting that, in this setting, including further interaction terms offers no predictive benefit.","tags":null,"title":"Development and External Validation of Prediction Models for 10-Year Survival of Invasive Breast Cancer. Comparison with PREDICT and CancerMath","type":"publication"},{"authors":["Solon Karapanagiotis","Simone Gambazza","Anna Brivio","Francesco D'Abrosca","Carla Colombo"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"13cfae4635392c140835b566ec03e2bd","permalink":"https://solon-karapanagiotis.com/publication/ventilatorylimitation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/ventilatorylimitation/","section":"publication","summary":"**Objective**: To investigate the presence of dynamic hyperinflation after the Modified Shuttle Test (MST) and its relationship with lung function, exercise tolerance, and clinical symptoms in Cystic Fibrosis (CF). **Methods**: Retrospective observational study. Subjects in clinically stable condition with a CF diagnosis based on a positive sweat test (chloride 60 mEq/L) and/or presence of two disease causing mutations, with available data on MST, spirometry, maximal voluntary ventilation, and inspiratory capacity manoeuvres were considered for the analysis. Breathing reserve was calculated and a threshold value of 0.7 was subsequently chosen as a value of pulmonary mechanical limit. Subjects were then categorized into two groups according to the change in the inspiratory capacity from rest to peak exercise. Unconditional logistic regression was used to estimate unadjusted odds ratios, 95% confidence intervals and P‐values. **Results**: Twenty‐two subjects demonstrated evidence of dynamic hyperinflation during the MST. Thirteen out of 22 subjects were ventilatory limited during exercise including 5 of those without evidence of dynamic hyperinflation (P = 0.24). No combination of variables resulted in a parsimonious regression model. **Conclusions**: Dynamic hyperinflation is common in CF and it is not associated with traditionally defined ventilatory limitation parameters during the MST.","tags":null,"title":"Ventilatory limitation and dynamic hyperinflation during exercise testing in Cystic Fibrosis","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://solon-karapanagiotis.com/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":null,"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://solon-karapanagiotis.com/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":null,"title":"Internal Project","type":"project"},{"authors":["Simone Gambazza","Clara Ceruti","Anna Brivio","Giancarlo Piaggi","Solon Karapanagiotis","Carla Colombo"],"categories":null,"content":"","date":1443657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443657600,"objectID":"89703def19b57b44c282eaa62bf90cb5","permalink":"https://solon-karapanagiotis.com/publication/isocapnichyperpnea/","publishdate":"2015-10-01T00:00:00Z","relpermalink":"/publication/isocapnichyperpnea/","section":"publication","summary":"To evaluate the bias and precision of the respiratory muscle training device formulas to predict respiratory minute volume (RMV) and volume of the reservoir bag (BV) on a cohort of subjects with Cystic Fibrosis (CF). CF patients with available pulmonary function tests and maximal voluntary manoeuvres were included in the study. Vital capacity and maximal voluntary ventilation were extracted from subjects’ records and then inserted to the manufacturer’s formulas to obtain RMV and BV (measured setting). RMV and BV were compared according to standard and measured formulas in males and females. Sample was described and then processed using Bland–Altman analysis. Bland–Altman analysis for RMV revealed a bias and precision of 8.8 ± 29 L/min in males and 28.8 ± 16 L/min in females; 0.4 ± 0.5 L in males and 0.7 ± 0.4 L in females for BV. Concordance correlation coefficients for RMV were −0.03 in males and 0.02 in females; 0.22 in males and 0.03 in females for BV, reinforcing an unsatisfactory concordance between measured and manufacturer setting. This study shows considerable discrepancies between the two methods, making the degree of agreement not clinically acceptable. This might cause inappropriate setting and disservice to patients with CF.","tags":null,"title":"Isocapnic hyperpnea with a portable device in Cystic Fibrosis: an agreement study between two different set-up modalities","type":"publication"},{"authors":["Solon Karapanagiotis","Simone Gambazza","Anna Brivio","Carla Colombo"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"7fa8967448de8cfe3e73d3bbc7f28f21","permalink":"https://solon-karapanagiotis.com/publication/mst/","publishdate":"2014-06-01T00:00:00Z","relpermalink":"/publication/mst/","section":"publication","summary":"","tags":null,"title":"Cystic fibrosis patients’ performance on Modified Shuttle Walk Test","type":"publication"},{"authors":["Solon Karapanagiotis","Simone Gambazza","Anna Brivio","Carla Colombo"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"ce29bd6e506f6eefa2f2e76e45e24141","permalink":"https://solon-karapanagiotis.com/publication/exerciseintensity/","publishdate":"2014-06-01T00:00:00Z","relpermalink":"/publication/exerciseintensity/","section":"publication","summary":"","tags":null,"title":"Exercise intensity during interactive video game","type":"publication"},{"authors":["Solon Karapanagiotis","Simone Gambazza","Arianna Bisogno","Anna Brivio","Carla Colombo"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"5b78ce9ecbe3fdd2d81cc6ddb780f60e","permalink":"https://solon-karapanagiotis.com/publication/quantifyingweight/","publishdate":"2014-06-01T00:00:00Z","relpermalink":"/publication/quantifyingweight/","section":"publication","summary":"","tags":null,"title":"Quantifying weight bearing activity in children and adolescents with cystic fibrosis","type":"publication"},{"authors":["M. Donà","F. Alatri","A. Brivio","G. Mamprin","M. Barbisan","M. Varchetta","S. De Sanctis","S. Gambazza","S. Karapanagiotis","M. Ros"],"categories":null,"content":"","date":1370044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1370044800,"objectID":"7eff796b9eeb6524f3d1f4eb19d267e7","permalink":"https://solon-karapanagiotis.com/publication/adherence/","publishdate":"2013-06-01T00:00:00Z","relpermalink":"/publication/adherence/","section":"publication","summary":"","tags":null,"title":"Adherence to the administration of aerosolized promixin with the I-neb adaptive aerosol delivery (AAD) system, lung function and administration times in patients with cystic fibrosis (CF)","type":"publication"},{"authors":["Solon Karapanagiotis","Anna Brivio","Simone Gambazza","Chiara Speziali","Carla Colombo"],"categories":null,"content":"","date":1370044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1370044800,"objectID":"7d3bd0ef73dcfb4e261c3ed6207ffdd5","permalink":"https://solon-karapanagiotis.com/publication/exerciseandsporthabits/","publishdate":"2013-06-01T00:00:00Z","relpermalink":"/publication/exerciseandsporthabits/","section":"publication","summary":"","tags":null,"title":"Exercise and sport habits in children and adolescents with cystic fibrosis","type":"publication"},{"authors":["Solon Karapanagiotis","Alessandro Rossi","Stefano Vercelli"],"categories":null,"content":"","date":1338508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338508800,"objectID":"28c5a32a015711d9ea5eb7d9f62705a0","permalink":"https://solon-karapanagiotis.com/publication/wiihabilitation/","publishdate":"2012-06-01T00:00:00Z","relpermalink":"/publication/wiihabilitation/","section":"publication","summary":"","tags":null,"title":"Wiihabilitation","type":"publication"}]