[{"authors":null,"categories":null,"content":"I\u0026rsquo;m a PhD student at the University of Cambridge, currenly supervised by Oscar Rueda at the MRC Biostatistics Unit. I\u0026rsquo;m interested in many areas across statistics, machine learning, and their interactions. My research focuses on tailored model development, that is targeted model building for a specific task of interest, mostly prediction. Under this scenario it is desired to use a metric which reflects the loss function to be used for the prediction problem thus \u0026ldquo;making\u0026rdquo; the model perform well for the particular task. Our approach is based on general Bayesian learning by incorporating loss functions into Bayesian inference. We will explore the use of such a framework for predictive and prognostic model building, whereby loss functions could be used to target metrics of real world clinical utility tailored to a particular setting. Before starting my PhD, I studied for a MSc in Statistics at KU Leuven. I focused on the Biometrics track and wrote my dissertation under the supervision of professor Geert Verbeke. I worked as Research Scientist at the MRC Biostatistics Unit during the 2016-2017 academic year supervised by Paul Newcombe and Chris Jackson.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://solon-karapanagiotis.com/author/solon-karapanagiotis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/solon-karapanagiotis/","section":"authors","summary":"I\u0026rsquo;m a PhD student at the University of Cambridge, currenly supervised by Oscar Rueda at the MRC Biostatistics Unit. I\u0026rsquo;m interested in many areas across statistics, machine learning, and their interactions.","tags":null,"title":"Solon Karapanagiotis","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://solon-karapanagiotis.com/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://solon-karapanagiotis.com/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://solon-karapanagiotis.com/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://solon-karapanagiotis.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Stavrinides V","et al"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"0f9dc1ec373f8edc46017e5761cb5677","permalink":"https://solon-karapanagiotis.com/publication/mri_prostate/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/mri_prostate/","section":"publication","summary":"**Background** False positive multiparametric magnetic resonance imaging (mpMRI) phenotypes prompt unnecessary biopsies. The Prostate MRI Imaging Study (PROMIS) provides a unique opportunity to explore such phenotypes in biopsy-naïve men with raised prostate-specific antigen (PSA) and suspected cancer. **Objective** To compare mpMRI lesions in men with/without significant cancer on transperineal mapping biopsy (TPM).**Design, setting, and participants** PROMIS participants (n=235) underwent mpMRI followed by a combined biopsy procedure at University College London Hospital, including 5-mm TPM as the reference standard. Patients were divided into four mutually exclusive groups according to TPM findings: (1) no cancer, (2) insignificant cancer, (3) definition 2 significant cancer (Gleason ≥3 + 4 of any length and/or maximum cancer core length ≥4 mm of any grade), and (4) definition 1 significant cancer (Gleason ≥4 + 3 of any length and/or maximum cancer core length ≥6 mm of any grade). **Outcome measurements and statistical analysis** Index and/or additional lesions present in 178 participants were compared between TPM groups in terms of number, conspicuity, volume, location, and radiological characteristics. **Results and limitations** Most lesions were located in the peripheral zone. More men with significant cancer had two or more lesions than those without significant disease (67% vs 37%; p ","tags":null,"title":"False Positive Multiparametric Magnetic Resonance Imaging Phenotypes in the Biopsy-naïve Prostate: Are They Distinct from Significant Cancer-associated Lesions? Lessons from PROMIS","type":"publication"},{"authors":null,"categories":["reporting","model evaluation","R"],"content":"  Overview Mitani and co-authors’ present a deep-learning algorithm trained with retinal images and participants’ clinical data from the UK Biobank to estimate blood-haemoglobin levels and predict the presence or absence of anaemia (Mitani et al. 2020). A major limitation of the study is the inadequate evaluation of the algorithm. I will show how a naïve classification (i.e. classify everybody as healthy) performs much better than their deep-learning approach, despite their model having AUC of around 80%. I will then explain why this is the case and finish with some thoughts on how (clinical) predictions models should be evaluated.\n Introduction The goal of the paper was to investigate whether anaemia can be detected via machine-learning algorithms trained using retinal images, study participants’ metadata or the combination of both.\nFirst, the authors develop a deep-learning algorithm to predict haemoglobin concentration (Hb) (which is the most reliable indicator of anaemia) and then three others to predict anaemia itself. They develop a deep convolutional neural network classification model to directly predict whether a patient is anaemic (rather than predicting Hb). They used the World Health Organization Hb cut-off values to label each participant as not having anaemia, mild, moderate or severe anaemia. One of the models was trained to classify: normal versus mild, moderate or severe. For concreteness, I’ll focus on this model but the reasoning below is valid for the others as well. Also, I focus on the combined model (images and metadata) because it showed the best performance (AUC of 0.88).\nThe authors present a detailed analysis of the data and their model. Nevertheless, a crucial point is missing. Is the model useful? If it is implemented tomorrow will it result in better care? This is important, especially since the authors argue their model potentially enables automated anaemia screening (see Discussion). This is slightly far-fetched in my opinion given they only evaluated their algorithm on a test set; with unsatisfactory results as I argue below. Algorithms need to be compared with human experts (i.e. ophthalmologists in this case), followed by extensive field testing to prove their trustworthiness and usefulness (Spiegelhalter 2020).\n The Issue As with many other models out there the authors have fallen in the trap of evaluating the absolute performance of their model when the important metric is the relative performance with respect to the actual reality of retinal screening. It is exactly the same idea when in clinical trials the experimental treatment is compared with the standard of care (or placebo). We want to see if the new treatment (deep-learning model classification in this case) is better than simply classifying every participant as having anaemia or not. This should be the benchmark. I call these naive classifications and I will demonstrate the model does not perform better than the naive rule of classifying everybody as healthy.\nFirst, some preliminary stuff. The performance of a model can be represented in a confusion matrix with four categories (see table below). True positives (TP) are positive examples that are correctly labelled as positives, and False positives (FP) are negative examples that are labelled incorrectly as positive. Likewise, True negatives (TN) are negatives labelled correctly as negative, and false negatives (FN) refer to positive examples labelled incorrectly as negative.\n Table 1: Confusion matrix showing correct classifications (in red) and incorrect (in blue)     Truth       positive  negative      Predict  positive  TP  FP     negative  FN  TN     Let’s use the information from the study to construct our confusion matrix using the data from Table 1 (last column; validation dataset) and Table 2 (column 1 and anaemia combined model). There are in total 10949 negatives (non anaemic) out of 11388 participants. The reported specificity is 0.7 and sensitivity is 0.875. Using these we can calculate the number of true negatives and true positives as follows:\n Specificity = true negative rate (TNR) = TN/#negative, so TN = 0.7*10949 = 7664. Sensitivity = true positive rate (TPR) = TP/ #positive, so TP = 0.875*439 = 384.  So the confusion matrix is\n   Truth       positive  negative      Predict  positive  384  3285     negative  55  7664     Looking at the table we see that in total 3285+55 = 3340 subjects have been misclassified. That is 29% misclassification rate.\nNow, let’s construct the confusion matrix for my naïve classification: “classify everybody as negative”,\n   Truth       positive  negative      Predict  positive  0  0     negative  439  10949     In total, 439 subjects have been misclassified. This is 4% misclassification rate.\nThis means the naive classification achieves 86% better performance!\n ROC (and AUC) is to blame Why was this not spotted by the authors? Probably because the model evaluation was based on the receiver operating curve (ROC) and area under the ROC (AUC). ROC curves can present an overly optimistic view of a model’s performance if there is a large skew in the class distribution. In this study the ratio positive (i.e. anaemic) to negative (i.e. not anaemic) participants is 439/10949 = 0.04 (see Table 2, validation column)!\nROC curves (and AUC) have the (un-)attractive property of being insensitive to changes in class distribution (Fawcett 2006). That is, if the proportion of positive to negative instances changes in a dataset, the ROC curves (and AUC) will not change. This is because ROC plots are based upon TPR and FPR which do not depend on class distributions. Increasing the number of positive samples by 10x would increase both TP and FN by 10x, which would not change the TPR at any threshold. Similarly, increasing the number of negative samples by 10x would increase both TN and FP by 10x, which would not change the FPR at any threshold. Thus, both the shape of the ROC curve and the AUC are insensitive to the class distribution. On the contrary, any performance metric that uses values from both columns will be inherently sensitive to class skews, for instance the misclassification rate.\nLet’s make this more concrete with a simple simulation example. I simulate one covariate, \\(X\\), which follows a standard Gaussian distribution in negative cases: \\(X \\sim N(0, 1)\\). Among positives, it follows \\(X \\sim N(1.5, 1)\\). The event rate (i.e. prevalence) is varied to be \\(20\\%\\) (I call this scenario 1) and \\(2\\%\\) (scenario 2). (The \\(2\\%\\) is close to the one observed in the study \\(\\approx 4\\%\\)). Then, I derive true risks (\\(R\\)) based on the event rate (\\(ER\\)) and the density of the covariate distributions for positives (\\(D_p\\)) and negatives (\\(D_{n}\\)) at the covariate values:\n\\[ R = \\frac{ER × D_p}{[ER × D_p] + [(1 − ER) × D_{n}]}.\\]\nI simulate two large samples of 5000 and 50000 and plot the ROC. The two plots below are almost identical despite the fact that scenario 2 has 10x more negative examples than scenario 1.\nlibrary(pROC) library(tibble) sim_data \u0026lt;- function(n_positives, n_negatives){# simulates dataset as described above and calculates the ROC # input arguments: the number of positives and negatives y \u0026lt;- c(rep(0, n_negatives), rep(1, n_positives)) # binary response x \u0026lt;- c(rnorm(n_negatives), rnorm(n_positives, mean = 1.5)) # simulate covariate df \u0026lt;- data.frame(y = y, x = x) ER \u0026lt;- mean(df$y) # event rate Dp \u0026lt;- dnorm(df$x, mean = 1.5, sd = 1) # covariate density for positives Dn \u0026lt;- dnorm(df$x, mean = 0, sd = 1) # covariate density for negatives true_risk \u0026lt;- (ER * Dp)/((ER * Dp) + ((1 - ER) * Dn)) # true risks roc_sim \u0026lt;- roc(df$y, true_risk) # calculates ROC curve df \u0026lt;- tibble(FPR = 1 - roc_sim$specificities, # false positive rate TPR = roc_sim$sensitivities) # true positive rate return(df) } n.sims \u0026lt;- 20 # times simulation is repeated n.positives \u0026lt;- 1000 # number of positives n.negatives \u0026lt;- 4000 # number of negatives library(purrr) library(dplyr) # scenario 1 multiplier \u0026lt;- 1 # the multiplier adjusts the number of the negatives - so I can have the event rate I want sims1 \u0026lt;- n.sims %\u0026gt;% rerun(sim_data(n.positives, n.negatives * multiplier)) %\u0026gt;% map(~ data.frame(.x)) %\u0026gt;% plyr::ldply(., data.frame, .id = \u0026quot;Name\u0026quot;) %\u0026gt;% mutate(sims = rep(1:n.sims, each = sum(n.positives + n.negatives * multiplier) + 1), Scenario = \u0026quot;Scenario 1\u0026quot;) # scenario 2 multiplier \u0026lt;- 10 sims2 \u0026lt;- n.sims %\u0026gt;% rerun(sim_data(n.positives, n.negatives * multiplier)) %\u0026gt;% map(~ data.frame(.x)) %\u0026gt;% plyr::ldply(., data.frame, .id = \u0026quot;Name\u0026quot;) %\u0026gt;% mutate(sims = rep(1:n.sims, each = sum(n.positives + n.negatives * multiplier) + 1), Scenario = \u0026quot;Scenario 2\u0026quot;) df_final \u0026lt;- rbind(sims1, sims2) library(ggplot2) ggplot(df_final) + geom_line(aes(x = FPR, y = TPR, group = sims, col = Scenario), alpha = 0.8) + facet_grid(~ Scenario)  Figure 1: ROC plots for each scenario; 20 repetitions each.   Unequal misclassification costs Of course, my naive classification can be easily debated by noting that the misclassification rate makes an inherent assumption, which is unlikely to be true in anaemia screening: it assumes that misclassifying someone with anaemia is of the same severity as misclassifying a healthy subject. This implies that one type of error is more costly (i.e. worse) than the other. In other words, the costs are asymmetric. I agree that most of time this is the case. This information should be taken into account when evaluating or fitting models.\nSome methods to account for differing consequences of correct and incorrect classification when evaluating models are the Weighted Net Reclassification Improvement (Pencina et al. 2011), Relative Utility (Baker et al. 2009), Net Benefit (Vickers and Elkin 2006) and the \\(H\\) measure (Hand 2009). Another option is to design models/algorithms that take misclassification costs into consideration. This area of research is called cost-sensitive learning (Elkan 2001).\nThis is another reason the ROC (AUC) is inadequate metric (in addition to the insensitivity in class imbalance). It ignores clinical differentials in misclassification costs and, therefore, risks finding a model worthwhile (or worthless) when patients and clinicians would consider otherwise. Strictly speaking, ROC weighs changes in sensitivity and specificity equally only where the curve slope equals one (Fawcett 2006). Other points assign different weights, determined by curve shape and without considering any clinically meaningful information. Thus, AUC can consider a model that increases sensitivity at low specificity superior to one that increases sensitivity at high specificity. However, in some situations, in disease screening for instance, better tests must increase sensitivity at high specificity to avoid numerous false positives.\n A way forward: estimate and validate probabilities Ultimately, the quality of algorithms is exposed to the nature of the performance metrics chosen. We must carefully choose the goals we ask these systems to optimize. Evaluation of models for use in healthcare should take the intended purpose of the model into account. Metrics such as AUC are rarely of any use in clinical practise. AUC represents how likely it is that the model will rank a pair of subjects; one with anaemia and one without, in the correct order, across all possible thresholds. More intuitively, AUC is the chance that a randomly selected participant with anaemia will be ranked above a randomly selected healthy participant. However, patients do not walk into the clinician’s room in pairs, and patients want their results, rather than the order of their results compared with another patient. They care about their individual risk of having a disease/condition (being anaemic in this case). Hence, the focus of modelling should be on estimating and validating risks/probabilities rather than the chance of correctly ranking a pair of patients.\nConsequently, model evaluation/comparison should focus (primarily) on calibration. Calibration refers to the agreement between observed and predicted probabilities. This means that for future cases predicted to be in class \\(A\\) with probability \\(p\\), a proportion of \\(p\\) cases will truly belong in class \\(A\\), and this should be true for all \\(p\\) in (0,1). In other words, for every 100 patients given a risk of \\(p\\)%, close to \\(p\\) have the event. Calibration approaches appropriate for representing prediction accuracy are crucial, especially when treatment decisions are made based on probability thresholds. Calibration of a model can be evaluated graphically by plotting expected against observed probabilities (Steyerberg et al. 2004) or using an aggregate score. The most widely used is the Brier score, which is given by the average over all squared differences between an observation and its predicted probability (Brier 1950). (It has nice properties (see e.g. Spiegelhalter 1986; Gneiting and Raftery 2007)).\nTo conclude, machine learning models in healthcare need rigorous evaluation. Beyond ethical, legal and moral issues, the technical/statistical fitness of the models needs thorough assessment. Statistical analysis should consider clinically relevant evaluation metrics. Motivated by the paper of Mitani et al. (2020) I have re-demonstrated why AUC is an irrelevant metric for clinical practise. This is because it is insensitive to class imbalances and integrates over all error regimes (under the best case scenario). This becomes increasingly important in predicting rare outcomes, where operating in a regime that corresponds to a high false positive rate may be impractical, because costly interventions might be applied in situations in which patients are unlikely to benefit. A way forward is to focus on evaluating predictions rather than (in addition to) classifications. That is, focus on estimating and evaluating probabilities. These convey more useful information to clinicians and patients in order to aid decision making.\n Further reading The limitations of the ROC (and AUC) have been discussed in\n Cook NR . Use and misuse of the receiver operating characteristic curve in risk prediction. Circulation (2007).\n Pencina, Michael J., et al. “Evaluating the added predictive ability of a new marker: from area under the ROC curve to reclassification and beyond.” Statistics in medicine (2008).\n Hand, David J. “Evaluating diagnostic tests: the area under the ROC curve and the balance of errors.” Statistics in medicine (2010).\n Hand, David J., and Christoforos Anagnostopoulos. “When is the area under the receiver operating characteristic curve an appropriate measure of classifier performance?.” Pattern Recognition Letters (2013).\n Halligan, Steve, Douglas G. Altman, and Susan Mallett. “Disadvantages of using the area under the receiver operating characteristic curve to assess imaging tests: a discussion and proposal for an alternative approach.” European radiology (2015).\n  On probability estimation and evaluation:\n Kruppa, Jochen, Andreas Ziegler, and Inke R. König. “Risk estimation and risk prediction using machine-learning methods.” Human genetics (2012).\n Kruppa, Jochen, et al. “Probability estimation with machine learning methods for dichotomous and multicategory outcome: Theory.” Biometrical Journal (2014).\n Kruppa, Jochen, et al. “Probability estimation with machine learning methods for dichotomous and multicategory outcome: Applications.” Biometrical Journal (2014).\n   References Baker, S. G., Cook, N. R., Vickers, A., and Kramer, B. S. (2009), “Using relative utility curves to evaluate risk prediction,” Journal of the Royal Statistical Society: Series A (Statistics in Society), Wiley Online Library, 172, 729–748.\n Brier, G. W. (1950), “Verification of forecasts expressed in terms of probability,” Monthly weather review, 78, 1–3.\n Elkan, C. (2001), “The foundations of cost-sensitive learning,” in International joint conference on artificial intelligence, Lawrence Erlbaum Associates Ltd, pp. 973–978.\n Fawcett, T. (2006), “An introduction to roc analysis,” Pattern recognition letters, Elsevier, 27, 861–874.\n Gneiting, T., and Raftery, A. E. (2007), “Strictly proper scoring rules, prediction, and estimation,” Journal of the American statistical Association, Taylor \u0026amp; Francis, 102, 359–378.\n Hand, D. J. (2009), “Measuring classifier performance: A coherent alternative to the area under the roc curve,” Machine learning, Springer, 77, 103–123.\n Mitani, A., Huang, A., Venugopalan, S., Corrado, G. S., Peng, L., Webster, D. R., Hammel, N., Liu, Y., and Varadarajan, A. V. (2020), “Detection of anaemia from retinal fundus images via deep learning,” Nature Biomedical Engineering, Nature Publishing Group, 4, 18–27.\n Pencina, M. J., D’Agostino Sr, R. B., and Steyerberg, E. W. (2011), “Extensions of net reclassification improvement calculations to measure usefulness of new biomarkers,” Statistics in medicine, Wiley Online Library, 30, 11–21.\n Spiegelhalter, D. (2020), “Should we trust algorithms?” Harvard Data Science Review, 2. https://doi.org/10.1162/99608f92.cb91a35a.\n Spiegelhalter, D. J. (1986), “Probabilistic prediction in patient management and clinical trials,” Statistics in medicine, Wiley Online Library, 5, 421–433.\n Steyerberg, E. W., Borsboom, G. J., Houwelingen, H. C. van, Eijkemans, M. J., and Habbema, J. D. F. (2004), “Validation and updating of predictive logistic regression models: A study on sample size and shrinkage,” Statistics in medicine, Wiley Online Library, 23, 2567–2586.\n Vickers, A. J., and Elkin, E. B. (2006), “Decision curve analysis: A novel method for evaluating prediction models,” Medical Decision Making, Sage Publications Sage CA: Thousand Oaks, CA, 26, 565–574.\n   ","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582156800,"objectID":"d02f5cf360cbef5a25ac3b0ce830fda1","permalink":"https://solon-karapanagiotis.com/post/auc_post/model-evaluation-auc/","publishdate":"2020-02-20T00:00:00Z","relpermalink":"/post/auc_post/model-evaluation-auc/","section":"post","summary":"Overview Mitani and co-authors’ present a deep-learning algorithm trained with retinal images and participants’ clinical data from the UK Biobank to estimate blood-haemoglobin levels and predict the presence or absence of anaemia (Mitani et al.","tags":["(ML) reporting","model evaluation","R"],"title":"Naive classification beats deep-learning","type":"post"},{"authors":null,"categories":["probability","R"],"content":" It is usually taught in statistics classes that Binomial probabilities can be approximated by Poisson probabilities, which are generally easier to calculate. This approximation is valid “when \\(n\\) is large and \\(np\\) is small,” and rules of thumb are sometimes given.\nIn this post I’ll walk through a simple proof showing that the Poisson distribution is really just the Binomial with \\(n\\) (the number of trials) approaching infinity and \\(p\\) (the probability of success in each trail) approaching zero. I’ll then provide some numerical examples to investigate how good is the approximation.\nProof The Binomial distribution describes the probability that there will be \\(x\\) successes in a sample of size \\(n\\), chosen with replacement from a population where the probability of success is \\(p\\).\nLet \\(X \\sim Binomial(n, p)\\), that is\n\\[\\begin{equation} \\tag{1} P(X = x) = {n\\choose x} p^x (1-p)^{n-x}, \\end{equation}\\]\nwhere \\(x= 0, 1, \\dots, n\\). Define the number\n\\[\\lambda = np\\]\nThis is the rate of success. That’s the number of trials \\(n\\)—however many there are—times the chance of success \\(p\\) for each of those trials. If we repeat the experiment every day, we will be getting \\(\\lambda\\) successes per day on average.\nSolving for \\(p\\), we get:\n\\[ p = \\frac{\\lambda}{n}\\] We then substitute this into (1), and take the limit as \\(n\\) goes to infinity\n\\[ \\lim_{n \\to \\infty}P(X = x) = \\lim_{n \\to \\infty} \\frac{n!}{x!(n-x)!} \\bigg( \\frac{\\lambda}{n} \\bigg)^x \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^{n-x}\\]\nI then collect the constants (terms that don’t depend on \\(n\\)) in front and split the last term into two\n\\[\\begin{equation} \\tag{2} \\frac{\\lambda^x}{x!} \\lim_{n \\to \\infty} \\color{blue}{\\frac{n!}{(n-x)!} \\bigg( \\frac{1}{n} \\bigg)^x} \\color{red}{ \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^n } \\color{green}{\\bigg( 1-\\frac{\\lambda}{n} \\bigg)^{-x}} \\end{equation}\\]\nNow let’s take the limit of this right-hand side one term at a time.\nWe start with the blue term  \\[\\color{blue}{ \\lim_{n \\to \\infty} \\frac{n!}{(n-x)!} \\bigg( \\frac{1}{n} \\bigg)^x }\\] The numerator and denominator can be expanded as follows\n\\[\\color{blue}{ \\lim_{n \\to \\infty} \\frac{(n)(n-1)(n-2)\\dots(n-x)(n-x-1)\\dots (1)}{(n-x)(n-x-1)(n-x-2)\\dots (1)}\\bigg( \\frac{1}{n} \\bigg)^x }\\]\nThe \\((n-x)(n-x-1)\\dots(1)\\) terms cancel from both the numerator and denominator, leaving the following\n\\[\\color{blue}{ \\lim_{n \\to \\infty} \\frac{(n)(n-1)(n-2)(n-x+1)}{n^x} }\\] This can be rewrited as\n\\[\\color{blue}{ \\lim_{n \\to \\infty} \\frac{n}{n} \\frac{(n-1)}{n} \\frac{(n-2)}{n} \\frac{(n-x+1)}{n} }\\] This is because there were \\(x\\) terms in both the numerator and denominator. Clearly, every one of these \\(x\\) terms approaches 1 as \\(n\\) approaches infinity. So we know this just simplifies to one. So we’re done with the first step.\nNow we focus on the red term of (2)  \\[\\color{red}{ \\lim_{n \\to \\infty} \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^n }\\]\nRecall the definition of \\(e= 2.7182\\dots\\) is\n\\[ \\lim_{a \\to \\infty} \\bigg(1 + \\frac{1}{a}\\bigg)^a\\] Our goal here is to find a way to manipulate our expression to look more like the definition of \\(e\\), which we know the limit of. Let’s define a number \\(a\\) as\n\\[ a = -\\frac{n}{\\lambda}\\]\nSubstituting it into our expression we get\n\\[ \\color{red}{ \\lim_{n \\to \\infty} \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^n = \\lim_{n \\to \\infty} \\bigg( 1+\\frac{1}{a} \\bigg)^{-a\\lambda} = e^{-\\lambda} }\\] So we’ve finished with the middle term.\nThe third term of (2) is  \\[\\color{green}{ \\lim_{n \\to \\infty} \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^{-x} }\\] As \\(n\\) approaches infinity, this term becomes \\(1^{-x}\\) which is equal to one. And that takes care of our last term.\nPutting these together we can re-write (2) as\n\\[ \\frac{\\lambda^x}{x!} \\lim_{n \\to \\infty} \\color{blue}{ \\frac{n!}{(n-x)!} \\bigg( \\frac{1}{n} \\bigg)^x} \\color{red}{ \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^n} \\color{green}{ \\bigg( 1-\\frac{\\lambda}{n} \\bigg)^{-x} } = \\frac{\\lambda^x}{x!} \\color{red}{ e^{-\\lambda} }\\] which is the probability mass function of a Poisson random variable \\(Y\\), i.e\n\\[P(Y = y) = \\frac{\\lambda^y}{y!} e^{-\\lambda}\\]\nwhere \\(y = 0, 1, \\dots\\). So we have shown that the Poisson distribution is a special case of the Binomial, in which the number of trials grows to infinity and the chance of success in any trial approaches zero. And that completes the proof.\nCasella and Berger (2002) provide a much shorter proof based on moment generating functions.\nA natural question is how good is this approximation? It turns out it is quite good even for moderate \\(p\\) and \\(n\\) as we’ll see with a few numerical examples.\n Code A rule of thumb says for the approximation to be good:\n “The sample size \\(n\\) should be equal to or larger than 20 and the probability of a single success, \\(p\\), should be smaller than or equal to 0.05. If \\(n\\) \u0026gt; 100, the approximation is excellent if \\(np\\) is also \u0026lt; 10.”\n Let’s try a few scenarios. I have slightly modified the code from here.\n# plots the pmfs of Binomial and Poisson pl \u0026lt;- function(n, p, a, b) { clr \u0026lt;- rainbow(15)[ceiling(c(10.68978, 14.24863))] lambda \u0026lt;- n * p mx \u0026lt;- max(dbinom(a:b, n, p)) plot( c(a:b, a:b), c(dbinom(a:b, n, p), dpois(a:b, lambda)), type = \u0026quot;n\u0026quot;, main = paste(\u0026quot;Poisson Approx. to Binomial, n=\u0026quot;, n, \u0026quot;, p=\u0026quot;, p, \u0026quot;, lambda=\u0026quot;, lambda), ylab = \u0026quot;Probability\u0026quot;, xlab = \u0026quot;x\u0026quot;) points((a:b) - .15, dbinom(a:b, n, p), type = \u0026quot;h\u0026quot;, col = clr[1], lwd = 10) points((a:b) + .15, dpois(a:b, lambda), type = \u0026quot;h\u0026quot;, col = clr[2], lwd = 10) legend(b - 3.5, mx, legend = c(\u0026quot;Binomial(x,n,p)\u0026quot;, \u0026quot;Poisson(x,lambda)\u0026quot;), fill = clr, bg = \u0026quot;white\u0026quot;) } I start with the recommendation: \\(n\\) = 20, \\(p\\) = 0.05. This gives \\(\\lambda= 1\\). Already the approximation seems reasonable.\npl(20, 0.05, 0, 10) For \\(n\\) = 10, \\(p\\) = 0.3 it doesn’t seem to work very well.\npl(10, 0.3, 0, 10) But if we increase \\(n\\) and decrease \\(p\\) in order to come home with the same \\(\\lambda\\) value things improve.\npl(100, 0.03, 0, 10) Lastly, for 1000 trials the distributions are indistinguishable.\npl(1000, 0.003, 0, 10)  References Casella, George, and Roger L Berger. 2002. Statistical Inference. Vol. 2. Duxbury Pacific Grove, CA.\n   ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"420fc3eee2f69b74d0c4b10af0c4ab08","permalink":"https://solon-karapanagiotis.com/post/approx_binomial/approximating-binomial-with-poisson/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/approx_binomial/approximating-binomial-with-poisson/","section":"post","summary":"It is usually taught in statistics classes that Binomial probabilities can be approximated by Poisson probabilities, which are generally easier to calculate. This approximation is valid “when \\(n\\) is large and \\(np\\) is small,” and rules of thumb are sometimes given.","tags":["probability","R"],"title":"Approximating Binomial with Poisson","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://solon-karapanagiotis.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["probability"],"content":" I present a solution to a modification of the “hardest logic puzzle ever” using probability theory.\nBackground “The hardest logic puzzle” was originally presented by Boolos (1996) and since then it has been amended several times in order to make it harder (see Rabern and Rabern 2008, @novozhilov2012hardest).\nThe puzzle: Three gods A, B, and C are called, in some order, True, False, and Random. True always speaks truly, False always speaks falsely, but whether Random speaks truly or falsely is a completely random matter. Your task is to determine the identities of A, B, and C by asking three yes-no questions; each question must be put to exactly one god. The gods understand English, but will answer all questions in their own language, in which the words for “yes” and “no” are “da” and “ja,” in some order. You do not know which word means which. \nBoolos (1996) then provides the following guidelines:\n1. It could be that some god gets asked more than one question (and hence that some god is not asked any question at all).\n2. What the second question is, and to which god it is put, may depend on the answer to the first question. (And of course similarly for the third question.)\n3. Whether Random speaks truly or not should be thought of as depending on the flip of a coin hidden in his brain: if the coin comes down heads, he speaks truly; if tails, falsely.\n4. Random will answer da or ja when asked any yes-no question.\nRabern and Rabern (2008) proposed to modify the third point above with the following: “Whether Random answers ‘da’ or ‘ja’ should be thought of as depending on the flip of a coin hidden in his brain: if the coin comes down heads, he answers ‘yes’; if tails, ‘no’.”\nBoolos’ article includes multiple ways of solving the problem. Rabern and Rabern (2008) give a simpler solution. The main ideas for the solutions can be found here and here.\n My solution My solution is based on the long-run frequency interpretation of probability. It involves two steps. At the first step we will identify the Random god and in step 2 we distinguish between the True and False gods.\nStep 1\nImagine the following scenario: you keep asking the same question to each god. The question is different for each god. Under the interpretation of probability as long-run frequency both True and False will always give the same answer. For example, if A is the True god he will always answer “da” or “ja” and similarly the False god will always answer the opposite. The crucial point is that Random will change between “da” and “ja” because his answers are random, “they depend on the flip of a coin hidden in his brain”. Suppose you ask your question to Random ten times, and assuming the coin in his head is fair (i.e., P(heads) = P(tails) = 0.5) then the probability that all his answers are same ( “da” or “ja” ) is \\(0.5^{10}\\), that is highly unlikely. In fact, you do not need to pre-specify how many times you ask the question, since the moment a given god switches from “da” to “ja” or vice-versa you know he is Random. Having identified Random we proceed to distinguish between True and False. An example question to each one is “are you True”?\nStep 2\nFor simplicity let’s assume C is Random. Now, we only need to identify one more god. Let’s use god A for illustration. All possibilities regarding god A and the word “da” are given below:\nA is True and “da” means “yes”, A is True and “da” means “no”, A is False and “da” means “yes”, A is False and “da” means “no”,  Then ask A the following question:\nQ1: Is C Random?\nAnd B:\nQ2: Is A True?\nFor each scenario above we end up with the following pattern:\n If scenario (1) the answers are “da” and “ja” for Q1 and Q2 respectively. If scenario (2) the answers are “ja” and “da”. If scenario (3) the answers are “ja” and “da”. If scenario (4) the answers are “da” and “da”.  Looking more carefully at the answers we distinguish 3 distinct patterns for the answers:\n P1: “da”and “ja”, P2: “ja” and “da” and P3: “da” and “da”.  Furthermore, P1 and P3 are unique, they appear only once. That means if the gods answer Q1 and Q2 using P1 or P3 we have identified them and the game is over! For example, if they answer with P3 then scenario (4) was correct: A is False and “da” means “no” and consequently B is True and “ja” means “yes”. If they answer using P2 then we need a further question because both scenarios (2) and (3) may be right. We can ask A: (repeat the 1st question)\nQ3: Are you True?\nNow, if scenario (2) the answer is “ja” and if scenario (3) the answer is “da”.\nUsing this approach we have also identified the meanings of “da” and “ja”.\n Comments The modification I used was allowing each question to be put to more than one god. In step 1 the question “Are you True?” was put to all gods and was repeated in step 2 as Q3. So technically I have solved the puzzle using only three questions in total, but allowing myself to repeat the same questions to more than one god.\nBoolos (1996) provided his solution in the same article in which he introduced the puzzle. He states that the “first move is to find a god that you can be certain is not Random, and hence is either True or False”. My approach does the reverse; first identifies the Random god and then the True and False gods.\n References Boolos, George. 1996. “The Hardest Logic Puzzle Ever.” The Harvard Review of Philosophy 6 (1): 62–65.\n Novozhilov, Nikolay. 2012. “The Hardest Logic Puzzle Ever Becomes Even Tougher.” arXiv Preprint arXiv:1206.1926.\n Rabern, Brian, and Landon Rabern. 2008. “A Simple Solution to the Hardest Logic Puzzle Ever.” Analysis 68 (2): 105–12.\n   ","date":1533254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533254400,"objectID":"485f2f95dcc4320e57b7608405c5a95b","permalink":"https://solon-karapanagiotis.com/post/hardest_puzzle/the-hardest-logic-puzzle/","publishdate":"2018-08-03T00:00:00Z","relpermalink":"/post/hardest_puzzle/the-hardest-logic-puzzle/","section":"post","summary":"I present a solution to a modification of the “hardest logic puzzle ever” using probability theory.\nBackground “The hardest logic puzzle” was originally presented by Boolos (1996) and since then it has been amended several times in order to make it harder (see Rabern and Rabern 2008, @novozhilov2012hardest).","tags":["probability"],"title":"Another solution to the 'The Hardest Logic Puzzle Ever' using probability","type":"post"},{"authors":null,"categories":["data analysis"],"content":" Recently, I came across this very interesting article published in Science about how plastic waste is associated with disease on coral reefs (J. B. Lamb et al. 2018). The main conclusions are\ncontact with plastic increases the probability of disease,\n the morphological structure of the reefs is associated with the probability of being in contact with plastic with more complex ones being more likely to be affected by plastic,\n the plastic levels correspond to estimates of mismanaged plastic waste into the ocean.  Overall, this study provides evidence how plastic waste negatively affects coral reefs, making them more susceptible to diseases. The authors made available both the datasets they used and the code (both can be downloaded from J. Lamb et al. 2018 - an excellent example of reproducible research). The methods section is straightforward to follow (see Supplementary Materials). My comment is about the 2nd point above, and more specifically the methodology that led to this conclusion (see Fig. 4 of the article). The issue is the authors interpret the models they are using wrongly. Let me explain …\nTheir model is a simple generalised linear mixed model (GLMM) - binomial error distribution and logistic link. The outcome is the disease prevalence (binary) among coral reefs with different morphology. The morphology assignments were massive, tabular, and branching (3-level categorical covariate). The morphological assignments were treated as fixed factors and the site as random (in order to take into account the correlation between reefs due to their geographical position). The model is\n\\[ logit(Disease Presense_{ik}) = \\sum_j \\beta_j x_{ik} + b_i \\]\nwhere \\(j_{1:3} = \\{massive, tabular, branching\\}\\) and \\(b_i\\) are the reef-specific intercepts. Such a \\(b_i\\) represents the deviation of the intercept of a specific reef from the average intercept in the group to which that reef belongs, i.e deviation from \\(\\beta_1\\), \\(\\beta_2\\) or \\(\\beta_3\\). The model is fitted only for reefs unaffected by plastic waste. The output is given in Fig. 4(B) in the paper. The conclusion is the disease risk increases from massive to branching and tabular reefs when not in contact with plastic debris (Fig. 4(B) and table S13).\nThe issue with this figure is the authors give a population-average interpretation of the coefficients. In GLLMs the fixed effects have a site-specific interpretation but not a population-average one. Let us now consider the logistic random-intercepts model above. The conditional means \\(E[Disease Presense_{ik}|b_i]\\) are given by\n\\[ E[Disease Presense_{ik}|b_i] = \\frac{\\exp(\\sum_j \\beta_j x_{ik} + b_i)}{1 + \\exp(\\sum_j \\beta_j x_{ik} + b_i)} \\] where \\(E[.]\\) is the expectation operator. The above model assumes logistic change in prevalence of disease for each morphology, all having different intercepts \\(\\beta_0 + b_i\\). The average reef, i.e, the reef with intercept \\(b_i = 0\\), has disease probability given by\n\\[ E[Disease Presense_{ik}|b_i = 0] = \\frac{\\exp(\\sum_j \\beta_j x_{ik} + 0)}{1 + \\exp(\\sum_j \\beta_j x_{ik} + 0)} \\]\nwhich is what the authors have calculated and produced Fig. 4. In other words, the authors have calculated the probability of disease for an “average” reef. They proceed interpreting this as marginal effect, which is wrong.\nThe issue arises due to the conditional interpretation, conditionally upon level of random effects, of the \\(\\beta\\)s in a GLMM model. And this is due to the fact that \\(E[g(Y )] \\neq g[E(Y)]\\) unless \\(g\\) is linear, which is not the case for this model. In what follows I fit the same model and demonstrate how the conclusions change when conditioning of different levels of the random coefficients. The code the authors use is\n# GLMM, Baseline Disease levels for different growth forms, Asia Pacific -------- library(lme4) Normal.Disease.Growth = glmer(Disease ~ -1 + Growth2+(1|Reef_Name), data = Plastic[which(Plastic$Plastic==0),], family = \u0026#39;binomial\u0026#39;, control = glmerControl(optimizer =\u0026quot;bobyqa\u0026quot;)) # As a sidenote: This code uses a Laplace approximation (nAGQ = 1 - the default) on the integral over the random effects space. \u0026quot;Values greater than 1 produce greater accuracy in the evaluation of the log-likelihood at the expense of speed\u0026quot;. The authors of the package suggest values up to 25 (see the documentation).  The following reproduces Fig. 4(B) of the publication.\n# PDF, Baseline disease levels by growth form ----------------------------- NormalDisease.by.Growth = data.frame(Tabular = rnorm(100000, mean = -3.1332, sd = .1549), Massive = rnorm(100000, mean = -3.8153, sd = .1095), Branching = rnorm(100000, mean = -3.5534, sd = .1103)) NormalDisease.by.Growth$Tabularbt = plogis(NormalDisease.by.Growth$Tabular) NormalDisease.by.Growth$Massivebt = plogis(NormalDisease.by.Growth$Massive) NormalDisease.by.Growth$Branchingbt = plogis(NormalDisease.by.Growth$Branching) NormalDisease.by.Growth = gather(NormalDisease.by.Growth, Growth, Estimate, Tabularbt:Branchingbt) library(ggplot2) ggplot(aes(x = Estimate*100), data = NormalDisease.by.Growth) + geom_density(aes(y = ..scaled.., fill = Growth)) + scale_x_continuous(limits = c(0, 10)) + ylab(\u0026quot;\u0026quot;) + labs(fill = \u0026quot;Morphology\u0026quot;) It is evident from the code that they plot the fixed effects estimates with their standard errors. This plot ignores the random effects and it only takes into consideration the variation of the fixed coefficients \\(\\beta_j\\). To get an idea for the variability of the random effects I simulate them from the model and plot them. Points that are distinguishable from zero (i.e. the confidence band based on level does not cross the red line) are highlighted. We see substantial variation on the random effects estimates with many “outliers” with both high and low averages that need to be accounted for.\nlibrary(merTools) sim_rfs_Normal.Disease \u0026lt;- REsim(Normal.Disease.Growth, n.sims = 200) plotREsim(sim_rfs_Normal.Disease) What the authors are effectively doing in Fig. 4(B) (see density plot above) is presenting the results for reefs with \\(b_i = 0\\) which corresponds to the red horizontal line. Let’s see how the density plot changes when we condition on more “extreme” reefs. I use the 0.1 and 0.9 quantiles.\nquantile0.9 \u0026lt;- REquantile(Normal.Disease.Growth, quantile = 0.9, groupFctr = \u0026quot;Reef_Name\u0026quot;) #which(sim_rfs_Normal.Disease$groupID == quantile0.9) quantile0.1 \u0026lt;- REquantile(Normal.Disease.Growth, quantile = 0.1, groupFctr = \u0026quot;Reef_Name\u0026quot;) #which(sim_rfs_Normal.Disease$groupID == quantile0.1) NormalDisease.by.Growth_quantile0.9 = data.frame( Tabular = rnorm(100000, mean = -3.1332 + 1.495773, sd = .1549), Massive = rnorm(100000, mean = -3.8153 + 1.495773, sd = .1095), Branching = rnorm(100000, mean = -3.5534 + 1.495773, sd = .1103)) NormalDisease.by.Growth_quantile0.9$Tabularbt = plogis(NormalDisease.by.Growth_quantile0.9$Tabular) NormalDisease.by.Growth_quantile0.9$Massivebt = plogis(NormalDisease.by.Growth_quantile0.9$Massive) NormalDisease.by.Growth_quantile0.9$Branchingbt = plogis(NormalDisease.by.Growth_quantile0.9$Branching) NormalDisease.by.Growth_quantile0.9 = gather(NormalDisease.by.Growth_quantile0.9, Growth, Estimate, Tabularbt:Branchingbt) NormalDisease.by.Growth_quantile0.1 = data.frame( Tabular = rnorm(100000, mean = -3.1332 - 1.689569, sd = .1549), Massive = rnorm(100000, mean = -3.8153 - 1.689569, sd = .1095), Branching = rnorm(100000, mean = -3.5534 - 1.689569, sd = .1103)) NormalDisease.by.Growth_quantile0.1$Tabularbt = plogis(NormalDisease.by.Growth_quantile0.1$Tabular) NormalDisease.by.Growth_quantile0.1$Massivebt = plogis(NormalDisease.by.Growth_quantile0.1$Massive) NormalDisease.by.Growth_quantile0.1$Branchingbt = plogis(NormalDisease.by.Growth_quantile0.1$Branching) NormalDisease.by.Growth_quantile0.1 = gather(NormalDisease.by.Growth_quantile0.1, Growth, Estimate, Tabularbt:Branchingbt) NormalDisease.by.Growth$ID = \u0026quot;average\u0026quot; NormalDisease.by.Growth_quantile0.1$ID = \u0026quot;0.1quantile\u0026quot; NormalDisease.by.Growth_quantile0.9$ID = \u0026quot;0.9quantile\u0026quot; overall \u0026lt;- rbind(NormalDisease.by.Growth, NormalDisease.by.Growth_quantile0.1, NormalDisease.by.Growth_quantile0.9) ggplot(aes(x = Estimate*100, col = ID), data = overall) + geom_density(aes(y = ..scaled.., fill = Growth), alpha = 0.9, size = 1.3) + scale_fill_brewer(palette = \u0026quot;Spectral\u0026quot;) + #scale_fill_manual(values = c(\u0026quot;#D55E00\u0026quot;, \u0026quot;#009E73\u0026quot;, \u0026quot;#0072B2\u0026quot;)) + scale_color_manual(values = c(\u0026quot;#000000\u0026quot;, \u0026quot;dodgerblue\u0026quot;, \u0026quot;darkmagenta\u0026quot;)) + scale_x_continuous(limits = c(0, 10)) + ylab(\u0026quot;\u0026quot;) + labs(col = \u0026quot;R effect\u0026quot;, fill = \u0026quot;Morphology\u0026quot;) It is evident both the center and the variability of the distributions change depending whether we look an “average” coral reef (purple line), a reef towards the upper extreme (blue line) or the lower extreme (black line). So the conclusions should be something along the lines: the increase in disease likelihood with plastic debris depends also on inherit/unobserved characteristics of the reefs, captured by the random effects, in addition to their morphology.\nOf course, what I have presented above is still conditional interpretation of the parameters. Ideally, we want the marginal population-average interpretation which is obtained from averaging over the random effects. This allows to take into account both the residual (observation-level) variance, the uncertainty in the variance parameters for the grouping factors added to the uncertainty in the fixed coefficients. See for example the predictInterval() function of the merTools package.\nReferences Lamb, JB, BL Willis, EA Fiorenza, CS Couch, R Howard, DN Rader, JD True, et al. 2018. “Data from: Plastic Waste Associated with Disease on Coral Reefs.” Science. Dryad Digital Repository. https://doi.org/10.5061/dryad.mp480.\n Lamb, Joleah B, Bette L Willis, Evan A Fiorenza, Courtney S Couch, Robert Howard, Douglas N Rader, James D True, et al. 2018. “Plastic Waste Associated with Disease on Coral Reefs.” Science 359 (6374): 460–62.\n   ","date":1526774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526774400,"objectID":"8b599fc00a28470359c907b53baf5d5f","permalink":"https://solon-karapanagiotis.com/post/reefs/plastic-waste-and-disease-on-coral-reefs/","publishdate":"2018-05-20T00:00:00Z","relpermalink":"/post/reefs/plastic-waste-and-disease-on-coral-reefs/","section":"post","summary":"Recently, I came across this very interesting article published in Science about how plastic waste is associated with disease on coral reefs (J. B. Lamb et al. 2018). The main conclusions are","tags":["data analysis","misinterpretation","random effects models"],"title":"Plastic waste and disease on coral reefs - Another misinterpretation of a statistical model","type":"post"},{"authors":null,"categories":["guidelines","reporting"],"content":" Poor quality statistical reporting in the biomedical literature is not uncommon. Here is another example by Cirio et al. (2016). The study itself is well planed, executed and reported. The aim was to assess whether heated and humidified high flow gases delivered through nasal cannula (HFNC) improve exercise performance in severe chronic obstructive pulmonary disease (COPD) patients. It all started when I saw their Fig.1. Here is my attempt to reproduce it\n Figure 1: Effect of the HFNC on exercise capacity compared to a control condition. Tlim = exercise duration.  In total there are 12 patients tested twice; once under the control test and once under the HFNC test. The outcome of interest is the endurance time (Tlim; y axis). This is practically how long each test lasted. The authors hypothesized that HFNC would improve exercise performance, that is the test would last longer. This was the case since Tlim increased for all subjects under the HFNC test (see figure 1). Moreover, this increase reached statistical significance (p-value = 0.015) - ready to publish! Looking at the plot I was pondered about the “outlying” patient (red dot). His/her Tlim increased by a whooping 400 seconds! This is huge compared to the other patients. Then I wondered how would the results change if we excluded him/her from the analysis? And here is where the problems start.\nThere is no way from the text to figure out which test was used to produce the p-value of 0.015. Is is a paired t-test or a Wilcoxon test? (they mention both in the statistical analysis section). So it is impossible to evaluate and/or try to reproduce the results.\nHaving abandoned the idea of being able to reproduce the analysis I started thinking about reporting guidelines, hence the title of this post. I thought the journal must have guidelines for reporting statistical analysis. No, it does not and unfortunately, most of the biomedical journals don’t have such guidelines even though 40 years ago O’Fallon and colleges recommended that “Standards governing the content and format of statistical aspects should be developed to guide authors in the preparation of manuscripts” (O’Fallon et al. 1978). Since then many have repeated the message. A few sporadic attempts are usually editorials such as Cummings and Rivara (2003), Curran-Everett and Benos (2004) and Arifin et al. (2016).\nRecently, Lang and Altman (2013) published a comprehensive set of statistical reporting guidelines suitable for medical journals - the SAMPL guidelines. “The SAMPL guidelines are designed to be included in a journal’s Instructions for Authors”. So the journals just need to refer to them! As there are many general reporting guidelines based on the study design as such CONSORT, STROBE, PRISMA etc (see http://www.equator-network.org/) that authors in many journals must adhere to, I believe the SAMPL guidelines is a big step forward on reporting statistics. The only journal (that I know of) that suggests the use of the SAMPL guidelines is the British Journal of Dermatology (Hollestein and Nijsten 2015). (I’ll keep adding to this list).\nNow that the guidelines exist, let’s make use of them.\nReferences Arifin, Wan Nor, Abdullah Sarimah, Bachok Norsa’adah, Yaacob Najib Majdi, Ab Hamid Siti-Azrin, Musa Kamarul Imran, Abd Aziz Aniza, and Lin Naing. 2016. “Reporting Statistical Results in Medical Journals.” The Malaysian Journal of Medical Sciences: MJMS 23 (5): 1.\n Cirio, Serena, Manuela Piran, Michele Vitacca, Giancarlo Piaggi, Piero Ceriana, Matteo Prazzoli, Mara Paneroni, and Annalisa Carlucci. 2016. “Effects of Heated and Humidified High Flow Gases During High-Intensity Constant-Load Exercise on Severe Copd Patients with Ventilatory Limitation.” Respiratory Medicine 118: 128–32.\n Cummings, Peter, and Frederick P Rivara. 2003. “Reporting Statistical Information in Medical Journal Articles.” Archives of Pediatrics \u0026amp; Adolescent Medicine 157 (4): 321–24.\n Curran-Everett, Douglas, and Dale J Benos. 2004. “Guidelines for Reporting Statistics in Journals Published by the American Physiological Society.” Am Physiological Soc.\n Hollestein, LM, and Tamar Nijsten. 2015. “Guidelines for Statistical Reporting in the British Journal of Dermatology.” British Journal of Dermatology 173 (1): 3–5.\n Lang, Thomas A, and Douglas G Altman. 2013. “Basic Statistical Reporting for Articles Published in Biomedical Journals: The ‘Statistical Analyses and Methods in the Published Literature’ or the Sampl Guidelines”.” Handbook, European Association of Science Editors 256: 256.\n O’Fallon, JR, SD Dubey, DS Salsburg, JH Edmonson, A Soffer, and T Colton. 1978. “Should There Be Statistical Guidelines for Medical Research Papers?” Biometrics, 687–95.\n   ","date":1526342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526342400,"objectID":"48baa9638f24c77831706a3c80545d5f","permalink":"https://solon-karapanagiotis.com/post/stat_reporting/on-statistical-reporting-in-biomedical-journals/","publishdate":"2018-05-15T00:00:00Z","relpermalink":"/post/stat_reporting/on-statistical-reporting-in-biomedical-journals/","section":"post","summary":"Poor quality statistical reporting in the biomedical literature is not uncommon. Here is another example by Cirio et al. (2016). The study itself is well planed, executed and reported. The aim was to assess whether heated and humidified high flow gases delivered through nasal cannula (HFNC) improve exercise performance in severe chronic obstructive pulmonary disease (COPD) patients.","tags":["guidelines","(stat) reporting"],"title":"On statistical reporting in biomedical journals","type":"post"},{"authors":null,"categories":["R","data"],"content":" There are many tutorials for importing data into R focusing on a specific function/package. This one focuses on 3 different packages. You will learn how to import all common formats of flat file data with base R functions and the dedicated readr and data.table packages. I first present these three packages and finish with a comparison table between them.\nTask Import a flat file into R: create an R object that contains the data from a flat file.\n What is a flat file? A flat file can be a plain text file that contains table data. A form of flat file is one in which table data is gathered in lines with the value from each table cell separated by a comma and each row represented with a new line. This type of flat file is also known as a comma-separated values (CSV) file. An alternative is a tab-delimited file where each field value is separated from the next using tabs.\nThe following sections describe various options for importing flat files. The ultimate goal is to convey, “translate”, them into an R data.frame.\n What are we going to import? For illustration purposes we use the Happiness dataset. It is based on the European quality of life survey with questions related to income, life satisfaction or perceived quality of society. The file is quite small but enough to sharpen your importing skills. It provides the average rating for the question “How happy would you say you are these days?”. Rating 1 (low) to 10 (high) by country and gender.\n## Country Gender Mean N. ## 1 AT Male 7.3 471 ## 2 Female 7.3 570 ## 3 Both 7.3 1041 ## 4 BE Male 7.8 468 ## 5 Female 7.8 542 ## 6 Both 7.8 1010 ## 7 BG Male 5.8 416 ## 8 Female 5.8 555 ## 9 Both 5.8 971 ## 10 CY Male 7.8 433  Let’s get going… the utils We start with the utils package. This package is loaded by default when you start your R session. This means that you can access its functions without further due. Here, we are interested in three of them: read.table(), read.csv(), and read.delim().\nReading data with read.table() Reads a file in table format and creates an R data.frame from it, with cases corresponding to rows and variables to columns. Let’s see how it works for our dataset.\nhappiness \u0026lt;- read.table(\u0026quot;happiness.csv\u0026quot;) head(happiness) ## V1 ## 1 Country,Gender,Mean,N= ## 2 AT,Male,7.3,471 ## 3 ,Female,7.3,570 ## 4 ,Both,7.3,1041 ## 5 BE,Male,7.8,468 ## 6 ,Female,7.8,542 Not what we wanted?! This data frame contains 108 rows and 1 column instead of 105 rows and 4 columns. That’s because additional arguments need to be specified in order to tell R what it has to deal with.\nhappiness \u0026lt;- read.table(file = \u0026quot;happiness.csv\u0026quot;, # path to flat file header = TRUE, # first row lists variables\u0026#39; names sep = \u0026quot;,\u0026quot;, # field separator is a comma stringsAsFactors = FALSE) # not import strings as categorical variables Let’s take a look now\nhead(happiness) ## Country Gender Mean N. ## 1 AT Male 7.3 471 ## 2 Female 7.3 570 ## 3 Both 7.3 1041 ## 4 BE Male 7.8 468 ## 5 Female 7.8 542 ## 6 Both 7.8 1010 str(happiness) ## \u0026#39;data.frame\u0026#39;: 105 obs. of 4 variables: ## $ Country: chr \u0026quot;AT\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;BE\u0026quot; ... ## $ Gender : chr \u0026quot;Male\u0026quot; \u0026quot;Female\u0026quot; \u0026quot;Both\u0026quot; \u0026quot;Male\u0026quot; ... ## $ Mean : num 7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ... ## $ N. : int 471 570 1041 468 542 1010 416 555 971 433 ... By specifying header = TRUE R sees the that the first line contains the names of the variables. With stringsAsFactors = FALSE we specify that we wanted Country and Gender to be character variables. The sep = \",\" identifies the field separator to be a comma. There are many more arguments you can specify and each one can take many values! For further details, consult the R documentation or type help(read.table) on the console.\n Note: In order to use read.table(), in same manner, you need to give the full path name of the target file if it’s not in your working directory. You can use the R Function of the Day, namely setwd(\"\u0026lt;location of your dataset\u0026gt;\"), to change your working directory. The same is valid for any other function we are going to encounter in this tutorial. Alternatively, you can specify the location of the flat file inside read.table(). Keep in mind that the specification of the file is platform dependent (Windows, Unix/Linux and OSX).\n read.table(file = \u0026quot;\u0026lt;location of your dataset\u0026gt;\u0026quot;, ...)   Another option is to use file.path(). It constructs the path to a file from components in a platform-independent way. For example,\n path \u0026lt;- file.path(\u0026quot;~\u0026quot;, \u0026quot;datasets\u0026quot;, \u0026quot;happiness.csv\u0026quot;) happiness \u0026lt;- read.table(file = path, header = TRUE, sep = \u0026quot;,\u0026quot;, stringsAsFactors = FALSE) Comment The stringsAsFactors argument is true by default which means that character variables are imported into R as factors, the data type to store categorical variables.\nhappiness_2 \u0026lt;- read.table(file = \u0026quot;happiness.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot;, stringsAsFactors = TRUE) At first sight you do not notice anything different and you shouldn’t! But for R it’s a big deal! For character variables each element is a string of one or more characters. On the other hand, factor variables are stored, internally, as numeric variables together with their levels. This has major impact in computations that R maybe has to carry out later.\nstr(happiness_2) ## \u0026#39;data.frame\u0026#39;: 105 obs. of 4 variables: ## $ Country: Factor w/ 36 levels \u0026quot;\u0026quot;,\u0026quot;AT\u0026quot;,\u0026quot;BE\u0026quot;,\u0026quot;BG\u0026quot;,..: 2 1 1 3 1 1 4 1 1 6 ... ## $ Gender : Factor w/ 3 levels \u0026quot;Both\u0026quot;,\u0026quot;Female\u0026quot;,..: 3 2 1 3 2 1 3 2 1 3 ... ## $ Mean : num 7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ... ## $ N. : int 471 570 1041 468 542 1010 416 555 971 433 ...  Reading data with read.csv() It is a wrapper around read.table(). This means that read.csv() calls read.table() behind the scenes but with different default arguments. More specifically, the defaults are header = TRUE and sep = \",\". These match with the standardized CSV format, where , is used as a separator and usually the first line contains the names of the columns. Therefore, it saves you time since you need to specify less arguments.\nread.csv(file = \u0026quot;happiness.csv\u0026quot;, stringsAsFactors = FALSE) which is equivalent to\nread.table(file = \u0026quot;happiness.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot;, stringsAsFactors = FALSE)  Reading data with read.delim() It is also a wrapper of read.table(). Now the default arguments match with tab-delimited files. More specifically, the defaults are header = TRUE and sep = \"\\t\", since \\t is the field separator in tab-delimited files.\nread.delim(file = \u0026quot;happiness.txt\u0026quot;, stringsAsFactors = FALSE) which is equivalent to\nread.table(file = \u0026quot;happiness.txt\u0026quot;, header = TRUE, sep = \u0026quot;\\t\u0026quot;, stringsAsFactors = FALSE) Both these functions make our lives easier since less arguments need to be specified.\nNote Locale differences. The standard field delimiters for CSV files are commas. On US versions, the comma is set as default for the “List Separator”, which is okay for CSV files. But on European versions this character is reserved as the Decimal Symbol and the “List Separator” is set by default to the semicolon. Why you should care?\nSuppose you try to import the European CSV version happiness_eu.csv.\nhead(happiness_eu) ## Country.Gender.Mean.N. ## 1 AT,Male,7.3,471 ## 2 ,Female,7.3,570 ## 3 ,Both,7.3,1041 ## 4 BE,Male,7.8,468 ## 5 ,Female,7.8,542 ## 6 ,Both,7.8,1010 R performs the operation but clearly not the one we wanted. It’s a data frame with 105 rows but a single variable! To deal with such problems you can use the read.csv2() function. The defaults are sep = \";\" and dec = \",\".\nhappiness_eu \u0026lt;- read.csv2(file = \u0026quot;happiness_eu.csv\u0026quot;, stringsAsFactors = FALSE) head(happiness_eu) ## Country Gender Mean N. ## 1 AT Male 7.3 471 ## 2 Female 7.3 570 ## 3 Both 7.3 1041 ## 4 BE Male 7.8 468 ## 5 Female 7.8 542 ## 6 Both 7.8 1010 Similarly, there is read.delim2(). The logic is the same.\nTo summarize, the read.table() is to read delimited data files. Some variants are: read.csv() and read.delim(), which have different default values and are tailored for CSV and tab-delimited files, respectively.\n In read.csv() default values are: header = T, sep = ???,???, dec = ???.??? In read.csv2() default values are: header = T, sep = ???;???, dec = ???,??? In read.delim() default values are: header = T, sep = ???\\t???, dec = ???.??? In read.delim2() default values are: header = T, sep = ???\\t???, dec = ???,???  ##readr … an alternative to import flat files\nAn alternative to the utils package is the readr. Compared the read.table family of functions, it is faster, easier to use and with a consistent naming scheme. We start by installing and loading it.\ninstall.packages(\u0026quot;readr\u0026quot;) library(readr) Let’s import our dataset. In readr you can use read_delim() for flat files. It can be considered the correspondent to read.table().\nhappiness_readr \u0026lt;- read_delim(\u0026quot;happiness.csv\u0026quot;, # path to flat file delim = \u0026quot;,\u0026quot;) # character that separates fields in the file ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Country = col_character(), ## Gender = col_character(), ## Mean = col_double(), ## `N=` = col_double() ## ) str(happiness_readr) ## tibble [105 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Country: chr [1:105] \u0026quot;AT\u0026quot; NA NA \u0026quot;BE\u0026quot; ... ## $ Gender : chr [1:105] \u0026quot;Male\u0026quot; \u0026quot;Female\u0026quot; \u0026quot;Both\u0026quot; \u0026quot;Male\u0026quot; ... ## $ Mean : num [1:105] 7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ... ## $ N= : num [1:105] 471 570 1041 468 542 ... ## - attr(*, \u0026quot;spec\u0026quot;)= ## .. cols( ## .. Country = col_character(), ## .. Gender = col_character(), ## .. Mean = col_double(), ## .. `N=` = col_double() ## .. ) head(happiness_readr) ## # A tibble: 6 x 4 ## Country Gender Mean `N=` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 AT Male 7.3 471 ## 2 \u0026lt;NA\u0026gt; Female 7.3 570 ## 3 \u0026lt;NA\u0026gt; Both 7.3 1041 ## 4 BE Male 7.8 468 ## 5 \u0026lt;NA\u0026gt; Female 7.8 542 ## 6 \u0026lt;NA\u0026gt; Both 7.8 1010 Notice, that the output is the same as when using the read.table(), previously. But we did not have to specify header=TRUE because by default read_delim() expects the first row to contain the column names. This is done through the col_names argument, set equal to true by default. Also, strings are never automatically converted to factors. Hence, stringsAsFactors = FALSE is not necessary. To control the types of the columns readr uses the col_types argument. Let’s see how these two work.\ncol_names is true by default meaning that it will use the the first row of data as column names. If your file does not have column names you can set col_names = FALSE and columns will be numbered sequentially.\nhead(read_delim(\u0026quot;happiness2.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, col_names = FALSE)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## X1 = col_character(), ## X2 = col_character(), ## X3 = col_double(), ## X4 = col_character() ## ) ## # A tibble: 6 x 4 ## X1 X2 X3 X4 ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026#39;AT Male 7.3 471\u0026#39; ## 2 \u0026#39; Female 7.3 570\u0026#39; ## 3 \u0026#39; Both 7.3 1041\u0026#39; ## 4 \u0026#39;BE Male 7.8 468\u0026#39; ## 5 \u0026#39; Female 7.8 542\u0026#39; ## 6 \u0026#39; Both 7.8 1010\u0026#39;  Note Instead of assigning the output of read_delim() to a variable I directly use the head() function to print the first 6 lines of the data frame. It is equivalent to\n happiness_delim \u0026lt;- read_delim(\u0026quot;happiness2.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, col_names = FALSE) head(happiness_delim) You can also manually set the column names.\nhead(read_delim(\u0026quot;happiness2.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, col_names = c(\u0026quot;Country\u0026quot;, \u0026quot;Gender\u0026quot;, \u0026quot;Mean\u0026quot;, \u0026quot;N\u0026quot;))) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Country = col_character(), ## Gender = col_character(), ## Mean = col_double(), ## N = col_character() ## ) ## # A tibble: 6 x 4 ## Country Gender Mean N ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026#39;AT Male 7.3 471\u0026#39; ## 2 \u0026#39; Female 7.3 570\u0026#39; ## 3 \u0026#39; Both 7.3 1041\u0026#39; ## 4 \u0026#39;BE Male 7.8 468\u0026#39; ## 5 \u0026#39; Female 7.8 542\u0026#39; ## 6 \u0026#39; Both 7.8 1010\u0026#39; As mentioned, there is col_types to control the column classes. If you leave the default value readr heuristically inspects the first 100 rows to guess the type of each column.\nsapply(happiness_readr, class) ## Country Gender Mean N= ## \u0026quot;character\u0026quot; \u0026quot;character\u0026quot; \u0026quot;numeric\u0026quot; \u0026quot;numeric\u0026quot; If you want to override the default column types you can also specify them manually. An option would be\nhappiness_readr2 \u0026lt;- read_delim(\u0026quot;happiness.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, col_types = \u0026quot;ccni\u0026quot;) sapply(happiness_readr2, class) ## Country Gender Mean N= ## \u0026quot;character\u0026quot; \u0026quot;character\u0026quot; \u0026quot;numeric\u0026quot; \u0026quot;integer\u0026quot; Where\n c = character d = double i = integer l = logical _ = skip  Let see how skip works\nhead(read_delim(\u0026quot;happiness.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, col_types = \u0026quot;ccn_\u0026quot;)) ## # A tibble: 6 x 3 ## Country Gender Mean ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 AT Male 7.3 ## 2 \u0026lt;NA\u0026gt; Female 7.3 ## 3 \u0026lt;NA\u0026gt; Both 7.3 ## 4 BE Male 7.8 ## 5 \u0026lt;NA\u0026gt; Female 7.8 ## 6 \u0026lt;NA\u0026gt; Both 7.8 Notice the fourth column has been skipped.\nYet another way of setting the types of the imported columns is using collectors. Collector functions can be passed in a list() to the col_types argument of read_ functions to tell them how to interpret values in a column.\ncar \u0026lt;- col_character() fac \u0026lt;- col_factor(levels = c(\u0026quot;Male\u0026quot;, \u0026quot;Female\u0026quot;, \u0026quot;Both\u0026quot;)) num \u0026lt;- col_number() int \u0026lt;- col_integer() str(read_delim(\u0026quot;happiness.csv\u0026quot;, delim = \u0026quot;,\u0026quot; , col_types = list(car, fac, num, int))) ## tibble [105 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Country: chr [1:105] \u0026quot;AT\u0026quot; NA NA \u0026quot;BE\u0026quot; ... ## $ Gender : Factor w/ 3 levels \u0026quot;Male\u0026quot;,\u0026quot;Female\u0026quot;,..: 1 2 3 1 2 3 1 2 3 1 ... ## $ Mean : num [1:105] 7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ... ## $ N= : int [1:105] 471 570 1041 468 542 1010 416 555 971 433 ... ## - attr(*, \u0026quot;spec\u0026quot;)= ## .. cols( ## .. Country = col_character(), ## .. Gender = col_factor(levels = c(\u0026quot;Male\u0026quot;, \u0026quot;Female\u0026quot;, \u0026quot;Both\u0026quot;), ordered = FALSE, include_na = FALSE), ## .. Mean = col_number(), ## .. `N=` = col_integer() ## .. ) For a complete list of collector functions, you can take a look at the collector documentation.\nIf you are working on large datasets you may prefer handling the data in smaller parts. In readr you can achieve this with the combination of skip and n_max arguments.\nhead(read_delim(\u0026quot;happiness.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, skip=2, n_max= 4)) ## Warning: Missing column names filled in: \u0026#39;X1\u0026#39; [1] ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## X1 = col_character(), ## Female = col_character(), ## `7.3` = col_double(), ## `570` = col_double() ## ) ## # A tibble: 4 x 4 ## X1 Female `7.3` `570` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 \u0026lt;NA\u0026gt; Both 7.3 1041 ## 2 BE Male 7.8 468 ## 3 \u0026lt;NA\u0026gt; Female 7.8 542 ## 4 \u0026lt;NA\u0026gt; Both 7.8 1010 We skipped two rows and then read four lines. There is something wrong though! Since the col_names is true by default the first line is used for the column names. Therefore, we need to manually specify the column names.\nhead(read_delim(\u0026quot;happiness2.csv\u0026quot;, delim = \u0026quot;,\u0026quot;, col_names = c(\u0026quot;Country\u0026quot;,\u0026quot;Gender\u0026quot;, \u0026quot;Mean\u0026quot;, \u0026quot;N\u0026quot;), skip=2, n_max= 4)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Country = col_character(), ## Gender = col_character(), ## Mean = col_double(), ## N = col_character() ## ) ## # A tibble: 4 x 4 ## Country Gender Mean N ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026#39; Both 7.3 1041\u0026#39; ## 2 \u0026#39;BE Male 7.8 468\u0026#39; ## 3 \u0026#39; Female 7.8 542\u0026#39; ## 4 \u0026#39; Both 7.8 1010\u0026#39; Like the utils package readr provides alternatives to read_delim(). The read_csv() and read_tsv() are used for CSV files and tab-delimited files, respectively. The functions of both packages are presented below. Notice the _ is used in readr instead of the ..\n  utils readr    read.table() read_delim()  read.csv() read_csv()  read.delim() read.tsv      data.table … yet another alternative to read data into R The data.table package is designed mainly for fast data manipulation. It also features a powerful function to import your data into R, the fread(). Once more you need to install and load the package.\ninstall.packages(\u0026quot;data.table\u0026quot;) library(data.table) Let’s see how it works with two versions of our dataset.\nhead(fread(\u0026quot;happiness.csv\u0026quot;)) ## Country Gender Mean N= ## 1: AT Male 7.3 471 ## 2: Female 7.3 570 ## 3: Both 7.3 1041 ## 4: BE Male 7.8 468 ## 5: Female 7.8 542 ## 6: Both 7.8 1010 head(fread(\u0026quot;happiness2.csv\u0026quot;)) ## V1 V2 V3 V4 ## 1: \u0026#39;AT Male 7.3 471\u0026#39; ## 2: \u0026#39; Female 7.3 570\u0026#39; ## 3: \u0026#39; Both 7.3 1041\u0026#39; ## 4: \u0026#39;BE Male 7.8 468\u0026#39; ## 5: \u0026#39; Female 7.8 542\u0026#39; ## 6: \u0026#39; Both 7.8 1010\u0026#39; Remember that the first row of happiness2.csv does not contain the column names. That’s not a problem for fread() as it automatically assignees names to the columns. As in this case, often simply specifying the path to the file is enough to successfully import your flat file using fread. Moreover, it can infer the column types and separators.\nstr(fread(\u0026quot;happiness.csv\u0026quot;)) ## Classes \u0026#39;data.table\u0026#39; and \u0026#39;data.frame\u0026#39;: 105 obs. of 4 variables: ## $ Country: chr \u0026quot;AT\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;BE\u0026quot; ... ## $ Gender : chr \u0026quot;Male\u0026quot; \u0026quot;Female\u0026quot; \u0026quot;Both\u0026quot; \u0026quot;Male\u0026quot; ... ## $ Mean : num 7.3 7.3 7.3 7.8 7.8 7.8 5.8 5.8 5.8 7.8 ... ## $ N= : int 471 570 1041 468 542 1010 416 555 971 433 ... ## - attr(*, \u0026quot;.internal.selfref\u0026quot;)=\u0026lt;externalptr\u0026gt; Two more useful arguments of fread() are drop and select. They enable you to drop or select variables of interest in your flat file. Suppose I want to select the 2nd and 3rd column.\nhead(fread(\u0026quot;happiness.csv\u0026quot;, select = c(2,3))) ## Gender Mean ## 1: Male 7.3 ## 2: Female 7.3 ## 3: Both 7.3 ## 4: Male 7.8 ## 5: Female 7.8 ## 6: Both 7.8 Alternatively,\nhead(fread(\u0026quot;happiness.csv\u0026quot;, select = c(\u0026quot;Gender\u0026quot;,\u0026quot;Mean\u0026quot;))) ## Gender Mean ## 1: Male 7.3 ## 2: Female 7.3 ## 3: Both 7.3 ## 4: Male 7.8 ## 5: Female 7.8 ## 6: Both 7.8 or\nhead(fread(\u0026quot;happiness.csv\u0026quot;, drop = c(1,4))) ## Gender Mean ## 1: Male 7.3 ## 2: Female 7.3 ## 3: Both 7.3 ## 4: Male 7.8 ## 5: Female 7.8 ## 6: Both 7.8 which is equivalent to\nhead(fread(\u0026quot;happiness.csv\u0026quot;, drop = c(\u0026quot;Country\u0026quot;,\u0026quot;N=\u0026quot;))) In short, fread() saves you work by automatically guessing the delimiter, whether or not the file has a header, how many lines to skip by default, providing an easy way to select variables and more. Nevertheless, if you wish to specify them, you can do it, along with other arguments. Check the documentation.\nComment You might have noticed by now that the fread() function produces data frames that look slightly different when you print them out. That’s because another class is assigned to the resulting data frames, namely data.table and data.frame. read_delim() creates an object with three classes: tbl_df, tbl and data.frame. The printout of such data.table objects is different. Well, it allows for a different treatment of the printouts, for example.\n When to use read.table(), read_delim() or fread() In a nutshell, the main differences between these functions are : the read_ functions from the readr package have more consistent naming scheme for the parameters (e.g. col_names and col_types) than read. and all functions work exactly the same way regardless of the current locale (to override the US-centric defaults, use locale()). It is also faster! But fread() is even faster! And it saves you work by automatically guessing parameters. This README goes in more detail.\n   Speed Auto-detection Locale    read.table() fast NO YES  read_delim() faster NO NO  fread() fastest YES NO    With some loss of generality a few suggestions are:\n for large files (many MB-GB) fread() will be the fastest (with a few exceptions) the consistency and independence of the actual locale makes readr a good candidate for everyday use if you are new to all these using read.table() will allow you to develop intuition on how R works.   ","date":1523318400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523318400,"objectID":"243cfc121bb1400aff481e6cecbb96d4","permalink":"https://solon-karapanagiotis.com/post/importflatfiles/importing-flat-files-into-r/","publishdate":"2018-04-10T00:00:00Z","relpermalink":"/post/importflatfiles/importing-flat-files-into-r/","section":"post","summary":"There are many tutorials for importing data into R focusing on a specific function/package. This one focuses on 3 different packages. You will learn how to import all common formats of flat file data with base R functions and the dedicated readr and data.","tags":["import","data","R"],"title":"Importing Flat Files Into R","type":"post"},{"authors":["Solon Karapanagiotis","Paul D. P. Pharoah","Christopher H. Jackson","Paul J. Newcombe"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"1748b5665cbdb6ac88b607f717cf465e","permalink":"https://solon-karapanagiotis.com/publication/clinicalcancerresearch/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/clinicalcancerresearch/","section":"publication","summary":"**Purpose**: To compare PREDICT and CancerMath, two widely used prognostic models for invasive breast cancer, taking into account their clinical utility. Furthermore, it is unclear whether these models could be improved. **Experimental Design**: A dataset of 5729 women was used for model development. A Bayesian variable selection algorithm was implemented to stochastically search for important interaction terms among the predictors. The derived models were then compared in three independent datasets (n = 5534). We examined calibration, discrimination and performed decision curve analysis. **Results**: CancerMath demonstrated worse calibration performance compared to PREDICT in oestrogen receptor (ER)-positive and ER-negative tumours. The decline in discrimination performance was -4.27% (-6.39 - -2.03) and -3.21% (-5.9 - -0.48) for ER-positive and ER-negative tumours, respectively. Our new models matched the performance of PREDICT in terms of calibration and discrimination, but offered no improvement. Decision curve analysis showed predictions for all models were clinically useful for treatment decisions made at risk thresholds between 5% and 55% for ER-positive tumours and at thresholds of 15% to 60% for ER-negative tumours. Within these threshold ranges, CancerMath provided the lowest clinical utility amongst all the models. **Conclusions**: Survival probabilities from PREDICT offer both improved accuracy and discrimination over CancerMath. Using PREDICT to make treatment decisions offers greater clinical utility than CancerMath over a range of risk thresholds. Our new models performed as well as PREDICT, but no better, suggesting that, in this setting, including further interaction terms offers no predictive benefit.","tags":null,"title":"Development and External Validation of Prediction Models for 10-Year Survival of Invasive Breast Cancer. Comparison with PREDICT and CancerMath","type":"publication"},{"authors":["Solon Karapanagiotis","Simone Gambazza","Anna Brivio","Francesco D'Abrosca","Carla Colombo"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"13cfae4635392c140835b566ec03e2bd","permalink":"https://solon-karapanagiotis.com/publication/ventilatorylimitation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/ventilatorylimitation/","section":"publication","summary":"**Objective**: To investigate the presence of dynamic hyperinflation after the Modified Shuttle Test (MST) and its relationship with lung function, exercise tolerance, and clinical symptoms in Cystic Fibrosis (CF). **Methods**: Retrospective observational study. Subjects in clinically stable condition with a CF diagnosis based on a positive sweat test (chloride 60 mEq/L) and/or presence of two disease causing mutations, with available data on MST, spirometry, maximal voluntary ventilation, and inspiratory capacity manoeuvres were considered for the analysis. Breathing reserve was calculated and a threshold value of 0.7 was subsequently chosen as a value of pulmonary mechanical limit. Subjects were then categorized into two groups according to the change in the inspiratory capacity from rest to peak exercise. Unconditional logistic regression was used to estimate unadjusted odds ratios, 95% confidence intervals and P‐values. **Results**: Twenty‐two subjects demonstrated evidence of dynamic hyperinflation during the MST. Thirteen out of 22 subjects were ventilatory limited during exercise including 5 of those without evidence of dynamic hyperinflation (P = 0.24). No combination of variables resulted in a parsimonious regression model. **Conclusions**: Dynamic hyperinflation is common in CF and it is not associated with traditionally defined ventilatory limitation parameters during the MST.","tags":null,"title":"Ventilatory limitation and dynamic hyperinflation during exercise testing in Cystic Fibrosis","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://solon-karapanagiotis.com/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://solon-karapanagiotis.com/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Simone Gambazza","Clara Ceruti","Anna Brivio","Giancarlo Piaggi","Solon Karapanagiotis","Carla Colombo"],"categories":null,"content":"","date":1443657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443657600,"objectID":"89703def19b57b44c282eaa62bf90cb5","permalink":"https://solon-karapanagiotis.com/publication/isocapnichyperpnea/","publishdate":"2015-10-01T00:00:00Z","relpermalink":"/publication/isocapnichyperpnea/","section":"publication","summary":"To evaluate the bias and precision of the respiratory muscle training device formulas to predict respiratory minute volume (RMV) and volume of the reservoir bag (BV) on a cohort of subjects with Cystic Fibrosis (CF). CF patients with available pulmonary function tests and maximal voluntary manoeuvres were included in the study. Vital capacity and maximal voluntary ventilation were extracted from subjects’ records and then inserted to the manufacturer’s formulas to obtain RMV and BV (measured setting). RMV and BV were compared according to standard and measured formulas in males and females. Sample was described and then processed using Bland–Altman analysis. Bland–Altman analysis for RMV revealed a bias and precision of 8.8 ± 29 L/min in males and 28.8 ± 16 L/min in females; 0.4 ± 0.5 L in males and 0.7 ± 0.4 L in females for BV. Concordance correlation coefficients for RMV were −0.03 in males and 0.02 in females; 0.22 in males and 0.03 in females for BV, reinforcing an unsatisfactory concordance between measured and manufacturer setting. This study shows considerable discrepancies between the two methods, making the degree of agreement not clinically acceptable. This might cause inappropriate setting and disservice to patients with CF.","tags":null,"title":"Isocapnic hyperpnea with a portable device in Cystic Fibrosis: an agreement study between two different set-up modalities","type":"publication"},{"authors":["Solon Karapanagiotis","Simone Gambazza","Anna Brivio","Carla Colombo"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"7fa8967448de8cfe3e73d3bbc7f28f21","permalink":"https://solon-karapanagiotis.com/publication/mst/","publishdate":"2014-06-01T00:00:00Z","relpermalink":"/publication/mst/","section":"publication","summary":"","tags":null,"title":"Cystic fibrosis patients’ performance on Modified Shuttle Walk Test","type":"publication"},{"authors":["Solon Karapanagiotis","Simone Gambazza","Anna Brivio","Carla Colombo"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"ce29bd6e506f6eefa2f2e76e45e24141","permalink":"https://solon-karapanagiotis.com/publication/exerciseintensity/","publishdate":"2014-06-01T00:00:00Z","relpermalink":"/publication/exerciseintensity/","section":"publication","summary":"","tags":null,"title":"Exercise intensity during interactive video game","type":"publication"},{"authors":["Solon Karapanagiotis","Simone Gambazza","Arianna Bisogno","Anna Brivio","Carla Colombo"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"5b78ce9ecbe3fdd2d81cc6ddb780f60e","permalink":"https://solon-karapanagiotis.com/publication/quantifyingweight/","publishdate":"2014-06-01T00:00:00Z","relpermalink":"/publication/quantifyingweight/","section":"publication","summary":"","tags":null,"title":"Quantifying weight bearing activity in children and adolescents with cystic fibrosis","type":"publication"},{"authors":["M. Donà","F. Alatri","A. Brivio","G. Mamprin","M. Barbisan","M. Varchetta","S. De Sanctis","S. Gambazza","S. Karapanagiotis","M. Ros"],"categories":null,"content":"","date":1370044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1370044800,"objectID":"7eff796b9eeb6524f3d1f4eb19d267e7","permalink":"https://solon-karapanagiotis.com/publication/adherence/","publishdate":"2013-06-01T00:00:00Z","relpermalink":"/publication/adherence/","section":"publication","summary":"","tags":null,"title":"Adherence to the administration of aerosolized promixin with the I-neb adaptive aerosol delivery (AAD) system, lung function and administration times in patients with cystic fibrosis (CF)","type":"publication"},{"authors":["Solon Karapanagiotis","Anna Brivio","Simone Gambazza","Chiara Speziali","Carla Colombo"],"categories":null,"content":"","date":1370044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1370044800,"objectID":"7d3bd0ef73dcfb4e261c3ed6207ffdd5","permalink":"https://solon-karapanagiotis.com/publication/exerciseandsporthabits/","publishdate":"2013-06-01T00:00:00Z","relpermalink":"/publication/exerciseandsporthabits/","section":"publication","summary":"","tags":null,"title":"Exercise and sport habits in children and adolescents with cystic fibrosis","type":"publication"},{"authors":["Solon Karapanagiotis","Alessandro Rossi","Stefano Vercelli"],"categories":null,"content":"","date":1338508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338508800,"objectID":"28c5a32a015711d9ea5eb7d9f62705a0","permalink":"https://solon-karapanagiotis.com/publication/wiihabilitation/","publishdate":"2012-06-01T00:00:00Z","relpermalink":"/publication/wiihabilitation/","section":"publication","summary":"","tags":null,"title":"Wiihabilitation","type":"publication"},{"authors":null,"categories":null,"content":"#\u0026mdash; #abstract: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus\nac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida. author_notes:\n Equal contribution Equal contribution authors: admin Robert Ford date: \u0026ldquo;2015-09-01T00:00:00Z\u0026rdquo; doi: \u0026quot;\u0026quot; featured: false image: caption: \u0026lsquo;Image credit: Unsplash\u0026rsquo; focal_point: \u0026quot;\u0026quot; preview_only: false projects: [] publication: \u0026lsquo;Journal of Source Themes, 1(1)\u0026rsquo; publication_short: \u0026quot;\u0026quot; publication_types: \u0026ldquo;0\u0026rdquo; publishDate: \u0026ldquo;2017-01-01T00:00:00Z\u0026rdquo; slides: example summary: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. tags: Source Themes title: An example journal article url_code: \u0026quot;\u0026quot; url_dataset: \u0026quot;\u0026quot; url_pdf: http://arxiv.org/pdf/1512.04133v1 url_poster: \u0026quot;\u0026quot; url_project: \u0026quot;\u0026quot; url_slides: \u0026quot;\u0026quot; url_source: \u0026quot;\u0026quot; url_video: \u0026quot;\u0026quot;    Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://solon-karapanagiotis.com/publication/journal-article/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"#\u0026mdash; #abstract: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus\nac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":null,"title":"","type":"publication"},{"authors":null,"categories":null,"content":"#\u0026mdash; #abstract: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus\nac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida. authors:\n admin date: \u0026ldquo;2019-04-07T00:00:00Z\u0026rdquo; doi: \u0026quot;\u0026quot; featured: false image: caption: \u0026lsquo;Image credit: Unsplash\u0026rsquo; focal_point: \u0026quot;\u0026quot; preview_only: false links: name: Custom Link url: http://example.org projects: internal-project publication: \u0026quot;\u0026quot; publication_short: \u0026quot;\u0026quot; publication_types: \u0026ldquo;3\u0026rdquo; publishDate: \u0026ldquo;2017-01-01T00:00:00Z\u0026rdquo; slides: example summary: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. tags: Source Themes title: An example preprint / working paper url_code: \u0026lsquo;#\u0026rsquo; url_dataset: \u0026lsquo;#\u0026rsquo; url_pdf: http://arxiv.org/pdf/1512.04133v1 url_poster: \u0026lsquo;#\u0026rsquo; url_project: \u0026quot;\u0026quot; url_slides: \u0026quot;\u0026quot; url_source: \u0026lsquo;#\u0026rsquo; url_video: \u0026lsquo;#\u0026rsquo;    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://solon-karapanagiotis.com/publication/preprint/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"#\u0026mdash; #abstract: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus\nac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":null,"title":"","type":"publication"}]